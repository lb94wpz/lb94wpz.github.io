<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/next/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/next/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/next/images/favicon-16x16.png"><link rel="mask-icon" href="/next/images/logo.svg" color="#222"><link rel="stylesheet" href="/next/css/main.css"><link rel="stylesheet" href="/next/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"wpz.me",root:"/next/",scheme:"Pisces",version:"7.8.0",exturl:!0,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"default"},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!0,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta property="og:type" content="article"><meta property="og:title" content="（一）PyTorch 张量"><meta property="og:url" content="https://wpz.me/posts/668e89ae/index.html"><meta property="og:site_name" content="Wpz&#39;s Blog"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2024-08-26T08:17:41.000Z"><meta property="article:modified_time" content="2025-04-02T08:13:59.622Z"><meta property="article:author" content="wpz"><meta property="article:tag" content="PyTorch"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://wpz.me/posts/668e89ae/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><link rel="stylesheet" href="//at.alicdn.com/t/c/font_4586467_ehp586b07d9.css"><title>（一）PyTorch 张量 | Wpz's Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript>
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/next/atom.xml" title="Wpz's Blog" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/next/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Wpz's Blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">闻道有先后，术业有专攻。</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/next/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/next/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-主站"><a href="https://wpz.me/" rel="section"><i class="fa fa-sitemap fa-fw"></i>主站</a></li><li class="menu-item menu-item-tags"><a href="/next/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/next/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-随机"><a href="javascript:toRandomPost()" rel="section"><i class="fa fa-random fa-fw"></i>随机</a></li><li class="menu-item menu-item-about"><a href="/next/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://wpz.me/posts/668e89ae/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/next/images/avatar.jpg"><meta itemprop="name" content="wpz"><meta itemprop="description" content="你只管努力，剩下的交给时间！"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Wpz's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">（一）PyTorch 张量</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2024-08-26 16:17:41" itemprop="dateCreated datePublished" datetime="2024-08-26T16:17:41+08:00">2024-08-26</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-04-02 16:13:59" itemprop="dateModified" datetime="2025-04-02T16:13:59+08:00">2025-04-02</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/next/categories/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>50k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>45 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><span></span></p><span id="more"></span><h2 id="Tensor-数据类型"><a href="#Tensor-数据类型" class="headerlink" title="Tensor 数据类型"></a>Tensor 数据类型</h2><div class="table-container"><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU Tensor</th><th>GPU Tensor</th></tr></thead><tbody><tr><td>Boolean</td><td>torch.bool</td><td>torch.BoolTensor</td><td>torch.cuda.BoolTensor</td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32 or torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64 or torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>16-bit floating point</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>16-bit floating point</td><td>torch.bfloat16</td><td>torch.BFloat16Tensor</td><td>torch.cuda.BFloat16Tensor</td></tr><tr><td>32-bit floating point</td><td>torch.float32 or torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64 or torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr></tbody></table></div><h2 id="设置-Tensor-默认类型"><a href="#设置-Tensor-默认类型" class="headerlink" title="设置 Tensor 默认类型"></a>设置 Tensor 默认类型</h2><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).dtype)  <span class="comment"># torch.bool</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).<span class="built_in">type</span>())  <span class="comment"># torch.BoolTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).dtype)  <span class="comment"># torch.int64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).<span class="built_in">type</span>())  <span class="comment"># torch.LongTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).dtype)  <span class="comment"># torch.float32</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).<span class="built_in">type</span>())  <span class="comment"># torch.FloatTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).dtype)  <span class="comment"># torch.complex64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).<span class="built_in">type</span>())  <span class="comment"># torch.ComplexFloatTensor</span></span><br><span class="line"></span><br><span class="line">torch.set_default_dtype(torch.double)  <span class="comment"># 设置默认类型为 double</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).dtype)  <span class="comment"># torch.bool</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).<span class="built_in">type</span>())  <span class="comment"># torch.BoolTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).dtype)  <span class="comment"># torch.int64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).<span class="built_in">type</span>())  <span class="comment"># torch.LongTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).dtype)  <span class="comment"># torch.float64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).<span class="built_in">type</span>())  <span class="comment"># torch.DoubleTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).dtype)  <span class="comment"># torch.complex128</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).<span class="built_in">type</span>())  <span class="comment"># torch.ComplexDoubleTensor</span></span><br></pre></td></tr></tbody></table></figure><p><code>set_default_dtype()</code>只能设置<code>floating-point</code>类型，否则会报<code>TypeError: only floating-point types are supported as the default type</code>错误。</p><h2 id="标量与张量"><a href="#标量与张量" class="headerlink" title="标量与张量"></a>标量与张量</h2><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标量</span></span><br><span class="line">a = torch.tensor(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)  <span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(a.size())  <span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(a.ndim)  <span class="comment"># 0</span></span><br><span class="line"><span class="built_in">print</span>(a.dim())  <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一维张量</span></span><br><span class="line">b = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(b.shape)  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(b.size())  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(b.ndim)  <span class="comment"># 1</span></span><br><span class="line"><span class="built_in">print</span>(b.dim())  <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维张量</span></span><br><span class="line">c = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(c.shape)  <span class="comment"># torch.Size([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(c.size())  <span class="comment"># torch.Size([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(c.ndim)  <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(c.dim())  <span class="comment"># 2</span></span><br></pre></td></tr></tbody></table></figure><p>标量是一个单独的数，<code>ndim</code>为<code>0</code>。</p><h2 id="创建-Tensor"><a href="#创建-Tensor" class="headerlink" title="创建 Tensor"></a>创建 Tensor</h2><h3 id="tensor"><a href="#tensor" class="headerlink" title=".tensor"></a>.tensor</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标量</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>))  <span class="comment"># tensor(1)</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>, dtype=torch.float64))  <span class="comment"># tensor(1., dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))  <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1</span>, <span class="number">2</span>], dtype=torch.float64))  <span class="comment"># tensor([1., 2.], dtype=torch.float64)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="from-numpy"><a href="#from-numpy" class="headerlink" title=".from_numpy"></a>.from_numpy</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(data))  <span class="comment"># tensor([1, 2, 3], dtype=torch.int32)</span></span><br><span class="line">data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(data))  <span class="comment"># tensor([1., 2., 3.], dtype=torch.float64)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数为 shape 大小</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor(<span class="number">1</span>))  <span class="comment"># tensor([-3.0434e+31])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># tensor([[0., 0.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[[-3.0741e+31,  1.6031e-42,  0.0000e+00],</span></span><br><span class="line"><span class="string">         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数为列表</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor([<span class="number">1</span>]))  <span class="comment"># tensor([1.])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor([<span class="number">1</span>, <span class="number">2</span>]))  <span class="comment"># tensor([1., 2.])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))  <span class="comment"># tensor([1., 2., 3.])</span></span><br></pre></td></tr></tbody></table></figure><p><code>Tensor</code>支持两种传参方式：</p><ol><li>当参数为列表时，创建列表对应维度的<code>Tensor</code>并初始化数据为列表数据。</li><li>当参数不为列表时，与<code>.empty()</code>类似，创建参数指定的<code>shape</code>的空的<code>Tensor</code>。</li></ol><p><code>BoolTensor</code>、<code>ByteTensor</code>、<code>CharTensor</code>、<code>ShortTensor</code>、<code>IntTensor</code>、<code>LongTensor</code>、<code>HalfTensor</code>、<code>FloatTensor</code>、<code>DoubleTensor</code>也是一样。</p><h3 id="empty-zeros-ones-full-eye"><a href="#empty-zeros-ones-full-eye" class="headerlink" title=".empty/.zeros/.ones/.full/.eye"></a>.empty/.zeros/.ones/.full/.eye</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.empty(()))  <span class="comment"># tensor(-6.5391e-19)</span></span><br><span class="line"><span class="built_in">print</span>(torch.empty((<span class="number">1</span>, <span class="number">5</span>)))  <span class="comment"># tensor([[-6.6069e-19,  1.3943e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.empty_like((<span class="built_in">input</span>)))  <span class="comment"># tensor([0, 0, 0, 0, 0])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.zeros(()))  <span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.zeros((<span class="number">1</span>, <span class="number">5</span>)))  <span class="comment"># tensor([[0., 0., 0., 0., 0.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.zeros_like((<span class="built_in">input</span>)))  <span class="comment"># tensor([0, 0, 0, 0, 0])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.ones(()))  <span class="comment"># tensor(1.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.ones((<span class="number">1</span>, <span class="number">5</span>)))  <span class="comment"># tensor([[1., 1., 1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.ones_like((<span class="built_in">input</span>)))  <span class="comment"># tensor([1, 1, 1, 1, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.full((), <span class="number">100</span>))  <span class="comment"># tensor(100)</span></span><br><span class="line"><span class="built_in">print</span>(torch.full((<span class="number">1</span>, <span class="number">5</span>), <span class="number">100</span>))  <span class="comment"># tensor([[100, 100, 100, 100, 100]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.full_like((<span class="built_in">input</span>), <span class="number">100</span>))  <span class="comment"># tensor([100, 100, 100, 100, 100])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.eye(<span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.eye(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 1., 0., 0.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="arange-linspace-logspace"><a href="#arange-linspace-logspace" class="headerlink" title=".arange/.linspace/.logspace"></a>.arange/.linspace/.logspace</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">0</span>, <span class="number">10</span>))  <span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">0</span>, <span class="number">10</span>, step=<span class="number">3</span>))  <span class="comment"># tensor([0, 3, 6, 9])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [0, 1] 等分成 5 份</span></span><br><span class="line"><span class="built_in">print</span>(torch.linspace(<span class="number">0</span>, <span class="number">1</span>, steps=<span class="number">5</span>))  <span class="comment"># tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认以 10 为底数，[0, 1] 等分成 5 份做为指数</span></span><br><span class="line"><span class="built_in">print</span>(torch.logspace(<span class="number">0</span>, <span class="number">1</span>, steps=<span class="number">5</span>))  <span class="comment"># tensor([ 1.0000,  1.7783,  3.1623,  5.6234, 10.0000])</span></span><br><span class="line"><span class="comment"># 以 2 为底数，[0, 1] 等分成 5 份做为指数</span></span><br><span class="line"><span class="built_in">print</span>(torch.logspace(<span class="number">0</span>, <span class="number">1</span>, steps=<span class="number">5</span>, base=<span class="number">2</span>))  <span class="comment"># tensor([1.0000, 1.1892, 1.4142, 1.6818, 2.0000])</span></span><br></pre></td></tr></tbody></table></figure><h2 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h2><h3 id="随机种子"><a href="#随机种子" class="headerlink" title="随机种子"></a>随机种子</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 CPU 随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 GPU 随机种子</span></span><br><span class="line">torch.cuda.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看设置的随机种子</span></span><br><span class="line"><span class="built_in">print</span>(torch.initial_seed())  <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机设置随机种子</span></span><br><span class="line">torch.seed()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.initial_seed())  <span class="comment"># 22287915889500</span></span><br></pre></td></tr></tbody></table></figure><h3 id="随机函数"><a href="#随机函数" class="headerlink" title="随机函数"></a>随机函数</h3><h4 id="rand-rand-like"><a href="#rand-rand-like" class="headerlink" title=".rand/.rand_like"></a>.rand/.rand_like</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999],</span></span><br><span class="line"><span class="string">        [0.3971, 0.7544, 0.5695]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.rand_like(<span class="built_in">input</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.4388, 0.6387, 0.5247],</span></span><br><span class="line"><span class="string">        [0.6826, 0.3051, 0.4635],</span></span><br><span class="line"><span class="string">        [0.4550, 0.5725, 0.4980]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.rand()</code>返回在区间<code>[0, 1)</code>均匀分布的随机数填充的张量。</p><h4 id="randint-randint-like"><a href="#randint-randint-like" class="headerlink" title=".randint/.randint_like"></a>.randint/.randint_like</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randint(low=<span class="number">0</span>, high=<span class="number">10</span>, size=(<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5, 9, 4],</span></span><br><span class="line"><span class="string">        [8, 3, 3],</span></span><br><span class="line"><span class="string">        [1, 1, 9]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randint_like(<span class="built_in">input</span>, high=<span class="number">10</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2., 8., 9.],</span></span><br><span class="line"><span class="string">        [6., 3., 3.],</span></span><br><span class="line"><span class="string">        [0., 2., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.randint(low, high)</code>返回在区间<code>[low, high)</code>的随机数填充的张量。</p><h4 id="randperm"><a href="#randperm" class="headerlink" title=".randperm"></a>.randperm</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randperm(<span class="number">10</span>))  <span class="comment"># tensor([5, 6, 1, 2, 0, 8, 9, 3, 7, 4])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.randperm(n)</code>返回在区间<code>[0, n)</code>的随机排列整数。</p><h4 id="randn-randn-like"><a href="#randn-randn-like" class="headerlink" title=".randn/.randn_like"></a>.randn/.randn_like</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准正态分布 N(0, 1)，均值为 0，方差为 1</span></span><br><span class="line"><span class="built_in">print</span>(torch.randn(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0.6614,  0.2669,  0.0617],</span></span><br><span class="line"><span class="string">        [ 0.6213, -0.4519, -0.1661],</span></span><br><span class="line"><span class="string">        [-1.5228,  0.3817, -1.0276]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randn_like(<span class="built_in">input</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-0.5631, -0.8923, -0.0583],</span></span><br><span class="line"><span class="string">        [-0.1955, -0.9656,  0.4224],</span></span><br><span class="line"><span class="string">        [ 0.2673, -0.4212, -0.5107]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.randn()</code>从<code>标准正态分布</code>中随机采样。</p><h4 id="normal"><a href="#normal" class="headerlink" title=".normal"></a>.normal</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 离散正态分布 N(mean, std)</span></span><br><span class="line"><span class="built_in">print</span>(torch.normal(mean=torch.full((<span class="number">5</span>,), <span class="number">0.</span>), std=torch.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.2</span>)))  <span class="comment"># tensor([ 0.0000,  0.0534,  0.0247,  0.3728, -0.3615])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.normal(mean, std)</code>从给定参数<code>mean</code>、<code>std</code>的<code>离散正态分布</code>中随机采样。</p><h4 id="bernoulli"><a href="#bernoulli" class="headerlink" title=".bernoulli"></a>.bernoulli</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># generate a uniform random matrix with range [0, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.bernoulli(<span class="built_in">input</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 0., 0.],</span></span><br><span class="line"><span class="string">        [1., 0., 1.],</span></span><br><span class="line"><span class="string">        [0., 1., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.bernoulli(torch.ones(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># probability of drawing "1" is 1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.bernoulli(torch.zeros(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># probability of drawing "1" is 0</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.bernoulli()</code>从<code>伯努利分布</code>中抽取二进制随机数（0 或 1）。输入的值必须在<code>[0, 1]</code>范围内。</p><h4 id="poisson"><a href="#poisson" class="headerlink" title=".poisson"></a>.poisson</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">rates = torch.rand(<span class="number">3</span>, <span class="number">3</span>) * <span class="number">5</span>  <span class="comment"># rate parameter between 0 and 5</span></span><br><span class="line"><span class="built_in">print</span>(torch.poisson(rates))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5., 1., 1.],</span></span><br><span class="line"><span class="string">        [3., 0., 2.],</span></span><br><span class="line"><span class="string">        [0., 2., 0.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.poisson()</code>从<code>泊松分布</code>中随机采样。</p><h4 id="multinomial"><a href="#multinomial" class="headerlink" title=".multinomial"></a>.multinomial</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">weights = torch.tensor([[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.multinomial(weights, <span class="number">5</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2, 3, 4, 1, 0],</span></span><br><span class="line"><span class="string">        [3, 2, 1, 4, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replacement 默认为 False，表示不允许重复抽查，所以采样次数不能大于抽查个数</span></span><br><span class="line"><span class="comment"># print(torch.multinomial(weights, 6))  # RuntimeError: cannot sample n_sample &gt; prob_dist.size(-1) samples without replacement</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replacement=True 允许重复抽查</span></span><br><span class="line"><span class="built_in">print</span>(torch.multinomial(weights, <span class="number">6</span>, replacement=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[3, 0, 3, 3, 4, 2],</span></span><br><span class="line"><span class="string">        [0, 1, 4, 4, 2, 4]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.multinomial(input, num_samples, replacement)</code>对<code>input</code>的每一行做从<code>多项式分布</code>中采样<code>num_samples</code>次，输出的张量是每一次取值时<code>input</code>张量对应行的下标。</p><h2 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h2><h3 id="Python-语法"><a href="#Python-语法" class="headerlink" title="Python 语法"></a>Python 语法</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第一张图片</span></span><br><span class="line"><span class="built_in">print</span>(images[<span class="number">0</span>].shape)  <span class="comment"># torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取第一张图片的第一个通道数据</span></span><br><span class="line"><span class="built_in">print</span>(images[<span class="number">0</span>, <span class="number">0</span>].shape)  <span class="comment"># torch.Size([28, 28])</span></span><br><span class="line"><span class="comment"># 获取前两张图片</span></span><br><span class="line"><span class="built_in">print</span>(images[:<span class="number">2</span>].shape)  <span class="comment"># torch.Size([2, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取前两张图片前两个通道数据</span></span><br><span class="line"><span class="built_in">print</span>(images[:<span class="number">2</span>, :<span class="number">2</span>].shape)  <span class="comment"># torch.Size([2, 2, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取前两张图片最后一个通道数据</span></span><br><span class="line"><span class="built_in">print</span>(images[:<span class="number">2</span>, -<span class="number">1</span>].shape)  <span class="comment"># torch.Size([2, 28, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取图片数据</span></span><br><span class="line"><span class="built_in">print</span>(images[:, :, ::<span class="number">2</span>, ::<span class="number">2</span>].shape)  <span class="comment"># torch.Size([4, 3, 14, 14])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有图片</span></span><br><span class="line"><span class="built_in">print</span>(images[...].shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取第一张图片</span></span><br><span class="line"><span class="built_in">print</span>(images[<span class="number">0</span>, ...].shape)  <span class="comment"># torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取图片宽数据</span></span><br><span class="line"><span class="built_in">print</span>(images[..., ::<span class="number">2</span>].shape)  <span class="comment"># torch.Size([4, 3, 28, 14])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="narrow-narrow-copy"><a href="#narrow-narrow-copy" class="headerlink" title=".narrow/.narrow_copy"></a>.narrow/.narrow_copy</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span> ,<span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 1,  2,  3,  4,  5],</span></span><br><span class="line"><span class="string">        [ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="string">        [11, 12, 13, 14, 15]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.narrow(data, dim=<span class="number">0</span>, start=<span class="number">1</span>, length=<span class="number">2</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="string">        [11, 12, 13, 14, 15]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.narrow(data, dim=<span class="number">1</span>, start=<span class="number">1</span>, length=<span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 2,  3,  4],</span></span><br><span class="line"><span class="string">        [ 7,  8,  9],</span></span><br><span class="line"><span class="string">        [12, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.narrow_copy(data, dim=<span class="number">0</span>, start=<span class="number">1</span>, length=<span class="number">2</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="string">        [11, 12, 13, 14, 15]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.narrow_copy(data, dim=<span class="number">1</span>, start=<span class="number">1</span>, length=<span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 2,  3,  4],</span></span><br><span class="line"><span class="string">        [ 7,  8,  9],</span></span><br><span class="line"><span class="string">        [12, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.narrow()</code>在指定维度缩小张量，可以简单理解为类似切片，<code>tensor[start: start + length]</code></p><p><code>torch.narrow_copy()</code>与<code>torch.narrow()</code>相同，但返回的是副本而不是共享存储。</p><h3 id="select-index-select"><a href="#select-index-select" class="headerlink" title=".select/.index_select"></a>.select/.index_select</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第一张图片</span></span><br><span class="line"><span class="built_in">print</span>(torch.select(images, dim=<span class="number">0</span>, index=<span class="number">0</span>).shape)  <span class="comment"># torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取所有图片的第一个通道</span></span><br><span class="line"><span class="built_in">print</span>(torch.select(images, dim=<span class="number">1</span>, index=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第二张和第四张图片</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">0</span>, index=torch.tensor([<span class="number">1</span>, <span class="number">3</span>])).shape)  <span class="comment"># torch.Size([2, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取所有图片二三通道数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">1</span>, index=torch.tensor([<span class="number">1</span>, <span class="number">2</span>])).shape)  <span class="comment"># torch.Size([4, 2, 28, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取所有图片高数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">2</span>, index=torch.arange(<span class="number">0</span>, <span class="number">28</span>, <span class="number">2</span>)).shape)  <span class="comment"># torch.Size([4, 3, 14, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取所有图片宽数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">3</span>, index=torch.arange(<span class="number">0</span>, <span class="number">28</span>, <span class="number">2</span>)).shape)  <span class="comment"># torch.Size([4, 3, 28, 14])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.select()</code>沿着维度<code>dim</code>，在给定索引<code>index</code>对<code>input</code>张量进行切片，等价于切片。比如：<br><code>tensor.select(0, index)</code>等于<code>tensor[index]</code>。<br><code>tensor.select(2, index)</code>等于<code>tensor[:,:,index]</code>。</p><p><code>torch.index_select()</code>沿着维度<code>dim</code>对<code>input</code>张量进行索引。</p><h3 id="masked-select"><a href="#masked-select" class="headerlink" title=".masked_select"></a>.masked_select</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0.6614,  0.2669,  0.0617,  0.6213],</span></span><br><span class="line"><span class="string">        [-0.4519, -0.1661, -1.5228,  0.3817],</span></span><br><span class="line"><span class="string">        [-1.0276, -0.5631, -0.8923, -0.0583]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">mask = data.ge(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(mask)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False, False,  True],</span></span><br><span class="line"><span class="string">        [False, False, False, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.masked_select(data, mask))  <span class="comment"># tensor([0.6614, 0.2669, 0.0617, 0.6213, 0.3817])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.masked_select()</code>根据布尔掩码选择数据，返回的是一维数据。</p><h3 id="gather"><a href="#gather" class="headerlink" title=".gather"></a>.gather</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">16</span>).view(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 5, 15,  6,  4],</span></span><br><span class="line"><span class="string">        [11,  2,  7, 12],</span></span><br><span class="line"><span class="string">        [ 1,  0,  9,  8],</span></span><br><span class="line"><span class="string">        [10,  3, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">index = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(index)  <span class="comment"># tensor([[1, 0, 3, 2]])</span></span><br><span class="line"><span class="built_in">print</span>(index.t())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1],</span></span><br><span class="line"><span class="string">        [0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">0</span>, index))  <span class="comment"># tensor([[11, 15, 13,  8]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">0</span>, index.t()))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[11],</span></span><br><span class="line"><span class="string">        [ 5],</span></span><br><span class="line"><span class="string">        [10],</span></span><br><span class="line"><span class="string">        [ 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">1</span>, index))  <span class="comment"># tensor([[15,  5,  4,  6]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">1</span>, index.t()))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15],</span></span><br><span class="line"><span class="string">        [11],</span></span><br><span class="line"><span class="string">        [ 8],</span></span><br><span class="line"><span class="string">        [13]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><ul><li><p><code>torch.gather(data, 0, index)</code>对<code>0</code>维(行)进行</p><pre><code>  tensor([[1, 0, 3, 2]])
        第 0  1  2  3 列
</code></pre><p>索引：<br>[<b>1</b>][0] == 11<br>[<b>0</b>][1] == 15<br>[<b>3</b>][2] == 13<br>[<b>2</b>][3] == 8<br>加粗的是<code>行索引</code>的值<code>1, 0, 3, 2</code>，没加粗的为什么是<code>0, 1, 2, 3</code>呢？因为索引<code>1</code>是第<code>0</code>列，索引<code>0</code>是第<code>1</code>列，索引<code>3</code>是第<code>2</code>列，索引<code>2</code>是第<code>3</code>列。</p></li><li><p><code>torch.gather(data, 0, index.t())</code>对<code>0</code>维(行)进行</p><pre><code>  tensor([[1],   第0列
          [0],   第0列
          [3],   第0列
          [2]])  第0列
</code></pre><p>索引：<br>[<b>1</b>][0] == 11<br>[<b>0</b>][0] == 5<br>[<b>3</b>][0] == 10<br>[<b>2</b>][0] == 1<br>加粗的是<code>行索引</code>的值<code>1, 0, 3, 2</code>，没加粗的都是<code>0</code>，因为索引<code>1, 0, 3, 2</code>都是第<code>0</code>列。</p></li><li><p><code>torch.gather(data, 1, index.t())</code>对<code>1</code>维(列)进行</p><pre><code>  tensor([[1],   第0行
          [0],   第1行
          [3],   第2行
          [2]])  第3行
</code></pre><p>索引：<br>[0][<b>1</b>] == 15<br>[1][<b>0</b>] == 11<br>[2][<b>3</b>] == 8<br>[3][<b>2</b>] == 13<br>加粗的是<code>列索引</code>的值<code>1, 0, 3, 2</code>，没加粗的<code>0, 1, 2, 3</code>则是因为索引<code>1</code>是第<code>0</code>行，索引<code>0</code>是第<code>1</code>行，索引<code>3</code>是第<code>2</code>行，索引<code>2</code>是第<code>3</code>行。</p></li></ul><h3 id="take-take-along-dim"><a href="#take-take-along-dim" class="headerlink" title=".take/.take_along_dim"></a>.take/.take_along_dim</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">16</span>).view(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 5, 15,  6,  4],</span></span><br><span class="line"><span class="string">        [11,  2,  7, 12],</span></span><br><span class="line"><span class="string">        [ 1,  0,  9,  8],</span></span><br><span class="line"><span class="string">        [10,  3, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">index1 = torch.tensor([<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>])</span><br><span class="line">index2 = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.take(data, index1))  <span class="comment"># tensor([ 5,  2,  9, 14])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index1))  <span class="comment"># tensor([ 5,  2,  9, 14])</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2, dim=<span class="number">0</span>))  <span class="comment"># tensor([[11, 15, 13,  8]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2.t(), dim=<span class="number">0</span>))  <span class="comment"># 这里与torch.gather不同</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[11,  2,  7, 12],</span></span><br><span class="line"><span class="string">        [ 5, 15,  6,  4],</span></span><br><span class="line"><span class="string">        [10,  3, 13, 14],</span></span><br><span class="line"><span class="string">        [ 1,  0,  9,  8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2, dim=<span class="number">1</span>))  <span class="comment"># 这里与torch.gather不同</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15,  5,  4,  6],</span></span><br><span class="line"><span class="string">        [ 2, 11, 12,  7],</span></span><br><span class="line"><span class="string">        [ 0,  1,  8,  9],</span></span><br><span class="line"><span class="string">        [ 3, 10, 14, 13]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2.t(), dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15],</span></span><br><span class="line"><span class="string">        [11],</span></span><br><span class="line"><span class="string">        [ 8],</span></span><br><span class="line"><span class="string">        [13]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.take_along_dim()</code>当<code>dim=None</code>时，等价于<code>torch.take()</code>，先把张量打平转成<code>1</code>维在根据索引获取元素。</p><p>当<code>dim</code>不等于<code>None</code>时，则与<code>data.gather()</code>相似。</p><h3 id="argwhere-nonzero"><a href="#argwhere-nonzero" class="headerlink" title=".argwhere/.nonzero"></a>.argwhere/.nonzero</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">data2 = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="built_in">print</span>(data1)  <span class="comment"># tensor([1, 0, 1, 0, 1, 0])</span></span><br><span class="line"><span class="built_in">print</span>(data2)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1, 0, 1],</span></span><br><span class="line"><span class="string">        [0, 1, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.argwhere(data1))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0],</span></span><br><span class="line"><span class="string">        [2],</span></span><br><span class="line"><span class="string">        [4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.argwhere(data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 0],</span></span><br><span class="line"><span class="string">        [0, 2],</span></span><br><span class="line"><span class="string">        [1, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data1))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0],</span></span><br><span class="line"><span class="string">        [2],</span></span><br><span class="line"><span class="string">        [4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data1, as_tuple=<span class="literal">True</span>))  <span class="comment"># (tensor([0, 2, 4]),)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 0],</span></span><br><span class="line"><span class="string">        [0, 2],</span></span><br><span class="line"><span class="string">        [1, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data2, as_tuple=<span class="literal">True</span>))  <span class="comment"># (tensor([0, 0, 1]), tensor([0, 2, 1]))</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.argwhere()</code>和<code>torch.nonzero()</code>都是返回非<code>0</code>元素的索引。</p><p>当<code>torch.nonzero()</code>参数<code>as_tuple=False</code>时，效果与<code>torch.argwhere()</code>相同。</p><h3 id="where"><a href="#where" class="headerlink" title=".where"></a>.where</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">condition = torch.randn(<span class="number">3</span>, <span class="number">5</span>) &gt; <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(condition)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True,  True,  True,  True, False],</span></span><br><span class="line"><span class="string">        [False, False,  True, False, False],</span></span><br><span class="line"><span class="string">        [False, False, False, False,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data1 = torch.ones(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data2 = torch.full_like(data1, <span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(data2)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[100., 100., 100., 100., 100.],</span></span><br><span class="line"><span class="string">        [100., 100., 100., 100., 100.],</span></span><br><span class="line"><span class="string">        [100., 100., 100., 100., 100.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 满足条件返回 data1, 不满足条件返回 data2</span></span><br><span class="line"><span class="built_in">print</span>(torch.where(condition, data1, data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[  1.,   1.,   1.,   1., 100.],</span></span><br><span class="line"><span class="string">        [100., 100.,   1., 100., 100.],</span></span><br><span class="line"><span class="string">        [100., 100., 100., 100.,   1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="unravel-index"><a href="#unravel-index" class="headerlink" title=".unravel_index"></a>.unravel_index</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">9</span>, <span class="number">18</span>).view(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 9, 10, 11],</span></span><br><span class="line"><span class="string">        [12, 13, 14],</span></span><br><span class="line"><span class="string">        [15, 16, 17]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.unravel_index(torch.tensor(<span class="number">2</span>), shape=(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># (tensor(0), tensor(2))</span></span><br><span class="line"><span class="built_in">print</span>(torch.unravel_index(torch.tensor([<span class="number">4</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">17</span>]), shape=(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># (tensor([1, 2, 0, 2]), tensor([1, 0, 0, 2]))</span></span><br></pre></td></tr></tbody></table></figure><p>索引<code>2</code>在<code>shape</code>为<code>(3, 3)</code>的<code>0</code>行<code>2</code>列。<br>索引<code>9</code>可以理解为<code>9 / prod(shape) == 9 % 9 == 0</code>，所以在<code>0</code>行<code>0</code>列。<br>索引<code>17</code>可以理解为<code>17 / prod(shape) == 17 % 9 == 8</code>，所以在<code>2</code>行<code>2</code>列。</p><h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><h3 id="t-transpose-movedim-permute"><a href="#t-transpose-movedim-permute" class="headerlink" title=".t/.transpose/.movedim/.permute"></a>.t/.transpose/.movedim/.permute</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(data)  <span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])</span></span><br><span class="line"><span class="built_in">print</span>(torch.t(data))  <span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])</span></span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.t(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 3, 6],</span></span><br><span class="line"><span class="string">        [1, 4, 7],</span></span><br><span class="line"><span class="string">        [2, 5, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 1080，宽 1920 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">1080</span>, <span class="number">1920</span>)</span><br><span class="line"><span class="comment"># BCHW</span></span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 1080, 1920])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BCHW --&gt; BWHC --&gt; BHWC</span></span><br><span class="line"><span class="built_in">print</span>(images.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).shape)  <span class="comment"># torch.Size([4, 1080, 1920, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BCHW --&gt; BHWC</span></span><br><span class="line"><span class="built_in">print</span>(images.movedim(<span class="number">1</span>, <span class="number">3</span>).shape)  <span class="comment"># torch.Size([4, 1080, 1920, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BCHW --&gt; BHWC</span></span><br><span class="line"><span class="built_in">print</span>(images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 1080, 1920, 3])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.t(input)</code>只能处理维度小于等于<code>2</code>的，否则会报错。当维度是<code>0</code>或<code>1</code>维时，返回相同的结果，当维度为<code>2</code>时，等价于<code>torch.transpose(input, 0, 1)</code></p><p><code>torch.transpose(input, dim0, dim1)</code>一次只能操作两个维度，对调两个维度的位置。</p><p><code>torch.movedim(input, source, destination)</code>将<code>source</code>维度移动到<code>destination</code>维度。</p><p><code>torch.permute(input, dims)</code>一次可以操作多个维度，<code>dims</code>指定所有维度的顺序。在多维度操作上使用<code>torch.permute()</code>更直观。</p><p><code>torch.swapaxes()</code>是<code>torch.transpose()</code>的别名。<br><code>torch.swapdims()</code>是<code>torch.transpose()</code>的别名。<br><code>torch.moveaxis()</code>是<code>torch.movedim()</code>的别名。</p><h3 id="view-reshape"><a href="#view-reshape" class="headerlink" title=".view/.reshape"></a>.view/.reshape</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(images.view(<span class="number">4</span>, <span class="number">3</span> * <span class="number">28</span> * <span class="number">28</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># -1 会自动计算出数值</span></span><br><span class="line"><span class="built_in">print</span>(images.view(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># print(images.transpose(1, 3).transpose(1, 2).view(4, -1).shape)  # 报错</span></span><br><span class="line"><span class="built_in">print</span>(images.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># print(images.permute(0, 2, 3, 1).view(4, -1).shape)  # 报错</span></span><br><span class="line"><span class="built_in">print</span>(images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous().view(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(images.reshape(<span class="number">4</span>, <span class="number">3</span> * <span class="number">28</span> * <span class="number">28</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># -1 会自动计算出数值</span></span><br><span class="line"><span class="built_in">print</span>(images.reshape(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="built_in">print</span>(images.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="built_in">print</span>(images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br></pre></td></tr></tbody></table></figure><p><code>view()</code>和<code>reshape()</code>都可以改变<code>Tensor</code>的维度，区别是：</p><p><code>view()</code>只能对满足连续性的张量进行转换，当对不满足连续性的张量进行操作时会报<code>RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</code>错误。<code>transpose()</code>和<code>permute()</code>会改变张量连续性，使用<code>view()</code>前需要先执行<code>contiguous()</code>。</p><p><code>reshape()</code>则没有上述要求，可以直接使用，无需先执行<code>contiguous()</code>。</p><h3 id="squeeze-unsqueeze"><a href="#squeeze-unsqueeze" class="headerlink" title=".squeeze/.unsqueeze"></a>.squeeze/.unsqueeze</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(data.shape)  <span class="comment"># torch.Size([2, 1, 2, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim=None，会把能挤压的维度挤压</span></span><br><span class="line"><span class="built_in">print</span>(torch.squeeze(data).shape)  <span class="comment"># torch.Size([2, 2])</span></span><br><span class="line"><span class="comment"># 第一维度是 2 没法挤压，所以保持不变</span></span><br><span class="line"><span class="built_in">print</span>(torch.squeeze(data, dim=<span class="number">0</span>).shape)  <span class="comment"># torch.Size([2, 1, 2, 1])</span></span><br><span class="line"><span class="comment"># 第二维度挤压</span></span><br><span class="line"><span class="built_in">print</span>(torch.squeeze(data, dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([2, 2, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第一维增加维度</span></span><br><span class="line"><span class="built_in">print</span>(torch.unsqueeze(data, dim=<span class="number">0</span>).shape)  <span class="comment"># torch.Size([1, 2, 1, 2, 1])</span></span><br><span class="line"><span class="comment"># 在最后一维增加维度</span></span><br><span class="line"><span class="built_in">print</span>(torch.unsqueeze(data, dim=-<span class="number">1</span>).shape)  <span class="comment"># torch.Size([2, 1, 2, 1, 1])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="expand-repeat"><a href="#expand-repeat" class="headerlink" title=".expand/.repeat"></a>.expand/.repeat</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张单通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">data = torch.empty(<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二维度扩展到 3 通道，-1 表示不扩展保持原样</span></span><br><span class="line"><span class="built_in">print</span>(images.expand(-<span class="number">1</span>, <span class="number">3</span>, -<span class="number">1</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="built_in">print</span>(images.expand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>).shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 扩展到和 images 相同的 shape</span></span><br><span class="line"><span class="built_in">print</span>(data.expand_as(images).shape)  <span class="comment"># torch.Size([4, 1, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 各维度重复指定次数的数据</span></span><br><span class="line"><span class="built_in">print</span>(images.repeat(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="tile"><a href="#tile" class="headerlink" title=".tile"></a>.tile</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果 dims 指定的维度小于 input 的维度，会扩展到相同的维度，比如 (2,) 会扩展到 (1, 2)</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">2</span>,)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">        [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">        [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 如果 dims 指定的维度大于 input 的维度，input 会扩展到相同的维度，比如 input 是 (3, 3) 会扩展到 (1, 3, 3)</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">         [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">         [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">         [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">         [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">         [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">         [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">         [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">         [6, 7, 8, 6, 7, 8]]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h2 id="合并与拆分"><a href="#合并与拆分" class="headerlink" title="合并与拆分"></a>合并与拆分</h2><h3 id="cat"><a href="#cat" class="headerlink" title=".cat"></a>.cat</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data1 = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">data2 = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 行合并</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat([data1, data1]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999],</span></span><br><span class="line"><span class="string">        [0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 列合并</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat([data1, data1], dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999, 0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.concat()</code>是<code>torch.cat()</code>的别名。<br><code>torch.concatenate()</code>是<code>torch.cat()</code>的别名。</p><h3 id="stack-hstack-vstack-column-stack-dstack"><a href="#stack-hstack-vstack-column-stack-dstack" class="headerlink" title=".stack/.hstack/.vstack/.column_stack/.dstack"></a>.stack/.hstack/.vstack/.column_stack/.dstack</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor(<span class="number">9</span>)</span><br><span class="line">data1 = torch.arange(<span class="number">9</span>)</span><br><span class="line">data2 = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data3 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data4 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data5 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.shape)  <span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(data1.shape)  <span class="comment"># torch.Size([9])</span></span><br><span class="line"><span class="built_in">print</span>(data2.shape)  <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(data3.shape)  <span class="comment"># torch.Size([1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(data4.shape)  <span class="comment"># torch.Size([1, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(data5.shape)  <span class="comment"># torch.Size([1, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim 默认为 0，在第一维度堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data, data]).shape)  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data1, data1]).shape)  <span class="comment"># torch.Size([2, 9])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data2, data2]).shape)  <span class="comment"># torch.Size([2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data3, data3]).shape)  <span class="comment"># torch.Size([2, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data4, data4]).shape)  <span class="comment"># torch.Size([2, 1, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data5, data5]).shape)  <span class="comment"># torch.Size([2, 1, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim=1，在第二维度堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data1, data1], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([9, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data2, data2], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([3, 2, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data3, data3], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([1, 2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data4, data4], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([1, 2, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data5, data5], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([1, 2, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 水平堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data, data]).shape)  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data1, data1]).shape)  <span class="comment"># torch.Size([18])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data2, data2]).shape)  <span class="comment"># torch.Size([3, 6])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data3, data3]).shape)  <span class="comment"># torch.Size([1, 6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data4, data4]).shape)  <span class="comment"># torch.Size([1, 2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data5, data5]).shape)  <span class="comment"># torch.Size([1, 2, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 垂直堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data, data]).shape)  <span class="comment"># torch.Size([2, 1])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data1, data1]).shape)  <span class="comment"># torch.Size([2, 9])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data2, data2]).shape)  <span class="comment"># torch.Size([6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data3, data3]).shape)  <span class="comment"># torch.Size([2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data4, data4]).shape)  <span class="comment"># torch.Size([2, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data5, data5]).shape)  <span class="comment"># torch.Size([2, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 列堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data, data]).shape)  <span class="comment"># torch.Size([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data1, data1]).shape)  <span class="comment"># torch.Size([9, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data2, data2]).shape)  <span class="comment"># torch.Size([3, 6])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data3, data3]).shape)  <span class="comment"># torch.Size([1, 6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data4, data4]).shape)  <span class="comment"># torch.Size([1, 2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data5, data5]).shape)  <span class="comment"># torch.Size([1, 2, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度堆叠，在第 3 维度堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data, data)).shape)  <span class="comment"># torch.Size([1, 1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data1, data1)).shape)  <span class="comment"># torch.Size([1, 9, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data2, data2)).shape)  <span class="comment"># torch.Size([3, 3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data3, data3)).shape)  <span class="comment"># torch.Size([1, 3, 6])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data4, data4)).shape)  <span class="comment"># torch.Size([1, 1, 6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data5, data5)).shape)  <span class="comment"># torch.Size([1, 1, 2, 3, 3])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.hstack)</code>按水平方向（列方向）依次堆叠张量。</p><p><code>torch.vstack)</code>按垂直方向（行方向）依次堆叠张量。</p><p><code>torch.column_stack()</code>除了张量是<code>0</code>维和<code>1</code>维外，等价于<code>torch.hstack()</code>。当张量<code>t</code>是<code>0</code>维或<code>1</code>维时，先<code>reshape</code>重塑为<code>(t.numel(), 1)</code>再水平堆叠。</p><p><code>torch.row_stack()</code>是<code>torch.vstack()</code>的别名。</p><p><code>torch.dstack()</code>在第<code>3</code>维度进行堆叠。</p><p><code>torch.stack()</code>是一个更通用的函数，通过<code>dim</code>指定在任意维度进行堆叠，<code>dim</code>默认等于<code>0</code>。它总是增加一个新的维度来堆叠张量。</p><h3 id="chunk"><a href="#chunk" class="headerlink" title=".chunk"></a>.chunk</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">20</span>).view(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0,  1,  2,  3,  4],</span></span><br><span class="line"><span class="string">        [ 5,  6,  7,  8,  9],</span></span><br><span class="line"><span class="string">        [10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="string">        [15, 16, 17, 18, 19]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按行分成 3 块（PS: 这里实际只分成两块，因为每块大小是 2，4 行只能分成两块）</span></span><br><span class="line"><span class="built_in">print</span>(torch.chunk(data, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0, 1, 2, 3, 4],</span></span><br><span class="line"><span class="string">        [5, 6, 7, 8, 9]]), tensor([[10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="string">        [15, 16, 17, 18, 19]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 按列分成 2 块</span></span><br><span class="line"><span class="built_in">print</span>(torch.chunk(data, <span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[ 0,  1,  2],</span></span><br><span class="line"><span class="string">        [ 5,  6,  7],</span></span><br><span class="line"><span class="string">        [10, 11, 12],</span></span><br><span class="line"><span class="string">        [15, 16, 17]]), tensor([[ 3,  4],</span></span><br><span class="line"><span class="string">        [ 8,  9],</span></span><br><span class="line"><span class="string">        [13, 14],</span></span><br><span class="line"><span class="string">        [18, 19]]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="split-hsplit-vsplit-dsplit-tensor-split"><a href="#split-hsplit-vsplit-dsplit-tensor-split" class="headerlink" title=".split/.hsplit/.vsplit/.dsplit/.tensor_split"></a>.split/.hsplit/.vsplit/.dsplit/.tensor_split</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.arange(<span class="number">9</span>)</span><br><span class="line">data2 = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data3 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按每两个一组分割</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data1, <span class="number">2</span>))  <span class="comment"># (tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6, 7]), tensor([8]))</span></span><br><span class="line"><span class="comment"># 分割成 3 组，第一组 1 份，第二组 3 份，第三组 5 份</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data1, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]))  <span class="comment"># (tensor([0]), tensor([1, 2, 3]), tensor([4, 5, 6, 7, 8]))</span></span><br><span class="line"><span class="comment"># 按每两个一组分割</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data2, <span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0, 1],</span></span><br><span class="line"><span class="string">        [3, 4],</span></span><br><span class="line"><span class="string">        [6, 7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 分割成 2 组，第一组 1 份，第二组 2 份</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data2, [<span class="number">1</span>, <span class="number">2</span>], dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [4, 5],</span></span><br><span class="line"><span class="string">        [7, 8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将具有一维或多维的张量 input 水平拆分为多个张量</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data1, <span class="number">3</span>))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]))</span></span><br><span class="line"><span class="comment"># 以索引 3、5、7 分成 4 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data1, [<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>]))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]), tensor([7, 8]))</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data2, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1],</span></span><br><span class="line"><span class="string">        [4],</span></span><br><span class="line"><span class="string">        [7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 以索引 1 分成 2 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data2, [<span class="number">1</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [4, 5],</span></span><br><span class="line"><span class="string">        [7, 8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将具有二维或多维的张量 input 垂直拆分为多个张量</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.vsplit(data2, <span class="number">3</span>))  <span class="comment"># (tensor([[0, 1, 2]]), tensor([[3, 4, 5]]), tensor([[6, 7, 8]]))</span></span><br><span class="line"><span class="comment"># 以索引 2 分成 2 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.vsplit(data2, [<span class="number">2</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5]]), tensor([[6, 7, 8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将具有三个或更多维度的张量 input 在深度方向上拆分为多个张量</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.dsplit(data3, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[[0],</span></span><br><span class="line"><span class="string">         [3],</span></span><br><span class="line"><span class="string">         [6]]]), tensor([[[1],</span></span><br><span class="line"><span class="string">         [4],</span></span><br><span class="line"><span class="string">         [7]]]), tensor([[[2],</span></span><br><span class="line"><span class="string">         [5],</span></span><br><span class="line"><span class="string">         [8]]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 以索引 1、2 分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.dsplit(data3, [<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[[0],</span></span><br><span class="line"><span class="string">         [3],</span></span><br><span class="line"><span class="string">         [6]]]), tensor([[[1],</span></span><br><span class="line"><span class="string">         [4],</span></span><br><span class="line"><span class="string">         [7]]]), tensor([[[2],</span></span><br><span class="line"><span class="string">         [5],</span></span><br><span class="line"><span class="string">         [8]]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.tensor_split(data1, <span class="number">3</span>))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]))</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor_split(data2, <span class="number">3</span>, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1],</span></span><br><span class="line"><span class="string">        [4],</span></span><br><span class="line"><span class="string">        [7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor_split(data3, <span class="number">3</span>, dim=<span class="number">2</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1],</span></span><br><span class="line"><span class="string">        [4],</span></span><br><span class="line"><span class="string">        [7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.hsplit(input, indices_or_sections)</code>当<code>input</code>是<code>1</code>维时，等价于<code>torch.tensor_split(input, indices_or_sections, dim=0)</code>，当<code>input</code>是大于等于<code>2</code>维时，等价于<code>torch.tensor_split(input, indices_or_sections, dim=1)</code>。<code>indices_or_sections</code>如果是数字，必须能被整除，否则会抛出异常。</p><p><code>torch.vsplit(input, indices_or_sections)</code>等价于<code>torch.tensor_split(input, indices_or_sections, dim=0)</code>。<code>input</code>必须大于等于<code>2</code>维。<code>indices_or_sections</code>如果是数字，必须能被整除，否则会抛出异常。</p><p><code>torch.dsplit(input, indices_or_sections)</code>等价于<code>torch.tensor_split(input, indices_or_sections, dim=2)</code>。<code>input</code>必须大于等于<code>3</code>维。<code>indices_or_sections</code>如果是数字，必须能被整除，否则会抛出异常。</p><h3 id="unbind"><a href="#unbind" class="headerlink" title=".unbind"></a>.unbind</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按行解绑</span></span><br><span class="line"><span class="built_in">print</span>(torch.unbind(data))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]))</span></span><br><span class="line"><span class="comment"># 按列解绑</span></span><br><span class="line"><span class="built_in">print</span>(torch.unbind(data, dim=<span class="number">1</span>))  <span class="comment"># (tensor([0, 3, 6]), tensor([1, 4, 7]), tensor([2, 5, 8]))</span></span><br></pre></td></tr></tbody></table></figure><h2 id="逐点运算"><a href="#逐点运算" class="headerlink" title="逐点运算"></a>逐点运算</h2><h3 id="add-sub-mul-div-remainder-fmod-positive-neg-abs"><a href="#add-sub-mul-div-remainder-fmod-positive-neg-abs" class="headerlink" title=".add/.sub/.mul/.div/.remainder/.fmod/.positive/.neg/.abs"></a>.add/.sub/.mul/.div/.remainder/.fmod/.positive/.neg/.abs</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randint(-<span class="number">10</span>, <span class="number">10</span>, (<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-5,  9, -6, -2, -7],</span></span><br><span class="line"><span class="string">        [ 3,  1, -9,  9,  2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加</span></span><br><span class="line"><span class="built_in">print</span>(torch.add(data, <span class="number">2</span>))  <span class="comment"># 等价于 data + 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-3, 11, -4,  0, -5],</span></span><br><span class="line"><span class="string">        [ 5,  3, -7, 11,  4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减</span></span><br><span class="line"><span class="built_in">print</span>(torch.sub(data, <span class="number">2</span>))  <span class="comment"># 等价于 data - 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ -7,   7,  -8,  -4,  -9],</span></span><br><span class="line"><span class="string">        [  1,  -1, -11,   7,   0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.mul(data, <span class="number">2</span>))  <span class="comment"># 等价于 data * 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-10,  18, -12,  -4, -14],</span></span><br><span class="line"><span class="string">        [  6,   2, -18,  18,   4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 除</span></span><br><span class="line"><span class="built_in">print</span>(torch.div(data, <span class="number">2</span>))  <span class="comment"># 等价于 data / 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-2.5000,  4.5000, -3.0000, -1.0000, -3.5000],</span></span><br><span class="line"><span class="string">        [ 1.5000,  0.5000, -4.5000,  4.5000,  1.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 除完向下取整</span></span><br><span class="line"><span class="built_in">print</span>(torch.div(data, <span class="number">2</span>, rounding_mode=<span class="string">'floor'</span>))  <span class="comment"># 等价于 data // 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-3,  4, -3, -1, -4],</span></span><br><span class="line"><span class="string">        [ 1,  0, -5,  4,  1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 除完只保留整数</span></span><br><span class="line"><span class="built_in">print</span>(torch.div(data, <span class="number">2</span>, rounding_mode=<span class="string">'trunc'</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-2,  4, -3, -1, -3],</span></span><br><span class="line"><span class="string">        [ 1,  0, -4,  4,  1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取余不保留负数</span></span><br><span class="line"><span class="built_in">print</span>(torch.remainder(data, <span class="number">2</span>))  <span class="comment"># 等价于 data % 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1, 1, 0, 0, 1],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 1, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取余保留负数</span></span><br><span class="line"><span class="built_in">print</span>(torch.fmod(data, <span class="number">2</span>))  <span class="comment"># 等价于 data % -2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-1,  1,  0,  0, -1],</span></span><br><span class="line"><span class="string">        [ 1,  1, -1,  1,  0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加号</span></span><br><span class="line"><span class="built_in">print</span>(torch.positive(data))  <span class="comment"># 等价于 +data</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-5,  9, -6, -2, -7],</span></span><br><span class="line"><span class="string">        [ 3,  1, -9,  9,  2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减号</span></span><br><span class="line"><span class="built_in">print</span>(torch.neg(data))  <span class="comment"># 等价于 -data</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 5, -9,  6,  2,  7],</span></span><br><span class="line"><span class="string">        [-3, -1,  9, -9, -2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绝对值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">abs</span>(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5, 9, 6, 2, 7],</span></span><br><span class="line"><span class="string">        [3, 1, 9, 9, 2]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.subtract()</code>是<code>torch.sub()</code>的别名。<br><code>torch.multiply()</code>是<code>torch.mul()</code>的别名。<br><code>torch.divide()</code>是<code>torch.div()</code>的别名。<br><code>torch.true_divide()</code>是<code>torch.div()</code>在<code>rounding_mode=None</code>时的别名。<br><code>torch.negative()</code>是<code>torch.neg()</code>的别名。<br><code>torch.absolute()</code>是<code>torch.abs()</code>的别名。</p><p>在<code>PyTorch 1.13</code>后（含1.13），可以认为<code>torch.floor_divide()</code>是<code>torch.div()</code>在<code>rounding_mode='floor'</code>时的别名，效果是一样的。</p><h3 id="pow-square-sqrt-rsqrt-reciprocal"><a href="#pow-square-sqrt-rsqrt-reciprocal" class="headerlink" title=".pow/.square/.sqrt/.rsqrt/.reciprocal"></a>.pow/.square/.sqrt/.rsqrt/.reciprocal</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[9, 9],</span></span><br><span class="line"><span class="string">        [9, 9]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 次方</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">pow</span>(data, <span class="number">3</span>))  <span class="comment"># 等价于 data ** 3</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[729, 729],</span></span><br><span class="line"><span class="string">        [729, 729]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方</span></span><br><span class="line"><span class="built_in">print</span>(torch.square(data))  <span class="comment"># 等价于 data ** 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[81, 81],</span></span><br><span class="line"><span class="string">        [81, 81]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开方</span></span><br><span class="line"><span class="built_in">print</span>(torch.sqrt(data))  <span class="comment"># 等价于 data ** 0.5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开方倒数</span></span><br><span class="line"><span class="built_in">print</span>(torch.rsqrt(data))  <span class="comment"># 等价于 data ** -0.5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.3333, 0.3333],</span></span><br><span class="line"><span class="string">        [0.3333, 0.3333]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 倒数</span></span><br><span class="line"><span class="built_in">print</span>(torch.reciprocal(data))  <span class="comment"># 等价于 data ** -1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.1111, 0.1111],</span></span><br><span class="line"><span class="string">        [0.1111, 0.1111]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="exp-log-log2-log10-log1p"><a href="#exp-log-log2-log10-log1p" class="headerlink" title=".exp/.log/.log2/.log10/.log1p"></a>.exp/.log/.log2/.log10/.log1p</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.exp(torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br><span class="line"><span class="built_in">print</span>(data1)  <span class="comment"># tensor([ 1.0000,  2.7183,  7.3891, 20.0855, 54.5981])</span></span><br><span class="line"></span><br><span class="line">data2 = torch.exp(torch.ones(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(data2)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2.7183, 2.7183],</span></span><br><span class="line"><span class="string">        [2.7183, 2.7183]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 e 为底</span></span><br><span class="line"><span class="built_in">print</span>(torch.log(data1))  <span class="comment"># tensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.log(data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1.0000, 1.0000],</span></span><br><span class="line"><span class="string">        [1.0000, 1.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data3 = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>])</span><br><span class="line"><span class="comment"># 以 2 为底</span></span><br><span class="line"><span class="built_in">print</span>(torch.log2(data3))  <span class="comment"># tensor([0., 1., 2., 3., 4.])</span></span><br><span class="line"></span><br><span class="line">data4 = torch.tensor([<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>, <span class="number">10000</span>])</span><br><span class="line"><span class="comment"># 以 10 为底</span></span><br><span class="line"><span class="built_in">print</span>(torch.log10(data4))  <span class="comment"># tensor([0., 1., 2., 3., 4.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 e 为底，input + 1 做为真数</span></span><br><span class="line"><span class="built_in">print</span>(torch.log1p(data1 - <span class="number">1</span>))  <span class="comment"># tensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.log1p(data2 - <span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1.0000, 1.0000],</span></span><br><span class="line"><span class="string">        [1.0000, 1.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.log1p()</code>计算<code>input + 1</code>的自然对数：<script type="math/tex">y_i = ln(x_i+1)</script>。</p><h3 id="sin-cos-tan-asin-acos-atan-atan2-sinh-cosh-tanh"><a href="#sin-cos-tan-asin-acos-atan-atan2-sinh-cosh-tanh" class="headerlink" title=".sin/.cos/.tan/.asin/.acos/.atan/.atan2/.sinh/.cosh/.tanh"></a>.sin/.cos/.tan/.asin/.acos/.atan/.atan2/.sinh/.cosh/.tanh</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.deg2rad(torch.arange(<span class="number">0</span>, <span class="number">361</span>, <span class="number">30</span>))  <span class="comment"># 角度转弧度</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.sin(data))</span><br><span class="line"><span class="comment"># 余弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.cos(data))</span><br><span class="line"><span class="comment"># 正切</span></span><br><span class="line"><span class="built_in">print</span>(torch.tan(data))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反正弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.asin(data))</span><br><span class="line"><span class="comment"># 反余弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.acos(data))</span><br><span class="line"><span class="comment"># 反正切</span></span><br><span class="line"><span class="built_in">print</span>(torch.atan(data))</span><br><span class="line"><span class="built_in">print</span>(torch.atan2(data, data))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 双曲正弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.sinh(data))</span><br><span class="line"><span class="comment"># 双曲余弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.cosh(data))</span><br><span class="line"><span class="comment"># 双曲正切</span></span><br><span class="line"><span class="built_in">print</span>(torch.tanh(data))</span><br></pre></td></tr></tbody></table></figure><p><code>torch.arcsin()</code>是<code>torch.asin()</code>的别名。<br><code>torch.arccos()</code>是<code>torch.acos()</code>的别名。<br><code>torch.arctan()</code>是<code>torch.atan()</code>的别名。<br><code>torch.arctan2()</code>是<code>torch.atan2()</code>的别名。<br><code>torch.arcsinh()</code>是<code>torch.asinh()</code>的别名。<br><code>torch.arccosh()</code>是<code>torch.acosh()</code>的别名。<br><code>torch.arctanh()</code>是<code>torch.atanh()</code>的别名。</p><h3 id="angle-deg2rad"><a href="#angle-deg2rad" class="headerlink" title=".angle/.deg2rad"></a>.angle/.deg2rad</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复数转弧度，结果在（π, -π）之间</span></span><br><span class="line"><span class="built_in">print</span>(torch.angle(torch.tensor([[<span class="number">1</span>, <span class="number">1</span> + <span class="number">1j</span>, <span class="number">1j</span>, -<span class="number">1</span> + <span class="number">1j</span>, -<span class="number">1</span>, -<span class="number">1</span> -<span class="number">1j</span>, -<span class="number">1j</span>, <span class="number">1</span> -<span class="number">1j</span>]])))  <span class="comment"># tensor([[ 0.0000,  0.7854,  1.5708,  2.3562,  3.1416, -2.3562, -1.5708, -0.7854]])</span></span><br><span class="line"><span class="comment"># 弧度转角度</span></span><br><span class="line"><span class="built_in">print</span>(torch.angle(torch.tensor([[<span class="number">1</span>, <span class="number">1</span> + <span class="number">1j</span>, <span class="number">1j</span>, -<span class="number">1</span> + <span class="number">1j</span>, -<span class="number">1</span>, -<span class="number">1</span> -<span class="number">1j</span>, -<span class="number">1j</span>, <span class="number">1</span> -<span class="number">1j</span>]])) * <span class="number">180</span> / np.pi)  <span class="comment"># tensor([[   0.0000,   45.0000,   90.0000,  135.0000,  180.0000, -135.0000, -90.0000,  -45.0000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 角度转弧度</span></span><br><span class="line"><span class="built_in">print</span>(torch.deg2rad(torch.tensor([[<span class="number">0</span>, <span class="number">45</span>, <span class="number">90</span>, <span class="number">135</span>, <span class="number">180</span>, <span class="number">360</span>], [-<span class="number">360</span>, -<span class="number">180</span>, -<span class="number">135</span>, -<span class="number">90</span>, -<span class="number">45</span>, <span class="number">0</span>]])))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0.0000,  0.7854,  1.5708,  2.3562,  3.1416,  6.2832],</span></span><br><span class="line"><span class="string">        [-6.2832, -3.1416, -2.3562, -1.5708, -0.7854,  0.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="bitwise-and-bitwise-or-bitwise-not-bitwise-xor-bitwise-left-shift-bitwise-right-shift"><a href="#bitwise-and-bitwise-or-bitwise-not-bitwise-xor-bitwise-left-shift-bitwise-right-shift" class="headerlink" title=".bitwise_and/.bitwise_or/.bitwise_not/.bitwise_xor/.bitwise_left_shift/.bitwise_right_shift"></a>.bitwise_and/.bitwise_or/.bitwise_not/.bitwise_xor/.bitwise_left_shift/.bitwise_right_shift</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位与</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_and(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([1, 0, 3], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_and(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([False,  True, False])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位或</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_or(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-1, -2,  3], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_or(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([ True,  True, False])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位非</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_not(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([ 0,  1, -4], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_not(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([False, False,  True])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位异或</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_xor(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-2, -2,  0], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_xor(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([ True, False, False])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 左移</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_left_shift(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-2, -2, 24], dtype=torch.int8)</span></span><br><span class="line"><span class="comment"># 右移</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_right_shift(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-1, -2,  0], dtype=torch.int8)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="floor-ceil-round"><a href="#floor-ceil-round" class="headerlink" title=".floor/.ceil/.round"></a>.floor/.ceil/.round</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">1</span>, <span class="number">5</span>) * <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(data)  <span class="comment"># tensor([[7.5763, 2.7931, 4.0307, 7.3468, 0.2928]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向下取整</span></span><br><span class="line"><span class="built_in">print</span>(torch.floor(data))  <span class="comment"># tensor([[7., 2., 4., 7., 0.]])</span></span><br><span class="line"><span class="comment"># 向上取整</span></span><br><span class="line"><span class="built_in">print</span>(torch.ceil(data))  <span class="comment"># tensor([[8., 3., 5., 8., 1.]])</span></span><br><span class="line"><span class="comment"># 四舍五入</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">round</span>(data))  <span class="comment"># tensor([[8., 3., 4., 7., 0.]])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.floor()</code>向下取整，<code>torch.ceil()</code>向上取整，<code>torch.round()</code>四舍五入。</p><h3 id="trunc-frac"><a href="#trunc-frac" class="headerlink" title=".trunc/.frac"></a>.trunc/.frac</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">1</span>, <span class="number">5</span>) * <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(data)  <span class="comment"># tensor([[7.5763, 2.7931, 4.0307, 7.3468, 0.2928]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取整数</span></span><br><span class="line"><span class="built_in">print</span>(torch.trunc(data))  <span class="comment"># tensor([[7., 2., 4., 7., 0.]])</span></span><br><span class="line"><span class="comment"># 取小数</span></span><br><span class="line"><span class="built_in">print</span>(torch.frac(data))  <span class="comment"># tensor([[0.5763, 0.7931, 0.0307, 0.3468, 0.2928]])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.trunc()</code>取整数，<code>torch.frac()</code>取小数。</p><p><code>torch.fix()</code>是<code>torch.trunc()</code>的别名。</p><h3 id="clamp-clamp-min-clamp-max"><a href="#clamp-clamp-min-clamp-max" class="headerlink" title=".clamp/.clamp_min/.clamp_max"></a>.clamp/.clamp_min/.clamp_max</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand((<span class="number">2</span>, <span class="number">5</span>)) * <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.1526,  5.5862,  8.0614, 14.6937,  0.5856],</span></span><br><span class="line"><span class="string">        [15.9972,  7.9427, 15.0874, 11.3902,  8.7756]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于 10 的数变为 10</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp(data, <span class="number">10</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.1526, 10.0000, 10.0000, 14.6937, 10.0000],</span></span><br><span class="line"><span class="string">        [15.9972, 10.0000, 15.0874, 11.3902, 10.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于 15 的数变为 15</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp(data, <span class="built_in">max</span>=<span class="number">15</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.0000,  5.5862,  8.0614, 14.6937,  0.5856],</span></span><br><span class="line"><span class="string">        [15.0000,  7.9427, 15.0000, 11.3902,  8.7756]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于 10 的数变为 10，大于 15 的数变为 15</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp(data, <span class="number">10</span>, <span class="number">15</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.0000, 10.0000, 10.0000, 14.6937, 10.0000],</span></span><br><span class="line"><span class="string">        [15.0000, 10.0000, 15.0000, 11.3902, 10.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于 10 的数变为 10</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp_min(data, <span class="number">10</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.1526, 10.0000, 10.0000, 14.6937, 10.0000],</span></span><br><span class="line"><span class="string">        [15.9972, 10.0000, 15.0874, 11.3902, 10.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于 15 的数变为 15</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp_max(data, <span class="number">15</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.0000,  5.5862,  8.0614, 14.6937,  0.5856],</span></span><br><span class="line"><span class="string">        [15.0000,  7.9427, 15.0000, 11.3902,  8.7756]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.clamp()</code>用于将输入的张量夹紧到区间<code>[min, max]</code>。</p><p><code>torch.clip()</code>是<code>torch.clamp()</code>的别名。</p><h3 id="dot-mm-bmm-matmul"><a href="#dot-mm-bmm-matmul" class="headerlink" title=".dot/.mm/.bmm/.matmul"></a>.dot/.mm/.bmm/.matmul</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一维矩阵相乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.dot(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.tensor([<span class="number">3</span>, <span class="number">4</span>])))  <span class="comment"># tensor(11)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维矩阵相乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.mm(torch.full((<span class="number">2</span>, <span class="number">3</span>), <span class="number">2</span>), torch.full((<span class="number">3</span>, <span class="number">2</span>), <span class="number">3</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[18, 18],</span></span><br><span class="line"><span class="string">        [18, 18]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 三维矩阵相乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.bmm(torch.full((<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="number">2</span>), torch.full((<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>), <span class="number">3</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[[18, 18],</span></span><br><span class="line"><span class="string">         [18, 18]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[18, 18],</span></span><br><span class="line"><span class="string">         [18, 18]]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data1 = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">800</span>, <span class="number">600</span>)</span><br><span class="line"><span class="built_in">print</span>(data1.shape)  <span class="comment"># torch.Size([4, 3, 800, 600])</span></span><br><span class="line">data2 = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">600</span>, <span class="number">400</span>)</span><br><span class="line"><span class="built_in">print</span>(data2.shape)  <span class="comment"># torch.Size([4, 3, 600, 400])</span></span><br><span class="line"><span class="comment"># data1 @ data2 等价于 data1.matmul(data2)</span></span><br><span class="line"><span class="built_in">print</span>(data1.matmul(data2).shape)  <span class="comment"># torch.Size([4, 3, 800, 400])</span></span><br></pre></td></tr></tbody></table></figure><p><code>dot()</code>只支持一维矩阵相乘，<code>mm()</code>只支持二维矩阵相乘，<code>bmm()</code>只支持三维矩阵相乘，<code>matmul()</code>支持任意维度矩阵相乘。</p><h2 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h2><h3 id="eq-ne-gt-ge-lt-le-equal"><a href="#eq-ne-gt-ge-lt-le-equal" class="headerlink" title=".eq/.ne/.gt/.ge/.lt/.le/.equal"></a>.eq/.ne/.gt/.ge/.lt/.le/.equal</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">10</span>).view(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5, 6, 1, 2, 0],</span></span><br><span class="line"><span class="string">        [8, 9, 3, 7, 4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.eq(data, <span class="number">5</span>))  <span class="comment"># 等价于 data == 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True, False, False, False, False],</span></span><br><span class="line"><span class="string">        [False, False, False, False, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.ne(data, <span class="number">5</span>))  <span class="comment"># 等价于 data != 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False,  True,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [ True,  True,  True,  True,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于</span></span><br><span class="line"><span class="built_in">print</span>(torch.gt(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &gt; 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False,  True, False, False, False],</span></span><br><span class="line"><span class="string">        [ True,  True, False,  True, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.ge(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &gt;= 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True,  True, False, False, False],</span></span><br><span class="line"><span class="string">        [ True,  True, False,  True, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于</span></span><br><span class="line"><span class="built_in">print</span>(torch.lt(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &lt; 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False, False,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False,  True, False,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.le(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &lt;= 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True, False,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False,  True, False,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较两个 tensor 是否相同</span></span><br><span class="line"><span class="built_in">print</span>(torch.equal(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.tensor([<span class="number">1</span>, <span class="number">2</span>])))  <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(torch.equal(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.tensor([<span class="number">2</span>, <span class="number">1</span>])))  <span class="comment"># False</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.not_equal()</code>是<code>torch.ne()</code>的别名。<br><code>torch.greater()</code>是<code>torch.gt()</code>的别名。<br><code>torch.greater_equal()</code>是<code>torch.ge()</code>的别名。<br><code>torch.less()</code>是<code>torch.lt()</code>的别名。<br><code>torch.less_equal()</code>是<code>torch.le()</code>的别名。</p><h3 id="isfinite-isinf-isposinf-isneginf-isnan-isreal-isin"><a href="#isfinite-isinf-isposinf-isneginf-isnan-isreal-isin" class="headerlink" title=".isfinite/.isinf/.isposinf/.isneginf/.isnan/.isreal/.isin"></a>.isfinite/.isinf/.isposinf/.isneginf/.isnan/.isreal/.isin</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([<span class="number">1</span>, <span class="built_in">float</span>(<span class="string">'inf'</span>), torch.inf, <span class="built_in">float</span>(<span class="string">'-inf'</span>), -torch.inf, <span class="built_in">float</span>(<span class="string">'nan'</span>), torch.nan, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"><span class="comment"># 是否不是无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isfinite(data))  <span class="comment"># tensor([ True, False, False, False, False, False, False,  True,  True])</span></span><br><span class="line"><span class="comment"># 是否是无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isinf(data))  <span class="comment"># tensor([False,  True,  True,  True,  True, False, False, False, False])</span></span><br><span class="line"><span class="comment"># 是否是正无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isposinf(data))  <span class="comment"># tensor([False,  True,  True, False, False, False, False, False, False])</span></span><br><span class="line"><span class="comment"># 是否是负无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isneginf(data))  <span class="comment"># tensor([False, False, False,  True,  True, False, False, False, False])</span></span><br><span class="line"><span class="comment"># 是否是 NaN</span></span><br><span class="line"><span class="built_in">print</span>(torch.isnan(data))  <span class="comment"># tensor([False, False, False, False, False,  True,  True, False, False])</span></span><br><span class="line"><span class="comment"># 是否是实数</span></span><br><span class="line"><span class="built_in">print</span>(torch.isreal(torch.tensor([<span class="number">1</span>, <span class="number">1</span>+<span class="number">1j</span>, <span class="number">2</span>+<span class="number">0j</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>), <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([ True, False,  True,  True,  True,  True])</span></span><br><span class="line"></span><br><span class="line">elements = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">test_elements = [<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="comment"># 对 elements 的每个元素进行判断是否在 test_elements 中</span></span><br><span class="line"><span class="built_in">print</span>(torch.isin(torch.tensor(elements), torch.tensor(test_elements)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False,  True],</span></span><br><span class="line"><span class="string">        [ True, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="isclose-allclose"><a href="#isclose-allclose" class="headerlink" title=".isclose/.allclose"></a>.isclose/.allclose</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否接近</span></span><br><span class="line"><span class="built_in">print</span>(torch.isclose(torch.tensor((<span class="number">1.</span>, <span class="number">2</span>, <span class="number">3</span>)), torch.tensor((<span class="number">1</span> + <span class="number">1e-10</span>, <span class="number">3</span>, <span class="number">4</span>))))  <span class="comment"># tensor([ True, False, False])</span></span><br><span class="line"><span class="built_in">print</span>(torch.isclose(torch.tensor((<span class="built_in">float</span>(<span class="string">'inf'</span>), <span class="number">4</span>)), torch.tensor((<span class="built_in">float</span>(<span class="string">'inf'</span>), <span class="number">6</span>)), rtol=<span class="number">.5</span>))  <span class="comment"># tensor([True, True])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否都接近</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">10000.</span>, <span class="number">1e-07</span>]), torch.tensor([<span class="number">10000.1</span>, <span class="number">1e-08</span>])))  <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">10000.</span>, <span class="number">1e-08</span>]), torch.tensor([<span class="number">10000.1</span>, <span class="number">1e-09</span>])))  <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)]), torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)])))  <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)]), torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)]), equal_nan=<span class="literal">True</span>))  <span class="comment"># True</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)</code></p><p><code>torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)</code></p><p>校验是否接近公式：<script type="math/tex">|input - other| \le atol + rtol \times |other|</script></p><h3 id="maximum-minimum-fmax-fmin"><a href="#maximum-minimum-fmax-fmin" class="headerlink" title=".maximum/.minimum/.fmax/.fmin"></a>.maximum/.minimum/.fmax/.fmin</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.tensor([<span class="number">9.7</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>), <span class="number">3.1</span>, torch.nan, <span class="number">11.1</span>])</span><br><span class="line">data2 = torch.tensor([-<span class="number">2.2</span>, <span class="number">0.5</span>, torch.nan, <span class="built_in">float</span>(<span class="string">'nan'</span>), <span class="number">7.8</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.maximum(data1, data2))  <span class="comment"># tensor([ 9.7000,     nan,     nan,     nan, 11.1000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.minimum(data1, data2))  <span class="comment"># tensor([-2.2000,     nan,     nan,     nan,  7.8000])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.fmax(data1, data2))  <span class="comment"># tensor([ 9.7000,  0.5000,  3.1000,     nan, 11.1000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.fmin(data1, data2))  <span class="comment"># tensor([-2.2000,  0.5000,  3.1000,     nan,  7.8000])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.maximum()</code>和<code>torch.minimum()</code>用于比较两数的大小，不支持比较<code>NaN</code>。</p><p><code>torch.fmax()</code>和<code>torch.fmin()</code>用于比较两数的大小，支持比较<code>NaN</code>。</p><h3 id="sort-msort-argsort"><a href="#sort-msort-argsort" class="headerlink" title=".sort/.msort/.argsort"></a>.sort/.msort/.argsort</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### dim 默认等于 -1，按列进行排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.sort(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.sort(</span></span><br><span class="line"><span class="string">values=tensor([[0.0293, 0.2793, 0.4031, 0.7347, 0.7576],</span></span><br><span class="line"><span class="string">        [0.3971, 0.4388, 0.5695, 0.7544, 0.7999]]),</span></span><br><span class="line"><span class="string">indices=tensor([[4, 1, 2, 3, 0],</span></span><br><span class="line"><span class="string">        [1, 4, 3, 2, 0]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 按行进行排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.sort(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.sort(</span></span><br><span class="line"><span class="string">values=tensor([[0.7576, 0.2793, 0.4031, 0.5695, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.3971, 0.7544, 0.7347, 0.4388]]),</span></span><br><span class="line"><span class="string">indices=tensor([[0, 0, 0, 1, 0],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 0, 1]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># descending=True 降序排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.sort(data, descending=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.sort(</span></span><br><span class="line"><span class="string">values=tensor([[0.7576, 0.7347, 0.4031, 0.2793, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.7544, 0.5695, 0.4388, 0.3971]]),</span></span><br><span class="line"><span class="string">indices=tensor([[0, 3, 2, 1, 4],</span></span><br><span class="line"><span class="string">        [0, 2, 3, 4, 1]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿第一维度进行排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.msort(data))  <span class="comment"># 等价于 torch.sort(data, dim=0)[0]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.5695, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.3971, 0.7544, 0.7347, 0.4388]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### dim 默认等于 -1，按列进行排序，返回索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argsort(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[4, 1, 2, 3, 0],</span></span><br><span class="line"><span class="string">        [1, 4, 3, 2, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 按行进行排序，返回索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argsort(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 0, 0, 1, 0],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 0, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># descending=True 降序排序，返回索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argsort(data, descending=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 3, 2, 1, 4],</span></span><br><span class="line"><span class="string">        [0, 2, 3, 4, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.sort()</code>排序后返回排序结果和索引。</p><p><code>torch.msort(input))</code>等价于<code>torch.sort()</code>对第一维度进行排序在取排序结果，<code>torch.sort(input, dim=0)[0]</code>，返回结果不包含索引。</p><p><code>torch.argsort()</code>返回排序后的索引。</p><h3 id="topk-kthvalue"><a href="#topk-kthvalue" class="headerlink" title=".topk/.kthvalue"></a>.topk/.kthvalue</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">30</span>, dtype=torch.double).view(<span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[25.,  4.,  6.,  8., 23., 18., 17., 20., 19., 12.],</span></span><br><span class="line"><span class="string">        [ 5., 14., 22.,  3., 27., 15.,  9., 13.,  7., 11.],</span></span><br><span class="line"><span class="string">        [10.,  2., 24., 29., 21., 26., 28.,  1., 16.,  0.]],</span></span><br><span class="line"><span class="string">       dtype=torch.float64)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取前 k 大的数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.topk(data, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.topk(</span></span><br><span class="line"><span class="string">values=tensor([[25., 23., 20.],</span></span><br><span class="line"><span class="string">        [27., 22., 15.],</span></span><br><span class="line"><span class="string">        [29., 28., 26.]], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([[0, 4, 7],</span></span><br><span class="line"><span class="string">        [4, 2, 5],</span></span><br><span class="line"><span class="string">        [3, 6, 5]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 获取前 k 小的数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.topk(data, <span class="number">3</span>, largest=<span class="literal">False</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.topk(</span></span><br><span class="line"><span class="string">values=tensor([[4., 6., 8.],</span></span><br><span class="line"><span class="string">        [3., 5., 7.],</span></span><br><span class="line"><span class="string">        [0., 1., 2.]], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([[1, 2, 3],</span></span><br><span class="line"><span class="string">        [3, 0, 8],</span></span><br><span class="line"><span class="string">        [9, 7, 1]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第 k 小的数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.kthvalue(data, <span class="number">8</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.kthvalue(</span></span><br><span class="line"><span class="string">values=tensor([20., 15., 26.], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([7, 5, 5]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.kthvalue(data, <span class="number">2</span>, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.kthvalue(</span></span><br><span class="line"><span class="string">values=tensor([10.,  4., 22.,  8., 23., 18., 17., 13., 16., 11.], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([2, 0, 1, 0, 0, 0, 0, 1, 2, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.topk()</code>获取前<code>k</code>大的数据，<code>torch.kthvalue()</code>获取第<code>k</code>小的数据。</p><h2 id="归约运算"><a href="#归约运算" class="headerlink" title="归约运算"></a>归约运算</h2><h3 id="max-min-amax-amin-aminmax-argmax-argmin"><a href="#max-min-amax-amin-aminmax-argmax-argmin" class="headerlink" title=".max/.min/.amax/.amin/.aminmax/.argmax/.argmin"></a>.max/.min/.amax/.amin/.aminmax/.argmax/.argmin</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.7347],</span></span><br><span class="line"><span class="string">        [0.0293, 0.7999, 0.3971, 0.7544],</span></span><br><span class="line"><span class="string">        [0.5695, 0.4388, 0.6387, 0.5247],</span></span><br><span class="line"><span class="string">        [0.6826, 0.3051, 0.4635, 0.4550]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(data))  <span class="comment"># tensor(0.7999)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.max(</span></span><br><span class="line"><span class="string">values=tensor([0.7576, 0.7999, 0.6387, 0.7544]),</span></span><br><span class="line"><span class="string">indices=tensor([0, 1, 2, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(data))  <span class="comment"># tensor(0.0293)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(data, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.min(</span></span><br><span class="line"><span class="string">values=tensor([0.2793, 0.0293, 0.4388, 0.3051]),</span></span><br><span class="line"><span class="string">indices=tensor([1, 0, 1, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值</span></span><br><span class="line"><span class="built_in">print</span>(torch.amax(data))  <span class="comment"># tensor(0.7999)</span></span><br><span class="line"><span class="built_in">print</span>(torch.amax(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([0.7576, 0.7999, 0.6387, 0.7544])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.amin(data))  <span class="comment"># tensor(0.0293)</span></span><br><span class="line"><span class="built_in">print</span>(torch.amin(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([0.2793, 0.0293, 0.4388, 0.3051])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值和最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.aminmax(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.aminmax(</span></span><br><span class="line"><span class="string">min=tensor(0.0293),</span></span><br><span class="line"><span class="string">max=tensor(0.7999))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.aminmax(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.aminmax(</span></span><br><span class="line"><span class="string">min=tensor([0.0293, 0.2793, 0.3971, 0.4550]),</span></span><br><span class="line"><span class="string">max=tensor([0.7576, 0.7999, 0.6387, 0.7544]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmax(data))  <span class="comment"># tensor(5)</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmax(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([0, 1, 2, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmin(data))  <span class="comment"># tensor(4)</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmin(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([1, 0, 1, 1])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.max()</code>和<code>torch.min()</code>指定了<code>dim</code>时，返回极值和索引。</p><p><code>torch.amax()</code>和<code>torch.amin()</code>只返回极值，不返回索引。</p><p><code>torch.argmax()</code>和<code>torch.argmin()</code>返回极值对应的索引。不指定<code>dim</code>时，返回的是打平后的索引。</p><h3 id="mean-nanmean"><a href="#mean-nanmean" class="headerlink" title=".mean/.nanmean"></a>.mean/.nanmean</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均值</span></span><br><span class="line"><span class="built_in">print</span>(torch.mean(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.mean(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([nan, 6., 7., 8., nan])</span></span><br><span class="line"><span class="built_in">print</span>(torch.mean(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([nan, 7., nan])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均值（忽略 NaN 值）</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmean(data))  <span class="comment"># tensor(7.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmean(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([7.5000, 6.0000, 7.0000, 8.0000, 6.5000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmean(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([ 2.5000,  7.0000, 11.5000])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="median-nanmedian"><a href="#median-nanmedian" class="headerlink" title=".median/.nanmedian"></a>.median/.nanmedian</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中位数</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.median(</span></span><br><span class="line"><span class="string">values=tensor([nan, 6., 7., 8., nan]),</span></span><br><span class="line"><span class="string">indices=tensor([0, 1, 1, 1, 2]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([nan, 7., nan])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.median(</span></span><br><span class="line"><span class="string">values=tensor([nan, 7., nan]),</span></span><br><span class="line"><span class="string">indices=tensor([0, 2, 4]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中位数（忽略 NaN 值）</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmedian(data))  <span class="comment"># tensor(7.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmedian(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.nanmedian(</span></span><br><span class="line"><span class="string">values=tensor([5., 6., 7., 8., 4.]),</span></span><br><span class="line"><span class="string">indices=tensor([1, 1, 1, 1, 0]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmedian(data, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.nanmedian(</span></span><br><span class="line"><span class="string">values=tensor([ 2.,  7., 11.]),</span></span><br><span class="line"><span class="string">indices=tensor([2, 2, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="sum-nansum"><a href="#sum-nansum" class="headerlink" title=".sum/.nansum"></a>.sum/.nansum</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([nan, 18., 21., 24., nan])</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([nan, 35., nan])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加（忽略 NaN 值）</span></span><br><span class="line"><span class="built_in">print</span>(torch.nansum(data))  <span class="comment"># tensor(91.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nansum(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([15., 18., 21., 24., 13.])</span></span><br><span class="line"><span class="built_in">print</span>(torch.nansum(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([10., 35., 46.])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="prod"><a href="#prod" class="headerlink" title=".prod"></a>.prod</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 累乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.prod(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.prod(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([ nan,  66., 168., 312.,  nan])</span></span><br><span class="line"><span class="built_in">print</span>(torch.prod(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([   nan, 15120.,    nan])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="var-var-mean-std-std-mean"><a href="#var-var-mean-std-std-mean" class="headerlink" title=".var/.var_mean/.std/.std_mean"></a>.var/.var_mean/.std/.std_mean</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">1</span>, <span class="number">10</span>, dtype=torch.double).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 2., 3.],</span></span><br><span class="line"><span class="string">        [4., 5., 6.],</span></span><br><span class="line"><span class="string">        [7., 8., 9.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本方差</span></span><br><span class="line"><span class="built_in">print</span>(torch.var(data))  <span class="comment"># tensor(7.5000, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 总体方差</span></span><br><span class="line"><span class="built_in">print</span>(torch.var(data, unbiased=<span class="literal">False</span>))  <span class="comment"># tensor(6.6667, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 同时计算方差和平均值</span></span><br><span class="line"><span class="built_in">print</span>(torch.var_mean(data))  <span class="comment"># (tensor(7.5000, dtype=torch.float64), tensor(5., dtype=torch.float64))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本标准差</span></span><br><span class="line"><span class="built_in">print</span>(torch.std(data))  <span class="comment"># tensor(2.7386, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 总体标准差</span></span><br><span class="line"><span class="built_in">print</span>(torch.std(data, unbiased=<span class="literal">False</span>))  <span class="comment"># tensor(2.5820, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 同时计算标准差和平均值</span></span><br><span class="line"><span class="built_in">print</span>(torch.std_mean(data))  <span class="comment"># (tensor(2.7386, dtype=torch.float64), tensor(5., dtype=torch.float64))</span></span><br></pre></td></tr></tbody></table></figure><p>总体方差：<script type="math/tex">\sigma^2 = \frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2</script></p><p>样本方差：<script type="math/tex">s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2</script></p><p>总体标准差：<script type="math/tex">\sigma = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2}</script></p><p>样本标准差：<script type="math/tex">s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}</script></p><h3 id="norm"><a href="#norm" class="headerlink" title=".norm"></a>.norm</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">2.</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2., 2.],</span></span><br><span class="line"><span class="string">        [2., 2.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.norm(data, p=<span class="number">1</span>))  <span class="comment"># tensor(8.)</span></span><br><span class="line"><span class="comment"># 二范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.norm(data))  <span class="comment"># tensor(4.)</span></span><br><span class="line"><span class="comment"># 三范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.norm(data, p=<span class="number">3</span>))  <span class="comment"># tensor(3.1748)</span></span><br></pre></td></tr></tbody></table></figure><p>一范数：<script type="math/tex">||x||_1 = \sum_{i=1}^N|x_i|</script></p><p>二范数：<script type="math/tex">||x||_2 = \sqrt{\sum_{i=1}^Nx_i^2}</script></p><p>p范数：<script type="math/tex">||x||_p = (\sum_{i=1}^N|x_i|^p)^\frac{1}{p}</script></p><h3 id="dist"><a href="#dist" class="headerlink" title=".dist"></a>.dist</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">4.</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[4., 4.],</span></span><br><span class="line"><span class="string">        [4., 4.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">other = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">2.</span>)</span><br><span class="line"><span class="built_in">print</span>(other)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2., 2.],</span></span><br><span class="line"><span class="string">        [2., 2.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.dist(<span class="built_in">input</span>, other, p=<span class="number">1</span>))  <span class="comment"># tensor(8.)</span></span><br><span class="line"><span class="comment"># 二范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.dist(<span class="built_in">input</span>, other))  <span class="comment"># tensor(4.)</span></span><br><span class="line"><span class="comment"># 三范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.dist(<span class="built_in">input</span>, other, p=<span class="number">3</span>))  <span class="comment"># tensor(3.1748)</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.dist()</code>返回<code>(input - other)</code>的<code>p</code>范数。</p><h3 id="any-all"><a href="#any-all" class="headerlink" title=".any/.all"></a>.any/.all</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([])))  <span class="comment"># tensor(False)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([<span class="literal">False</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(False)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(False)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([<span class="literal">False</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(False)</span></span><br></pre></td></tr></tbody></table></figure><h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><h3 id="save"><a href="#save" class="headerlink" title=".save"></a>.save</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to file</span></span><br><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">torch.save(x, <span class="string">'tensor.pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to io.BytesIO buffer</span></span><br><span class="line">buffer = io.BytesIO()</span><br><span class="line">torch.save(x, buffer)</span><br></pre></td></tr></tbody></table></figure><h3 id="load"><a href="#load" class="headerlink" title=".load"></a>.load</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a module with 'ascii' encoding for unpickling</span></span><br><span class="line">torch.load(<span class="string">'tensors.pt'</span>, encoding=<span class="string">'ascii'</span>)</span><br><span class="line"><span class="comment"># Load all tensors onto the CPU</span></span><br><span class="line">torch.load(<span class="string">'tensors.pt'</span>, map_location=torch.device(<span class="string">'cpu'</span>))</span><br><span class="line"><span class="comment"># Map tensors from GPU 1 to GPU 0</span></span><br><span class="line">torch.load(<span class="string">'tensors.pt'</span>, map_location={<span class="string">'cuda:1'</span>: <span class="string">'cuda:0'</span>})</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load from io.BytesIO buffer</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tensor.pt'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    buffer = io.BytesIO(f.read())</span><br><span class="line">torch.load(buffer)</span><br></pre></td></tr></tbody></table></figure><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>wpz</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://wpz.me/posts/668e89ae/" title="（一）PyTorch 张量">https://wpz.me/posts/668e89ae/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpoLWhhbnM="><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div><div style="text-align:center;color:#ccc;font-size:14px">----------------本文结束<i class="far fa-hand-point-right"></i>感谢您的阅读----------------</div><footer class="post-footer"><div class="post-tags"><a href="/next/tags/PyTorch/" rel="tag"><i class="fa fa-tag"></i> PyTorch</a></div><style>.posts-expand .post-tags a{display:inline-block;font-size:.8em;padding:0 10px;border-radius:8px;color:#555;border:0}</style><script data-pjax>for(var alltags=document.getElementsByClassName("post-tags"),tags=alltags[0].getElementsByTagName("a"),i=tags.length-1;i>=0;i--){var r,g,b,golden_ratio=.618033988749895,s=.5,v=.999,h=golden_ratio+.8*Math.random()-.5,h_i=parseInt(6*h),f=6*h-h_i,p=v*(1-s),q=v*(1-f*s),t=v*(1-(1-f)*s);switch(h_i){case 0:r=v,g=t,b=p;break;case 1:r=q,g=v,b=p;break;case 2:r=p,g=v,b=t;break;case 3:r=p,g=q,b=v;break;case 4:r=t,g=p,b=v;break;case 5:r=v,g=p,b=q;break;default:r=1,g=1,b=1}tags[i].style.background="rgba("+parseInt(255*r)+","+parseInt(255*g)+","+parseInt(255*b)+",0.5)"}</script><div class="post-nav"><div class="post-nav-item"><a href="/next/posts/3d0f623f/" rel="prev" title="汉诺塔"><i class="fa fa-chevron-left"></i> 汉诺塔</a></div><div class="post-nav-item"><a href="/next/posts/e8b7be43/" rel="next" title="Mathjax 与 LaTex 公式">Mathjax 与 LaTex 公式 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><script>window.addEventListener("tabs:register",()=>{let{activeClass:t}=CONFIG.comments;if(CONFIG.comments.storage&&(t=localStorage.getItem("comments_active")||t),t){let e=document.querySelector(`a[href="#comment-${t}"]`);e&&e.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{if(!t.target.matches(".tabs-comment .tab-content .tab-pane"))return;let e=t.target.classList[1];localStorage.setItem("comments_active",e)})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">Tensor 数据类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE-Tensor-%E9%BB%98%E8%AE%A4%E7%B1%BB%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">设置 Tensor 默认类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%87%E9%87%8F%E4%B8%8E%E5%BC%A0%E9%87%8F"><span class="nav-number">3.</span> <span class="nav-text">标量与张量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Tensor"><span class="nav-number">4.</span> <span class="nav-text">创建 Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tensor"><span class="nav-number">4.1.</span> <span class="nav-text">.tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#from-numpy"><span class="nav-number">4.2.</span> <span class="nav-text">.from_numpy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor"><span class="nav-number">4.3.</span> <span class="nav-text">Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#empty-zeros-ones-full-eye"><span class="nav-number">4.4.</span> <span class="nav-text">.empty&#x2F;.zeros&#x2F;.ones&#x2F;.full&#x2F;.eye</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#arange-linspace-logspace"><span class="nav-number">4.5.</span> <span class="nav-text">.arange&#x2F;.linspace&#x2F;.logspace</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="nav-number">5.</span> <span class="nav-text">随机采样</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90"><span class="nav-number">5.1.</span> <span class="nav-text">随机种子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%87%BD%E6%95%B0"><span class="nav-number">5.2.</span> <span class="nav-text">随机函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#rand-rand-like"><span class="nav-number">5.2.1.</span> <span class="nav-text">.rand&#x2F;.rand_like</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#randint-randint-like"><span class="nav-number">5.2.2.</span> <span class="nav-text">.randint&#x2F;.randint_like</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#randperm"><span class="nav-number">5.2.3.</span> <span class="nav-text">.randperm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#randn-randn-like"><span class="nav-number">5.2.4.</span> <span class="nav-text">.randn&#x2F;.randn_like</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#normal"><span class="nav-number">5.2.5.</span> <span class="nav-text">.normal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#bernoulli"><span class="nav-number">5.2.6.</span> <span class="nav-text">.bernoulli</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#poisson"><span class="nav-number">5.2.7.</span> <span class="nav-text">.poisson</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#multinomial"><span class="nav-number">5.2.8.</span> <span class="nav-text">.multinomial</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="nav-number">6.</span> <span class="nav-text">索引与切片</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Python-%E8%AF%AD%E6%B3%95"><span class="nav-number">6.1.</span> <span class="nav-text">Python 语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#narrow-narrow-copy"><span class="nav-number">6.2.</span> <span class="nav-text">.narrow&#x2F;.narrow_copy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#select-index-select"><span class="nav-number">6.3.</span> <span class="nav-text">.select&#x2F;.index_select</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#masked-select"><span class="nav-number">6.4.</span> <span class="nav-text">.masked_select</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gather"><span class="nav-number">6.5.</span> <span class="nav-text">.gather</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#take-take-along-dim"><span class="nav-number">6.6.</span> <span class="nav-text">.take&#x2F;.take_along_dim</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#argwhere-nonzero"><span class="nav-number">6.7.</span> <span class="nav-text">.argwhere&#x2F;.nonzero</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#where"><span class="nav-number">6.8.</span> <span class="nav-text">.where</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unravel-index"><span class="nav-number">6.9.</span> <span class="nav-text">.unravel_index</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2"><span class="nav-number">7.</span> <span class="nav-text">维度变换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#t-transpose-movedim-permute"><span class="nav-number">7.1.</span> <span class="nav-text">.t&#x2F;.transpose&#x2F;.movedim&#x2F;.permute</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#view-reshape"><span class="nav-number">7.2.</span> <span class="nav-text">.view&#x2F;.reshape</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#squeeze-unsqueeze"><span class="nav-number">7.3.</span> <span class="nav-text">.squeeze&#x2F;.unsqueeze</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#expand-repeat"><span class="nav-number">7.4.</span> <span class="nav-text">.expand&#x2F;.repeat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tile"><span class="nav-number">7.5.</span> <span class="nav-text">.tile</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%88%E5%B9%B6%E4%B8%8E%E6%8B%86%E5%88%86"><span class="nav-number">8.</span> <span class="nav-text">合并与拆分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cat"><span class="nav-number">8.1.</span> <span class="nav-text">.cat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stack-hstack-vstack-column-stack-dstack"><span class="nav-number">8.2.</span> <span class="nav-text">.stack&#x2F;.hstack&#x2F;.vstack&#x2F;.column_stack&#x2F;.dstack</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chunk"><span class="nav-number">8.3.</span> <span class="nav-text">.chunk</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#split-hsplit-vsplit-dsplit-tensor-split"><span class="nav-number">8.4.</span> <span class="nav-text">.split&#x2F;.hsplit&#x2F;.vsplit&#x2F;.dsplit&#x2F;.tensor_split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unbind"><span class="nav-number">8.5.</span> <span class="nav-text">.unbind</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%90%E7%82%B9%E8%BF%90%E7%AE%97"><span class="nav-number">9.</span> <span class="nav-text">逐点运算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#add-sub-mul-div-remainder-fmod-positive-neg-abs"><span class="nav-number">9.1.</span> <span class="nav-text">.add&#x2F;.sub&#x2F;.mul&#x2F;.div&#x2F;.remainder&#x2F;.fmod&#x2F;.positive&#x2F;.neg&#x2F;.abs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pow-square-sqrt-rsqrt-reciprocal"><span class="nav-number">9.2.</span> <span class="nav-text">.pow&#x2F;.square&#x2F;.sqrt&#x2F;.rsqrt&#x2F;.reciprocal</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#exp-log-log2-log10-log1p"><span class="nav-number">9.3.</span> <span class="nav-text">.exp&#x2F;.log&#x2F;.log2&#x2F;.log10&#x2F;.log1p</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sin-cos-tan-asin-acos-atan-atan2-sinh-cosh-tanh"><span class="nav-number">9.4.</span> <span class="nav-text">.sin&#x2F;.cos&#x2F;.tan&#x2F;.asin&#x2F;.acos&#x2F;.atan&#x2F;.atan2&#x2F;.sinh&#x2F;.cosh&#x2F;.tanh</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#angle-deg2rad"><span class="nav-number">9.5.</span> <span class="nav-text">.angle&#x2F;.deg2rad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bitwise-and-bitwise-or-bitwise-not-bitwise-xor-bitwise-left-shift-bitwise-right-shift"><span class="nav-number">9.6.</span> <span class="nav-text">.bitwise_and&#x2F;.bitwise_or&#x2F;.bitwise_not&#x2F;.bitwise_xor&#x2F;.bitwise_left_shift&#x2F;.bitwise_right_shift</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#floor-ceil-round"><span class="nav-number">9.7.</span> <span class="nav-text">.floor&#x2F;.ceil&#x2F;.round</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trunc-frac"><span class="nav-number">9.8.</span> <span class="nav-text">.trunc&#x2F;.frac</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#clamp-clamp-min-clamp-max"><span class="nav-number">9.9.</span> <span class="nav-text">.clamp&#x2F;.clamp_min&#x2F;.clamp_max</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dot-mm-bmm-matmul"><span class="nav-number">9.10.</span> <span class="nav-text">.dot&#x2F;.mm&#x2F;.bmm&#x2F;.matmul</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AF%94%E8%BE%83%E8%BF%90%E7%AE%97"><span class="nav-number">10.</span> <span class="nav-text">比较运算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#eq-ne-gt-ge-lt-le-equal"><span class="nav-number">10.1.</span> <span class="nav-text">.eq&#x2F;.ne&#x2F;.gt&#x2F;.ge&#x2F;.lt&#x2F;.le&#x2F;.equal</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#isfinite-isinf-isposinf-isneginf-isnan-isreal-isin"><span class="nav-number">10.2.</span> <span class="nav-text">.isfinite&#x2F;.isinf&#x2F;.isposinf&#x2F;.isneginf&#x2F;.isnan&#x2F;.isreal&#x2F;.isin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#isclose-allclose"><span class="nav-number">10.3.</span> <span class="nav-text">.isclose&#x2F;.allclose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#maximum-minimum-fmax-fmin"><span class="nav-number">10.4.</span> <span class="nav-text">.maximum&#x2F;.minimum&#x2F;.fmax&#x2F;.fmin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sort-msort-argsort"><span class="nav-number">10.5.</span> <span class="nav-text">.sort&#x2F;.msort&#x2F;.argsort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#topk-kthvalue"><span class="nav-number">10.6.</span> <span class="nav-text">.topk&#x2F;.kthvalue</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E7%BA%A6%E8%BF%90%E7%AE%97"><span class="nav-number">11.</span> <span class="nav-text">归约运算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#max-min-amax-amin-aminmax-argmax-argmin"><span class="nav-number">11.1.</span> <span class="nav-text">.max&#x2F;.min&#x2F;.amax&#x2F;.amin&#x2F;.aminmax&#x2F;.argmax&#x2F;.argmin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mean-nanmean"><span class="nav-number">11.2.</span> <span class="nav-text">.mean&#x2F;.nanmean</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#median-nanmedian"><span class="nav-number">11.3.</span> <span class="nav-text">.median&#x2F;.nanmedian</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sum-nansum"><span class="nav-number">11.4.</span> <span class="nav-text">.sum&#x2F;.nansum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prod"><span class="nav-number">11.5.</span> <span class="nav-text">.prod</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#var-var-mean-std-std-mean"><span class="nav-number">11.6.</span> <span class="nav-text">.var&#x2F;.var_mean&#x2F;.std&#x2F;.std_mean</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#norm"><span class="nav-number">11.7.</span> <span class="nav-text">.norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dist"><span class="nav-number">11.8.</span> <span class="nav-text">.dist</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#any-all"><span class="nav-number">11.9.</span> <span class="nav-text">.any&#x2F;.all</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96"><span class="nav-number">12.</span> <span class="nav-text">序列化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#save"><span class="nav-number">12.1.</span> <span class="nav-text">.save</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#load"><span class="nav-number">12.2.</span> <span class="nav-text">.load</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="wpz" src="/next/images/avatar.jpg"><p class="site-author-name" itemprop="name">wpz</p><div class="site-description" itemprop="description">你只管努力，剩下的交给时间！</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/next/archives/"><span class="site-state-item-count">18</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/next/categories/"><span class="site-state-item-count">5</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/next/tags/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xiOTR3cHo=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lb94wpz"><i class="fab fa-github fa-fw"></i></span> </span><span class="links-of-author-item"><span class="exturl" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTE2NjQ3OTQwNDQ=" title="CloudMusic → https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;1664794044"><i class="fa fa-music fa-fw"></i></span> </span><span class="links-of-author-item"><a href="/next/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fas fa-rss fa-fw"></i></a></span></div><div class="cc-license motion-element" itemprop="license"><span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpoLWhhbnM="><img src="/next/images/cc-by-nc-sa.svg" alt="Creative Commons"></span></div><style>.links-of-recent-posts{font-size:.8125em;margin-top:10px}.links-of-recent-posts-title{font-size:.875em;font-weight:600;margin-top:0}.links-of-recent-posts-list{list-style:none;margin:0;padding:0}</style><div class="links-of-recent-posts motion-element"><div class="links-of-recent-posts-title"><i class="fa fa-history fa-fw"></i> 近期文章</div><ul class="links-of-recent-posts-list"><li class="links-of-recent-posts-item"><a href="/next/posts/e8b7be43/" title="posts&#x2F;e8b7be43&#x2F;">Mathjax 与 LaTex 公式</a></li><li class="links-of-recent-posts-item"><a href="/next/posts/668e89ae/" title="posts&#x2F;668e89ae&#x2F;">（一）PyTorch 张量</a></li><li class="links-of-recent-posts-item"><a href="/next/posts/3d0f623f/" title="posts&#x2F;3d0f623f&#x2F;">汉诺塔</a></li><li class="links-of-recent-posts-item"><a href="/next/posts/6ab4a85b/" title="posts&#x2F;6ab4a85b&#x2F;">二叉树前序、中序、后序遍历</a></li><li class="links-of-recent-posts-item"><a href="/next/posts/c2a5fdc5/" title="posts&#x2F;c2a5fdc5&#x2F;">堆排序</a></li></ul></div><script type="text/javascript" charset="utf-8" src="/next/js/tagcloud.js"></script><script type="text/javascript" charset="utf-8" src="/next/js/tagcanvas.js"></script><div class="widget-wrap"><h3 class="widget-title">标签云</h3><div id="myCanvasContainer" class="widget tagcloud"><canvas width="250" height="250" id="resCanvas" style="width:100%"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/Crontab/" rel="tag">Crontab</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/Docker/" rel="tag">Docker</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/PyTorch/" rel="tag">PyTorch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/Redis/" rel="tag">Redis</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/nohup/" rel="tag">nohup</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/%E6%8E%92%E5%BA%8F/" rel="tag">排序</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/next/tags/%E6%B1%89%E8%AF%BA%E5%A1%94/" rel="tag">汉诺塔</a><span class="tag-list-count">1</span></li></ul></canvas></div></div><div><canvas id="canvasDiyBlock" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/clock.min.js"></script></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2024 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">&nbsp;wpz</span><br><span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">103k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">1:33</span></div><div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly9waXNjZXMudGhlbWUtbmV4dC5vcmc=">NexT.Pisces</span> 强力驱动</div></div></footer></div><script size="100" alpha="0.6" zindex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script><script src="/next/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/next/lib/velocity/velocity.min.js"></script><script src="/next/lib/velocity/velocity.ui.min.js"></script><script src="/next/js/utils.js"></script><script src="/next/js/motion.js"></script><script src="/next/js/schemes/pisces.js"></script><script src="/next/js/next-boot.js"></script><script>var pjax=new Pjax({selectors:["head title","#page-configurations",".content-wrap",".post-toc-wrap",".languages","#pjax"],switches:{".post-toc-wrap":Pjax.switches.innerHTML},analytics:!1,cacheBust:!1,scrollTo:!CONFIG.bookmark.enable});window.addEventListener("pjax:success",()=>{document.querySelectorAll("script[data-pjax], script#page-configurations, #pjax script").forEach(e=>{var t=e.text||e.textContent||e.innerHTML||"",a=e.parentNode;a.removeChild(e);var s=document.createElement("script");e.id&&(s.id=e.id),e.className&&(s.className=e.className),e.type&&(s.type=e.type),e.src&&(s.src=e.src,s.async=!1),void 0!==e.dataset.pjax&&(s.dataset.pjax=""),""!==t&&s.appendChild(document.createTextNode(t)),a.appendChild(s)}),NexT.boot.refresh(),CONFIG.motion.enable&&NexT.motion.integrator.init().add(NexT.motion.middleWares.subMenu).add(NexT.motion.middleWares.postList).bootstrap(),NexT.utils.updateSidebarPosition()})</script><script src="/next/js/local-search.js"></script><script data-pjax>document.querySelectorAll(".pdfobject-container").forEach(e=>{let t=e.dataset.target,a="#"+Object.entries({navpanes:0,toolbar:0,statusbar:0,pagemode:"thumbs",view:"FitH"}).map(([e,t])=>`${e}=${encodeURIComponent(t)}`).join("&"),i=`/next/lib/pdf/web/viewer.html?file=${encodeURIComponent(t)}${a}`;NexT.utils.supportsPDFs()?e.innerHTML=`<embed class="pdfobject" src="${t+a}" type="application/pdf" style="height: ${e.dataset.height};">`:e.innerHTML=`<iframe src="${i}" style="height: ${e.dataset.height};" frameborder="0"></iframe>`})</script><script data-pjax>document.querySelectorAll("pre.mermaid").length&&NexT.utils.getScript("//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js",()=>{mermaid.initialize({theme:"forest",logLevel:3,flowchart:{curve:"linear"},gantt:{axisFormat:"%m/%d/%Y"},sequence:{actorMargin:50}})},window.mermaid)</script><div id="pjax"><script>"undefined"==typeof MathJax?(window.MathJax={loader:{source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},tags:"ams"},options:{renderActions:{findScript:[10,e=>{document.querySelectorAll('script[type^="math/tex"]').forEach(t=>{const a=!!t.type.match(/; *mode=display/),n=new e.options.MathItem(t.textContent,e.inputJax[0],a),d=document.createTextNode("");t.parentNode.replaceChild(d,t),n.start={node:d,delim:"",n:0},n.end={node:d,delim:"",n:0},e.math.push(n)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script></div><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/title-change.min.js"></script><script id="click-heart" async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/click-heart.min.js"></script><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/fireworks.min.js"></script><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/live2d-widget/autoload.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script async src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script async src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js server="netease" type="playlist" id="2503010581" order="random" fixed="true"></meting-js><script>function removeLRC(){document.querySelector(".aplayer-icon-lrc")&&(document.removeEventListener("DOMNodeInserted",removeLRC),setTimeout((function(){document.querySelector(".aplayer-icon-lrc").click()}),1))}document.addEventListener("DOMNodeInserted",removeLRC)</script><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/css/szgotop.min.css"><div class="go-to-top faa-float animated" style="top:-900px"></div><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/szgotop.min.js"></script><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/sakura.min.js"></script><script src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/tw_cn.min.js"></script><script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script><script>const darkmode=new Darkmode;window.darkmode=darkmode</script><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-sync"></i></a><a class="rightMenu-item" href="/next/"><i class="fa fa-home"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-selection"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:rmf.search('Google');"><i class="iconfont icon-google"></i><span>谷歌搜索</span></a><a class="rightMenu-item" href="javascript:rmf.search('Bing');"><i class="iconfont icon-bing"></i><span>必应搜索</span></a><a class="rightMenu-item" href="javascript:rmf.searchInThisPage();"><i class="fas fa-search"></i><span>站内搜索</span></a><a class="rightMenu-item" id="menu-to" href="javascript:window.open(window.getSelection().toString());"><i class="iconfont icon-tiaozhuan"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-a"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.open()"><i class="iconfont icon-tiaozhuan"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a><a class="rightMenu-item" href="javascript:toRandomPost();"><i class="fa fa-random"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:rmf.playMusic();"><i class="fa fa-music"></i><span>听听小曲</span></a><a class="rightMenu-item" href="javascript:rmf.switchTranslate();"><i class="iconfont icon-jianfanqiehuan"></i><span>简繁切换</span></a><a class="rightMenu-item" href="javascript:rmf.switchNightMode();"><i class="iconfont icon-zhouye"></i><span>昼夜更替</span></a><a class="rightMenu-item" href="javascript:changeMouseMode();"><i class="iconfont icon-mouseR"></i><span>切换鼠标右键</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa fa-bus"></i><span>开往</span></a><a class="rightMenu-item" target="_blank" rel="noopener" href="https://foreverblog.cn/go.html"><i class="fa fa-spinner"></i><span>虫洞</span></a><a class="rightMenu-item" target="_blank" rel="noopener" href="https://travel.moe/go.html?travel=on"><i class="iconfont icon-xingqiu"></i><span>跃迁</span></a></div></div><script src="/next/js/rightmenu.js"></script></body></html><script>var posts = ["posts/f189e9a9/","posts/e8b7be43/","posts/668e89ae/","posts/6ab4a85b/","posts/14e6f1eb/","posts/3d0f623f/","posts/c2a5fdc5/","posts/96555fb2/","posts/ff8068c0/","posts/43d00a99/","posts/1ac49179/","posts/e2f703e6/","posts/b3b04b5f/","posts/2262a086/","posts/6cc4d10b/","posts/8777e763/","posts/c09adc05/","posts/6574bc1f/"];function toRandomPost(){ window.pjax ? pjax.loadUrl(/next/ + posts[Math.floor(Math.random() * posts.length)]) : window.open(/next/ + posts[Math.floor(Math.random() * posts.length)], "_self"); };</script>