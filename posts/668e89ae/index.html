<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>（一）PyTorch 张量 | Wpz's Blog</title><meta name="author" content="wpz"><meta name="copyright" content="wpz"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="article"><meta property="og:title" content="（一）PyTorch 张量"><meta property="og:url" content="https://wpz.me/posts/668e89ae/index.html"><meta property="og:site_name" content="Wpz&#39;s Blog"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://wpz.me/img/avatar.jpg"><meta property="article:published_time" content="2024-08-26T08:17:41.000Z"><meta property="article:modified_time" content="2025-03-31T10:24:44.381Z"><meta property="article:author" content="wpz"><meta property="article:tag" content="PyTorch"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://wpz.me/img/avatar.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://wpz.me/posts/668e89ae/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体中文",cht_to_chs:"你已切换为简体中文",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#1f1f1f",position:"top-center"},infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"（一）PyTorch 张量",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-03-31 18:24:44"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach(e=>{n.setAttribute(e,t[e])}),document.head.appendChild(n)}),e.getCSS=(e,t=!1)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)}),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css"><svg aria-hidden="true" style="position:absolute;overflow:hidden;width:0;height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path></symbol></svg><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload='this.media="all"'><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload='this.media="screen"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><meta name="generator" content="Hexo 7.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Wpz's Blog" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-bg"><img class="loading-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" alt=""><div class="loading-image-dot"></div></div></div><script>(()=>{const e=document.getElementById("loading-box"),d=document.body,t=()=>{d.style.overflow="",e.classList.add("loaded")},n=()=>{d.style.overflow="hidden",e.classList.remove("loaded")};n(),window.addEventListener("load",()=>{t()}),document.addEventListener("pjax:send",()=>{n()}),document.addEventListener("pjax:complete",()=>{t()}),e.addEventListener("click",()=>{t()})})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/next/"><i class="fa-fw fas fa-sitemap"></i><span> 分站</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:toRandomPost()"><i class="fa-fw fa fa-shuffle"></i><span> 随机</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-ghost"></i><span> 探索</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly93d3cudHJhdmVsbGluZ3MuY24vZ28uaHRtbA==" rel="noopener external nofollow noreferrer"><i class="fa-fw fa fa-bus"></i><span> 开往</span></a></li><li><a class="site-page child" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly9mb3JldmVyYmxvZy5jbi9nby5odG1s" rel="noopener external nofollow noreferrer"><i class="fa-fw fa fa-spinner"></i><span> 虫洞</span></a></li><li><a class="site-page child" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly90cmF2ZWwubW9lL2dvLmh0bWw/dHJhdmVsPW9u" rel="noopener external nofollow noreferrer"><i class="fa-fw iconfont icon-xingqiu"></i><span> 跃迁</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/%E6%B8%85%E6%BC%AA.jpg)"><nav id="nav"><span id="blog-info"><a href="/" title="Wpz's Blog"><span class="site-name">Wpz's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/next/"><i class="fa-fw fas fa-sitemap"></i><span> 分站</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:toRandomPost()"><i class="fa-fw fa fa-shuffle"></i><span> 随机</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-ghost"></i><span> 探索</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly93d3cudHJhdmVsbGluZ3MuY24vZ28uaHRtbA==" rel="noopener external nofollow noreferrer"><i class="fa-fw fa fa-bus"></i><span> 开往</span></a></li><li><a class="site-page child" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly9mb3JldmVyYmxvZy5jbi9nby5odG1s" rel="noopener external nofollow noreferrer"><i class="fa-fw fa fa-spinner"></i><span> 虫洞</span></a></li><li><a class="site-page child" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly90cmF2ZWwubW9lL2dvLmh0bWw/dHJhdmVsPW9u" rel="noopener external nofollow noreferrer"><i class="fa-fw iconfont icon-xingqiu"></i><span> 跃迁</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">（一）PyTorch 张量</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-26T08:17:41.000Z" title="发表于 2024-08-26 16:17:41">2024-08-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-31T10:24:44.381Z" title="更新于 2025-03-31 18:24:44">2025-03-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>55分钟</span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><span></span></p><span id="more"></span><h2 id="Tensor-数据类型"><a href="#Tensor-数据类型" class="headerlink" title="Tensor 数据类型"></a>Tensor 数据类型</h2><div class="table-container"><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU Tensor</th><th>GPU Tensor</th></tr></thead><tbody><tr><td>Boolean</td><td>torch.bool</td><td>torch.BoolTensor</td><td>torch.cuda.BoolTensor</td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32 or torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64 or torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>16-bit floating point</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>16-bit floating point</td><td>torch.bfloat16</td><td>torch.BFloat16Tensor</td><td>torch.cuda.BFloat16Tensor</td></tr><tr><td>32-bit floating point</td><td>torch.float32 or torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64 or torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr></tbody></table></div><h2 id="设置-Tensor-默认类型"><a href="#设置-Tensor-默认类型" class="headerlink" title="设置 Tensor 默认类型"></a>设置 Tensor 默认类型</h2><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).dtype)  <span class="comment"># torch.bool</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).<span class="built_in">type</span>())  <span class="comment"># torch.BoolTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).dtype)  <span class="comment"># torch.int64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).<span class="built_in">type</span>())  <span class="comment"># torch.LongTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).dtype)  <span class="comment"># torch.float32</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).<span class="built_in">type</span>())  <span class="comment"># torch.FloatTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).dtype)  <span class="comment"># torch.complex64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).<span class="built_in">type</span>())  <span class="comment"># torch.ComplexFloatTensor</span></span><br><span class="line"></span><br><span class="line">torch.set_default_dtype(torch.double)  <span class="comment"># 设置默认类型为 double</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).dtype)  <span class="comment"># torch.bool</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="literal">True</span>).<span class="built_in">type</span>())  <span class="comment"># torch.BoolTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).dtype)  <span class="comment"># torch.int64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).<span class="built_in">type</span>())  <span class="comment"># torch.LongTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).dtype)  <span class="comment"># torch.float64</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1.</span>).<span class="built_in">type</span>())  <span class="comment"># torch.DoubleTensor</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).dtype)  <span class="comment"># torch.complex128</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">3j</span>).<span class="built_in">type</span>())  <span class="comment"># torch.ComplexDoubleTensor</span></span><br></pre></td></tr></tbody></table></figure><p><code>set_default_dtype()</code>只能设置<code>floating-point</code>类型，否则会报<code>TypeError: only floating-point types are supported as the default type</code>错误。</p><h2 id="标量与张量"><a href="#标量与张量" class="headerlink" title="标量与张量"></a>标量与张量</h2><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标量</span></span><br><span class="line">a = torch.tensor(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)  <span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(a.size())  <span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(a.ndim)  <span class="comment"># 0</span></span><br><span class="line"><span class="built_in">print</span>(a.dim())  <span class="comment"># 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一维张量</span></span><br><span class="line">b = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(b.shape)  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(b.size())  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(b.ndim)  <span class="comment"># 1</span></span><br><span class="line"><span class="built_in">print</span>(b.dim())  <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维张量</span></span><br><span class="line">c = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(c.shape)  <span class="comment"># torch.Size([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(c.size())  <span class="comment"># torch.Size([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(c.ndim)  <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(c.dim())  <span class="comment"># 2</span></span><br></pre></td></tr></tbody></table></figure><p>标量是一个单独的数，<code>ndim</code>为<code>0</code>。</p><h2 id="创建-Tensor"><a href="#创建-Tensor" class="headerlink" title="创建 Tensor"></a>创建 Tensor</h2><h3 id="tensor"><a href="#tensor" class="headerlink" title=".tensor"></a>.tensor</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标量</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>))  <span class="comment"># tensor(1)</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>, dtype=torch.float64))  <span class="comment"># tensor(1., dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))  <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1</span>, <span class="number">2</span>], dtype=torch.float64))  <span class="comment"># tensor([1., 2.], dtype=torch.float64)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="from-numpy"><a href="#from-numpy" class="headerlink" title=".from_numpy"></a>.from_numpy</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(data))  <span class="comment"># tensor([1, 2, 3], dtype=torch.int32)</span></span><br><span class="line">data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=np.float64)</span><br><span class="line"><span class="built_in">print</span>(torch.from_numpy(data))  <span class="comment"># tensor([1., 2., 3.], dtype=torch.float64)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数为 shape 大小</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor(<span class="number">1</span>))  <span class="comment"># tensor([-3.0434e+31])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># tensor([[0., 0.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[[-3.0741e+31,  1.6031e-42,  0.0000e+00],</span></span><br><span class="line"><span class="string">         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数为列表</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor([<span class="number">1</span>]))  <span class="comment"># tensor([1.])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor([<span class="number">1</span>, <span class="number">2</span>]))  <span class="comment"># tensor([1., 2.])</span></span><br><span class="line"><span class="built_in">print</span>(torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))  <span class="comment"># tensor([1., 2., 3.])</span></span><br></pre></td></tr></tbody></table></figure><p><code>Tensor</code>支持两种传参方式：</p><ol><li>当参数为列表时，创建列表对应维度的<code>Tensor</code>并初始化数据为列表数据。</li><li>当参数不为列表时，与<code>.empty()</code>类似，创建参数指定的<code>shape</code>的空的<code>Tensor</code>。</li></ol><p><code>BoolTensor</code>、<code>ByteTensor</code>、<code>CharTensor</code>、<code>ShortTensor</code>、<code>IntTensor</code>、<code>LongTensor</code>、<code>HalfTensor</code>、<code>FloatTensor</code>、<code>DoubleTensor</code>也是一样。</p><h3 id="empty-zeros-ones-full-eye"><a href="#empty-zeros-ones-full-eye" class="headerlink" title=".empty/.zeros/.ones/.full/.eye"></a>.empty/.zeros/.ones/.full/.eye</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.empty(()))  <span class="comment"># tensor(-6.5391e-19)</span></span><br><span class="line"><span class="built_in">print</span>(torch.empty((<span class="number">1</span>, <span class="number">5</span>)))  <span class="comment"># tensor([[-6.6069e-19,  1.3943e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.empty_like((<span class="built_in">input</span>)))  <span class="comment"># tensor([0, 0, 0, 0, 0])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.zeros(()))  <span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.zeros((<span class="number">1</span>, <span class="number">5</span>)))  <span class="comment"># tensor([[0., 0., 0., 0., 0.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.zeros_like((<span class="built_in">input</span>)))  <span class="comment"># tensor([0, 0, 0, 0, 0])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.ones(()))  <span class="comment"># tensor(1.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.ones((<span class="number">1</span>, <span class="number">5</span>)))  <span class="comment"># tensor([[1., 1., 1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.ones_like((<span class="built_in">input</span>)))  <span class="comment"># tensor([1, 1, 1, 1, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.full((), <span class="number">100</span>))  <span class="comment"># tensor(100)</span></span><br><span class="line"><span class="built_in">print</span>(torch.full((<span class="number">1</span>, <span class="number">5</span>), <span class="number">100</span>))  <span class="comment"># tensor([[100, 100, 100, 100, 100]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.full_like((<span class="built_in">input</span>), <span class="number">100</span>))  <span class="comment"># tensor([100, 100, 100, 100, 100])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.eye(<span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.eye(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 1., 0., 0.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="arange-linspace-logspace"><a href="#arange-linspace-logspace" class="headerlink" title=".arange/.linspace/.logspace"></a>.arange/.linspace/.logspace</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">0</span>, <span class="number">10</span>))  <span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">0</span>, <span class="number">10</span>, step=<span class="number">3</span>))  <span class="comment"># tensor([0, 3, 6, 9])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [0, 1] 等分成 5 份</span></span><br><span class="line"><span class="built_in">print</span>(torch.linspace(<span class="number">0</span>, <span class="number">1</span>, steps=<span class="number">5</span>))  <span class="comment"># tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认以 10 为底数，[0, 1] 等分成 5 份做为指数</span></span><br><span class="line"><span class="built_in">print</span>(torch.logspace(<span class="number">0</span>, <span class="number">1</span>, steps=<span class="number">5</span>))  <span class="comment"># tensor([ 1.0000,  1.7783,  3.1623,  5.6234, 10.0000])</span></span><br><span class="line"><span class="comment"># 以 2 为底数，[0, 1] 等分成 5 份做为指数</span></span><br><span class="line"><span class="built_in">print</span>(torch.logspace(<span class="number">0</span>, <span class="number">1</span>, steps=<span class="number">5</span>, base=<span class="number">2</span>))  <span class="comment"># tensor([1.0000, 1.1892, 1.4142, 1.6818, 2.0000])</span></span><br></pre></td></tr></tbody></table></figure><h2 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h2><h3 id="随机种子"><a href="#随机种子" class="headerlink" title="随机种子"></a>随机种子</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 CPU 随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 GPU 随机种子</span></span><br><span class="line">torch.cuda.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看设置的随机种子</span></span><br><span class="line"><span class="built_in">print</span>(torch.initial_seed())  <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机设置随机种子</span></span><br><span class="line">torch.seed()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.initial_seed())  <span class="comment"># 22287915889500</span></span><br></pre></td></tr></tbody></table></figure><h3 id="随机函数"><a href="#随机函数" class="headerlink" title="随机函数"></a>随机函数</h3><h4 id="rand-rand-like"><a href="#rand-rand-like" class="headerlink" title=".rand/.rand_like"></a>.rand/.rand_like</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999],</span></span><br><span class="line"><span class="string">        [0.3971, 0.7544, 0.5695]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.rand_like(<span class="built_in">input</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.4388, 0.6387, 0.5247],</span></span><br><span class="line"><span class="string">        [0.6826, 0.3051, 0.4635],</span></span><br><span class="line"><span class="string">        [0.4550, 0.5725, 0.4980]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.rand()</code>返回在区间<code>[0, 1)</code>均匀分布的随机数填充的张量。</p><h4 id="randint-randint-like"><a href="#randint-randint-like" class="headerlink" title=".randint/.randint_like"></a>.randint/.randint_like</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randint(low=<span class="number">0</span>, high=<span class="number">10</span>, size=(<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5, 9, 4],</span></span><br><span class="line"><span class="string">        [8, 3, 3],</span></span><br><span class="line"><span class="string">        [1, 1, 9]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randint_like(<span class="built_in">input</span>, high=<span class="number">10</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2., 8., 9.],</span></span><br><span class="line"><span class="string">        [6., 3., 3.],</span></span><br><span class="line"><span class="string">        [0., 2., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.randint(low, high)</code>返回在区间<code>[low, high)</code>的随机数填充的张量。</p><h4 id="randperm"><a href="#randperm" class="headerlink" title=".randperm"></a>.randperm</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randperm(<span class="number">10</span>))  <span class="comment"># tensor([5, 6, 1, 2, 0, 8, 9, 3, 7, 4])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.randperm(n)</code>返回在区间<code>[0, n)</code>的随机排列整数。</p><h4 id="randn-randn-like"><a href="#randn-randn-like" class="headerlink" title=".randn/.randn_like"></a>.randn/.randn_like</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准正态分布 N(0, 1)，均值为 0，方差为 1</span></span><br><span class="line"><span class="built_in">print</span>(torch.randn(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0.6614,  0.2669,  0.0617],</span></span><br><span class="line"><span class="string">        [ 0.6213, -0.4519, -0.1661],</span></span><br><span class="line"><span class="string">        [-1.5228,  0.3817, -1.0276]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.randn_like(<span class="built_in">input</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-0.5631, -0.8923, -0.0583],</span></span><br><span class="line"><span class="string">        [-0.1955, -0.9656,  0.4224],</span></span><br><span class="line"><span class="string">        [ 0.2673, -0.4212, -0.5107]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.randn()</code>从<code>标准正态分布</code>中随机采样。</p><h4 id="normal"><a href="#normal" class="headerlink" title=".normal"></a>.normal</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 离散正态分布 N(mean, std)</span></span><br><span class="line"><span class="built_in">print</span>(torch.normal(mean=torch.full((<span class="number">5</span>,), <span class="number">0.</span>), std=torch.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.2</span>)))  <span class="comment"># tensor([ 0.0000,  0.0534,  0.0247,  0.3728, -0.3615])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.normal(mean, std)</code>从给定参数<code>mean</code>、<code>std</code>的<code>离散正态分布</code>中随机采样。</p><h4 id="bernoulli"><a href="#bernoulli" class="headerlink" title=".bernoulli"></a>.bernoulli</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.empty(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># generate a uniform random matrix with range [0, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.bernoulli(<span class="built_in">input</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 0., 0.],</span></span><br><span class="line"><span class="string">        [1., 0., 1.],</span></span><br><span class="line"><span class="string">        [0., 1., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.bernoulli(torch.ones(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># probability of drawing "1" is 1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.bernoulli(torch.zeros(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># probability of drawing "1" is 0</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.bernoulli()</code>从<code>伯努利分布</code>中抽取二进制随机数（0 或 1）。输入的值必须在<code>[0, 1]</code>范围内。</p><h4 id="poisson"><a href="#poisson" class="headerlink" title=".poisson"></a>.poisson</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">rates = torch.rand(<span class="number">3</span>, <span class="number">3</span>) * <span class="number">5</span>  <span class="comment"># rate parameter between 0 and 5</span></span><br><span class="line"><span class="built_in">print</span>(torch.poisson(rates))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5., 1., 1.],</span></span><br><span class="line"><span class="string">        [3., 0., 2.],</span></span><br><span class="line"><span class="string">        [0., 2., 0.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.poisson()</code>从<code>泊松分布</code>中随机采样。</p><h4 id="multinomial"><a href="#multinomial" class="headerlink" title=".multinomial"></a>.multinomial</h4><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">weights = torch.tensor([[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.multinomial(weights, <span class="number">5</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2, 3, 4, 1, 0],</span></span><br><span class="line"><span class="string">        [3, 2, 1, 4, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replacement 默认为 False，表示不允许重复抽查，所以采样次数不能大于抽查个数</span></span><br><span class="line"><span class="comment"># print(torch.multinomial(weights, 6))  # RuntimeError: cannot sample n_sample &gt; prob_dist.size(-1) samples without replacement</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># replacement=True 允许重复抽查</span></span><br><span class="line"><span class="built_in">print</span>(torch.multinomial(weights, <span class="number">6</span>, replacement=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[3, 0, 3, 3, 4, 2],</span></span><br><span class="line"><span class="string">        [0, 1, 4, 4, 2, 4]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.multinomial(input, num_samples, replacement)</code>对<code>input</code>的每一行做从<code>多项式分布</code>中采样<code>num_samples</code>次，输出的张量是每一次取值时<code>input</code>张量对应行的下标。</p><h2 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h2><h3 id="Python-语法"><a href="#Python-语法" class="headerlink" title="Python 语法"></a>Python 语法</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第一张图片</span></span><br><span class="line"><span class="built_in">print</span>(images[<span class="number">0</span>].shape)  <span class="comment"># torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取第一张图片的第一个通道数据</span></span><br><span class="line"><span class="built_in">print</span>(images[<span class="number">0</span>, <span class="number">0</span>].shape)  <span class="comment"># torch.Size([28, 28])</span></span><br><span class="line"><span class="comment"># 获取前两张图片</span></span><br><span class="line"><span class="built_in">print</span>(images[:<span class="number">2</span>].shape)  <span class="comment"># torch.Size([2, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取前两张图片前两个通道数据</span></span><br><span class="line"><span class="built_in">print</span>(images[:<span class="number">2</span>, :<span class="number">2</span>].shape)  <span class="comment"># torch.Size([2, 2, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取前两张图片最后一个通道数据</span></span><br><span class="line"><span class="built_in">print</span>(images[:<span class="number">2</span>, -<span class="number">1</span>].shape)  <span class="comment"># torch.Size([2, 28, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取图片数据</span></span><br><span class="line"><span class="built_in">print</span>(images[:, :, ::<span class="number">2</span>, ::<span class="number">2</span>].shape)  <span class="comment"># torch.Size([4, 3, 14, 14])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有图片</span></span><br><span class="line"><span class="built_in">print</span>(images[...].shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取第一张图片</span></span><br><span class="line"><span class="built_in">print</span>(images[<span class="number">0</span>, ...].shape)  <span class="comment"># torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取图片宽数据</span></span><br><span class="line"><span class="built_in">print</span>(images[..., ::<span class="number">2</span>].shape)  <span class="comment"># torch.Size([4, 3, 28, 14])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="narrow-narrow-copy"><a href="#narrow-narrow-copy" class="headerlink" title=".narrow/.narrow_copy"></a>.narrow/.narrow_copy</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span> ,<span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 1,  2,  3,  4,  5],</span></span><br><span class="line"><span class="string">        [ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="string">        [11, 12, 13, 14, 15]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.narrow(data, dim=<span class="number">0</span>, start=<span class="number">1</span>, length=<span class="number">2</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="string">        [11, 12, 13, 14, 15]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.narrow(data, dim=<span class="number">1</span>, start=<span class="number">1</span>, length=<span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 2,  3,  4],</span></span><br><span class="line"><span class="string">        [ 7,  8,  9],</span></span><br><span class="line"><span class="string">        [12, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.narrow_copy(data, dim=<span class="number">0</span>, start=<span class="number">1</span>, length=<span class="number">2</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 6,  7,  8,  9, 10],</span></span><br><span class="line"><span class="string">        [11, 12, 13, 14, 15]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.narrow_copy(data, dim=<span class="number">1</span>, start=<span class="number">1</span>, length=<span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 2,  3,  4],</span></span><br><span class="line"><span class="string">        [ 7,  8,  9],</span></span><br><span class="line"><span class="string">        [12, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.narrow()</code>在指定维度缩小张量，可以简单理解为类似切片，<code>tensor[start: start + length]</code></p><p><code>torch.narrow_copy()</code>与<code>torch.narrow()</code>相同，但返回的是副本而不是共享存储。</p><h3 id="select-index-select"><a href="#select-index-select" class="headerlink" title=".select/.index_select"></a>.select/.index_select</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第一张图片</span></span><br><span class="line"><span class="built_in">print</span>(torch.select(images, dim=<span class="number">0</span>, index=<span class="number">0</span>).shape)  <span class="comment"># torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取所有图片的第一个通道</span></span><br><span class="line"><span class="built_in">print</span>(torch.select(images, dim=<span class="number">1</span>, index=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第二张和第四张图片</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">0</span>, index=torch.tensor([<span class="number">1</span>, <span class="number">3</span>])).shape)  <span class="comment"># torch.Size([2, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 获取所有图片二三通道数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">1</span>, index=torch.tensor([<span class="number">1</span>, <span class="number">2</span>])).shape)  <span class="comment"># torch.Size([4, 2, 28, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取所有图片高数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">2</span>, index=torch.arange(<span class="number">0</span>, <span class="number">28</span>, <span class="number">2</span>)).shape)  <span class="comment"># torch.Size([4, 3, 14, 28])</span></span><br><span class="line"><span class="comment"># 间隔获取所有图片宽数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.index_select(images, dim=<span class="number">3</span>, index=torch.arange(<span class="number">0</span>, <span class="number">28</span>, <span class="number">2</span>)).shape)  <span class="comment"># torch.Size([4, 3, 28, 14])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.select()</code>沿着维度<code>dim</code>，在给定索引<code>index</code>对<code>input</code>张量进行切片，等价于切片。比如：<br><code>tensor.select(0, index)</code>等于<code>tensor[index]</code>。<br><code>tensor.select(2, index)</code>等于<code>tensor[:,:,index]</code>。</p><p><code>torch.index_select()</code>沿着维度<code>dim</code>对<code>input</code>张量进行索引。</p><h3 id="masked-select"><a href="#masked-select" class="headerlink" title=".masked_select"></a>.masked_select</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0.6614,  0.2669,  0.0617,  0.6213],</span></span><br><span class="line"><span class="string">        [-0.4519, -0.1661, -1.5228,  0.3817],</span></span><br><span class="line"><span class="string">        [-1.0276, -0.5631, -0.8923, -0.0583]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">mask = data.ge(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(mask)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False, False,  True],</span></span><br><span class="line"><span class="string">        [False, False, False, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.masked_select(data, mask))  <span class="comment"># tensor([0.6614, 0.2669, 0.0617, 0.6213, 0.3817])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.masked_select()</code>根据布尔掩码选择数据，返回的是一维数据。</p><h3 id="gather"><a href="#gather" class="headerlink" title=".gather"></a>.gather</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">16</span>).view(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 5, 15,  6,  4],</span></span><br><span class="line"><span class="string">        [11,  2,  7, 12],</span></span><br><span class="line"><span class="string">        [ 1,  0,  9,  8],</span></span><br><span class="line"><span class="string">        [10,  3, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">index = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(index)  <span class="comment"># tensor([[1, 0, 3, 2]])</span></span><br><span class="line"><span class="built_in">print</span>(index.t())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1],</span></span><br><span class="line"><span class="string">        [0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">0</span>, index))  <span class="comment"># tensor([[11, 15, 13,  8]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">0</span>, index.t()))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[11],</span></span><br><span class="line"><span class="string">        [ 5],</span></span><br><span class="line"><span class="string">        [10],</span></span><br><span class="line"><span class="string">        [ 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">1</span>, index))  <span class="comment"># tensor([[15,  5,  4,  6]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.gather(data, <span class="number">1</span>, index.t()))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15],</span></span><br><span class="line"><span class="string">        [11],</span></span><br><span class="line"><span class="string">        [ 8],</span></span><br><span class="line"><span class="string">        [13]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><ul><li><p><code>torch.gather(data, 0, index)</code>对<code>0</code>维(行)进行</p><pre><code>  tensor([[1, 0, 3, 2]])
        第 0  1  2  3 列
</code></pre><p>索引：<br>[<b>1</b>][0] == 11<br>[<b>0</b>][1] == 15<br>[<b>3</b>][2] == 13<br>[<b>2</b>][3] == 8<br>加粗的是<code>行索引</code>的值<code>1, 0, 3, 2</code>，没加粗的为什么是<code>0, 1, 2, 3</code>呢？因为索引<code>1</code>是第<code>0</code>列，索引<code>0</code>是第<code>1</code>列，索引<code>3</code>是第<code>2</code>列，索引<code>2</code>是第<code>3</code>列。</p></li><li><p><code>torch.gather(data, 0, index.t())</code>对<code>0</code>维(行)进行</p><pre><code>  tensor([[1],   第0列
          [0],   第0列
          [3],   第0列
          [2]])  第0列
</code></pre><p>索引：<br>[<b>1</b>][0] == 11<br>[<b>0</b>][0] == 5<br>[<b>3</b>][0] == 10<br>[<b>2</b>][0] == 1<br>加粗的是<code>行索引</code>的值<code>1, 0, 3, 2</code>，没加粗的都是<code>0</code>，因为索引<code>1, 0, 3, 2</code>都是第<code>0</code>列。</p></li><li><p><code>torch.gather(data, 1, index.t())</code>对<code>1</code>维(列)进行</p><pre><code>  tensor([[1],   第0行
          [0],   第1行
          [3],   第2行
          [2]])  第3行
</code></pre><p>索引：<br>[0][<b>1</b>] == 15<br>[1][<b>0</b>] == 11<br>[2][<b>3</b>] == 8<br>[3][<b>2</b>] == 13<br>加粗的是<code>列索引</code>的值<code>1, 0, 3, 2</code>，没加粗的<code>0, 1, 2, 3</code>则是因为索引<code>1</code>是第<code>0</code>行，索引<code>0</code>是第<code>1</code>行，索引<code>3</code>是第<code>2</code>行，索引<code>2</code>是第<code>3</code>行。</p></li></ul><h3 id="take-take-along-dim"><a href="#take-take-along-dim" class="headerlink" title=".take/.take_along_dim"></a>.take/.take_along_dim</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">16</span>).view(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 5, 15,  6,  4],</span></span><br><span class="line"><span class="string">        [11,  2,  7, 12],</span></span><br><span class="line"><span class="string">        [ 1,  0,  9,  8],</span></span><br><span class="line"><span class="string">        [10,  3, 13, 14]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">index1 = torch.tensor([<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>])</span><br><span class="line">index2 = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.take(data, index1))  <span class="comment"># tensor([ 5,  2,  9, 14])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index1))  <span class="comment"># tensor([ 5,  2,  9, 14])</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2, dim=<span class="number">0</span>))  <span class="comment"># tensor([[11, 15, 13,  8]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2.t(), dim=<span class="number">0</span>))  <span class="comment"># 这里与torch.gather不同</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[11,  2,  7, 12],</span></span><br><span class="line"><span class="string">        [ 5, 15,  6,  4],</span></span><br><span class="line"><span class="string">        [10,  3, 13, 14],</span></span><br><span class="line"><span class="string">        [ 1,  0,  9,  8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2, dim=<span class="number">1</span>))  <span class="comment"># 这里与torch.gather不同</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15,  5,  4,  6],</span></span><br><span class="line"><span class="string">        [ 2, 11, 12,  7],</span></span><br><span class="line"><span class="string">        [ 0,  1,  8,  9],</span></span><br><span class="line"><span class="string">        [ 3, 10, 14, 13]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.take_along_dim(data, index2.t(), dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15],</span></span><br><span class="line"><span class="string">        [11],</span></span><br><span class="line"><span class="string">        [ 8],</span></span><br><span class="line"><span class="string">        [13]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.take_along_dim()</code>当<code>dim=None</code>时，等价于<code>torch.take()</code>，先把张量打平转成<code>1</code>维在根据索引获取元素。</p><p>当<code>dim</code>不等于<code>None</code>时，则与<code>data.gather()</code>相似。</p><h3 id="argwhere-nonzero"><a href="#argwhere-nonzero" class="headerlink" title=".argwhere/.nonzero"></a>.argwhere/.nonzero</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">data2 = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="built_in">print</span>(data1)  <span class="comment"># tensor([1, 0, 1, 0, 1, 0])</span></span><br><span class="line"><span class="built_in">print</span>(data2)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1, 0, 1],</span></span><br><span class="line"><span class="string">        [0, 1, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.argwhere(data1))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0],</span></span><br><span class="line"><span class="string">        [2],</span></span><br><span class="line"><span class="string">        [4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.argwhere(data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 0],</span></span><br><span class="line"><span class="string">        [0, 2],</span></span><br><span class="line"><span class="string">        [1, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data1))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0],</span></span><br><span class="line"><span class="string">        [2],</span></span><br><span class="line"><span class="string">        [4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data1, as_tuple=<span class="literal">True</span>))  <span class="comment"># (tensor([0, 2, 4]),)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 0],</span></span><br><span class="line"><span class="string">        [0, 2],</span></span><br><span class="line"><span class="string">        [1, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.nonzero(data2, as_tuple=<span class="literal">True</span>))  <span class="comment"># (tensor([0, 0, 1]), tensor([0, 2, 1]))</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.argwhere()</code>和<code>torch.nonzero()</code>都是返回非<code>0</code>元素的索引。</p><p>当<code>torch.nonzero()</code>参数<code>as_tuple=False</code>时，效果与<code>torch.argwhere()</code>相同。</p><h3 id="where"><a href="#where" class="headerlink" title=".where"></a>.where</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">condition = torch.randn(<span class="number">3</span>, <span class="number">5</span>) &gt; <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(condition)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True,  True,  True,  True, False],</span></span><br><span class="line"><span class="string">        [False, False,  True, False, False],</span></span><br><span class="line"><span class="string">        [False, False, False, False,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data1 = torch.ones(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1., 1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data2 = torch.full_like(data1, <span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(data2)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[100., 100., 100., 100., 100.],</span></span><br><span class="line"><span class="string">        [100., 100., 100., 100., 100.],</span></span><br><span class="line"><span class="string">        [100., 100., 100., 100., 100.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 满足条件返回 data1, 不满足条件返回 data2</span></span><br><span class="line"><span class="built_in">print</span>(torch.where(condition, data1, data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[  1.,   1.,   1.,   1., 100.],</span></span><br><span class="line"><span class="string">        [100., 100.,   1., 100., 100.],</span></span><br><span class="line"><span class="string">        [100., 100., 100., 100.,   1.]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="unravel-index"><a href="#unravel-index" class="headerlink" title=".unravel_index"></a>.unravel_index</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">9</span>, <span class="number">18</span>).view(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 9, 10, 11],</span></span><br><span class="line"><span class="string">        [12, 13, 14],</span></span><br><span class="line"><span class="string">        [15, 16, 17]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.unravel_index(torch.tensor(<span class="number">2</span>), shape=(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># (tensor(0), tensor(2))</span></span><br><span class="line"><span class="built_in">print</span>(torch.unravel_index(torch.tensor([<span class="number">4</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">17</span>]), shape=(<span class="number">3</span>, <span class="number">3</span>)))  <span class="comment"># (tensor([1, 2, 0, 2]), tensor([1, 0, 0, 2]))</span></span><br></pre></td></tr></tbody></table></figure><p>索引<code>2</code>在<code>shape</code>为<code>(3, 3)</code>的<code>0</code>行<code>2</code>列。<br>索引<code>9</code>可以理解为<code>9 / prod(shape) == 9 % 9 == 0</code>，所以在<code>0</code>行<code>0</code>列。<br>索引<code>17</code>可以理解为<code>17 / prod(shape) == 17 % 9 == 8</code>，所以在<code>2</code>行<code>2</code>列。</p><h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><h3 id="t-transpose-movedim-permute"><a href="#t-transpose-movedim-permute" class="headerlink" title=".t/.transpose/.movedim/.permute"></a>.t/.transpose/.movedim/.permute</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(data)  <span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])</span></span><br><span class="line"><span class="built_in">print</span>(torch.t(data))  <span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])</span></span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.t(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 3, 6],</span></span><br><span class="line"><span class="string">        [1, 4, 7],</span></span><br><span class="line"><span class="string">        [2, 5, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 1080，宽 1920 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">1080</span>, <span class="number">1920</span>)</span><br><span class="line"><span class="comment"># BCHW</span></span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 1080, 1920])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BCHW --&gt; BWHC --&gt; BHWC</span></span><br><span class="line"><span class="built_in">print</span>(images.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).shape)  <span class="comment"># torch.Size([4, 1080, 1920, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BCHW --&gt; BHWC</span></span><br><span class="line"><span class="built_in">print</span>(images.movedim(<span class="number">1</span>, <span class="number">3</span>).shape)  <span class="comment"># torch.Size([4, 1080, 1920, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BCHW --&gt; BHWC</span></span><br><span class="line"><span class="built_in">print</span>(images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 1080, 1920, 3])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.t(input)</code>只能处理维度小于等于<code>2</code>的，否则会报错。当维度是<code>0</code>或<code>1</code>维时，返回相同的结果，当维度为<code>2</code>时，等价于<code>torch.transpose(input, 0, 1)</code></p><p><code>torch.transpose(input, dim0, dim1)</code>一次只能操作两个维度，对调两个维度的位置。</p><p><code>torch.movedim(input, source, destination)</code>将<code>source</code>维度移动到<code>destination</code>维度。</p><p><code>torch.permute(input, dims)</code>一次可以操作多个维度，<code>dims</code>指定所有维度的顺序。在多维度操作上使用<code>torch.permute()</code>更直观。</p><p><code>torch.swapaxes()</code>是<code>torch.transpose()</code>的别名。<br><code>torch.swapdims()</code>是<code>torch.transpose()</code>的别名。<br><code>torch.moveaxis()</code>是<code>torch.movedim()</code>的别名。</p><h3 id="view-reshape"><a href="#view-reshape" class="headerlink" title=".view/.reshape"></a>.view/.reshape</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张 3 通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(images.shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(images.view(<span class="number">4</span>, <span class="number">3</span> * <span class="number">28</span> * <span class="number">28</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># -1 会自动计算出数值</span></span><br><span class="line"><span class="built_in">print</span>(images.view(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># print(images.transpose(1, 3).transpose(1, 2).view(4, -1).shape)  # 报错</span></span><br><span class="line"><span class="built_in">print</span>(images.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># print(images.permute(0, 2, 3, 1).view(4, -1).shape)  # 报错</span></span><br><span class="line"><span class="built_in">print</span>(images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous().view(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(images.reshape(<span class="number">4</span>, <span class="number">3</span> * <span class="number">28</span> * <span class="number">28</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="comment"># -1 会自动计算出数值</span></span><br><span class="line"><span class="built_in">print</span>(images.reshape(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="built_in">print</span>(images.transpose(<span class="number">1</span>, <span class="number">3</span>).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br><span class="line"><span class="built_in">print</span>(images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(<span class="number">4</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 2352])</span></span><br></pre></td></tr></tbody></table></figure><p><code>view()</code>和<code>reshape()</code>都可以改变<code>Tensor</code>的维度，区别是：</p><p><code>view()</code>只能对满足连续性的张量进行转换，当对不满足连续性的张量进行操作时会报<code>RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</code>错误。<code>transpose()</code>和<code>permute()</code>会改变张量连续性，使用<code>view()</code>前需要先执行<code>contiguous()</code>。</p><p><code>reshape()</code>则没有上述要求，可以直接使用，无需先执行<code>contiguous()</code>。</p><h3 id="squeeze-unsqueeze"><a href="#squeeze-unsqueeze" class="headerlink" title=".squeeze/.unsqueeze"></a>.squeeze/.unsqueeze</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(data.shape)  <span class="comment"># torch.Size([2, 1, 2, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim=None，会把能挤压的维度挤压</span></span><br><span class="line"><span class="built_in">print</span>(torch.squeeze(data).shape)  <span class="comment"># torch.Size([2, 2])</span></span><br><span class="line"><span class="comment"># 第一维度是 2 没法挤压，所以保持不变</span></span><br><span class="line"><span class="built_in">print</span>(torch.squeeze(data, dim=<span class="number">0</span>).shape)  <span class="comment"># torch.Size([2, 1, 2, 1])</span></span><br><span class="line"><span class="comment"># 第二维度挤压</span></span><br><span class="line"><span class="built_in">print</span>(torch.squeeze(data, dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([2, 2, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第一维增加维度</span></span><br><span class="line"><span class="built_in">print</span>(torch.unsqueeze(data, dim=<span class="number">0</span>).shape)  <span class="comment"># torch.Size([1, 2, 1, 2, 1])</span></span><br><span class="line"><span class="comment"># 在最后一维增加维度</span></span><br><span class="line"><span class="built_in">print</span>(torch.unsqueeze(data, dim=-<span class="number">1</span>).shape)  <span class="comment"># torch.Size([2, 1, 2, 1, 1])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="expand-repeat"><a href="#expand-repeat" class="headerlink" title=".expand/.repeat"></a>.expand/.repeat</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 张单通道，高 28，宽 28 的图片</span></span><br><span class="line">images = torch.rand(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">data = torch.empty(<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二维度扩展到 3 通道，-1 表示不扩展保持原样</span></span><br><span class="line"><span class="built_in">print</span>(images.expand(-<span class="number">1</span>, <span class="number">3</span>, -<span class="number">1</span>, -<span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="built_in">print</span>(images.expand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">28</span>, <span class="number">28</span>).shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 扩展到和 images 相同的 shape</span></span><br><span class="line"><span class="built_in">print</span>(data.expand_as(images).shape)  <span class="comment"># torch.Size([4, 1, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 各维度重复指定次数的数据</span></span><br><span class="line"><span class="built_in">print</span>(images.repeat(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).shape)  <span class="comment"># torch.Size([4, 3, 28, 28])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="tile"><a href="#tile" class="headerlink" title=".tile"></a>.tile</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果 dims 指定的维度小于 input 的维度，会扩展到相同的维度，比如 (2,) 会扩展到 (1, 2)</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">2</span>,)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">        [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">        [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 如果 dims 指定的维度大于 input 的维度，input 会扩展到相同的维度，比如 input 是 (3, 3) 会扩展到 (1, 3, 3)</span></span><br><span class="line"><span class="built_in">print</span>(torch.tile(data, dims=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">         [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">         [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">         [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">         [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">         [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">         [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">         [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">         [6, 7, 8, 6, 7, 8]]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h2 id="合并与拆分"><a href="#合并与拆分" class="headerlink" title="合并与拆分"></a>合并与拆分</h2><h3 id="cat"><a href="#cat" class="headerlink" title=".cat"></a>.cat</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data1 = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">data2 = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 行合并</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat([data1, data1]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999],</span></span><br><span class="line"><span class="string">        [0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 列合并</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat([data1, data1], dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.7576, 0.2793, 0.4031],</span></span><br><span class="line"><span class="string">        [0.7347, 0.0293, 0.7999, 0.7347, 0.0293, 0.7999]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.concat()</code>是<code>torch.cat()</code>的别名。<br><code>torch.concatenate()</code>是<code>torch.cat()</code>的别名。</p><h3 id="stack-hstack-vstack-column-stack-dstack"><a href="#stack-hstack-vstack-column-stack-dstack" class="headerlink" title=".stack/.hstack/.vstack/.column_stack/.dstack"></a>.stack/.hstack/.vstack/.column_stack/.dstack</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor(<span class="number">9</span>)</span><br><span class="line">data1 = torch.arange(<span class="number">9</span>)</span><br><span class="line">data2 = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data3 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data4 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data5 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.shape)  <span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(data1.shape)  <span class="comment"># torch.Size([9])</span></span><br><span class="line"><span class="built_in">print</span>(data2.shape)  <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(data3.shape)  <span class="comment"># torch.Size([1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(data4.shape)  <span class="comment"># torch.Size([1, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(data5.shape)  <span class="comment"># torch.Size([1, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim 默认为 0，在第一维度堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data, data]).shape)  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data1, data1]).shape)  <span class="comment"># torch.Size([2, 9])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data2, data2]).shape)  <span class="comment"># torch.Size([2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data3, data3]).shape)  <span class="comment"># torch.Size([2, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data4, data4]).shape)  <span class="comment"># torch.Size([2, 1, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data5, data5]).shape)  <span class="comment"># torch.Size([2, 1, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim=1，在第二维度堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data1, data1], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([9, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data2, data2], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([3, 2, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data3, data3], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([1, 2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data4, data4], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([1, 2, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.stack([data5, data5], dim=<span class="number">1</span>).shape)  <span class="comment"># torch.Size([1, 2, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 水平堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data, data]).shape)  <span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data1, data1]).shape)  <span class="comment"># torch.Size([18])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data2, data2]).shape)  <span class="comment"># torch.Size([3, 6])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data3, data3]).shape)  <span class="comment"># torch.Size([1, 6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data4, data4]).shape)  <span class="comment"># torch.Size([1, 2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.hstack([data5, data5]).shape)  <span class="comment"># torch.Size([1, 2, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 垂直堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data, data]).shape)  <span class="comment"># torch.Size([2, 1])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data1, data1]).shape)  <span class="comment"># torch.Size([2, 9])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data2, data2]).shape)  <span class="comment"># torch.Size([6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data3, data3]).shape)  <span class="comment"># torch.Size([2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data4, data4]).shape)  <span class="comment"># torch.Size([2, 1, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.vstack([data5, data5]).shape)  <span class="comment"># torch.Size([2, 1, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 列堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data, data]).shape)  <span class="comment"># torch.Size([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data1, data1]).shape)  <span class="comment"># torch.Size([9, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data2, data2]).shape)  <span class="comment"># torch.Size([3, 6])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data3, data3]).shape)  <span class="comment"># torch.Size([1, 6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data4, data4]).shape)  <span class="comment"># torch.Size([1, 2, 3, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.column_stack([data5, data5]).shape)  <span class="comment"># torch.Size([1, 2, 1, 3, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度堆叠，在第 3 维度堆叠</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data, data)).shape)  <span class="comment"># torch.Size([1, 1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data1, data1)).shape)  <span class="comment"># torch.Size([1, 9, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data2, data2)).shape)  <span class="comment"># torch.Size([3, 3, 2])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data3, data3)).shape)  <span class="comment"># torch.Size([1, 3, 6])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data4, data4)).shape)  <span class="comment"># torch.Size([1, 1, 6, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.dstack((data5, data5)).shape)  <span class="comment"># torch.Size([1, 1, 2, 3, 3])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.hstack)</code>按水平方向（列方向）依次堆叠张量。</p><p><code>torch.vstack)</code>按垂直方向（行方向）依次堆叠张量。</p><p><code>torch.column_stack()</code>除了张量是<code>0</code>维和<code>1</code>维外，等价于<code>torch.hstack()</code>。当张量<code>t</code>是<code>0</code>维或<code>1</code>维时，先<code>reshape</code>重塑为<code>(t.numel(), 1)</code>再水平堆叠。</p><p><code>torch.row_stack()</code>是<code>torch.vstack()</code>的别名。</p><p><code>torch.dstack()</code>在第<code>3</code>维度进行堆叠。</p><p><code>torch.stack()</code>是一个更通用的函数，通过<code>dim</code>指定在任意维度进行堆叠，<code>dim</code>默认等于<code>0</code>。它总是增加一个新的维度来堆叠张量。</p><h3 id="chunk"><a href="#chunk" class="headerlink" title=".chunk"></a>.chunk</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">20</span>).view(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0,  1,  2,  3,  4],</span></span><br><span class="line"><span class="string">        [ 5,  6,  7,  8,  9],</span></span><br><span class="line"><span class="string">        [10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="string">        [15, 16, 17, 18, 19]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按行分成 3 块（PS: 这里实际只分成两块，因为每块大小是 2，4 行只能分成两块）</span></span><br><span class="line"><span class="built_in">print</span>(torch.chunk(data, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0, 1, 2, 3, 4],</span></span><br><span class="line"><span class="string">        [5, 6, 7, 8, 9]]), tensor([[10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="string">        [15, 16, 17, 18, 19]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 按列分成 2 块</span></span><br><span class="line"><span class="built_in">print</span>(torch.chunk(data, <span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[ 0,  1,  2],</span></span><br><span class="line"><span class="string">        [ 5,  6,  7],</span></span><br><span class="line"><span class="string">        [10, 11, 12],</span></span><br><span class="line"><span class="string">        [15, 16, 17]]), tensor([[ 3,  4],</span></span><br><span class="line"><span class="string">        [ 8,  9],</span></span><br><span class="line"><span class="string">        [13, 14],</span></span><br><span class="line"><span class="string">        [18, 19]]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="split-hsplit-vsplit-dsplit-tensor-split"><a href="#split-hsplit-vsplit-dsplit-tensor-split" class="headerlink" title=".split/.hsplit/.vsplit/.dsplit/.tensor_split"></a>.split/.hsplit/.vsplit/.dsplit/.tensor_split</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.arange(<span class="number">9</span>)</span><br><span class="line">data2 = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">data3 = torch.arange(<span class="number">9</span>).view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按每两个一组分割</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data1, <span class="number">2</span>))  <span class="comment"># (tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6, 7]), tensor([8]))</span></span><br><span class="line"><span class="comment"># 分割成 3 组，第一组 1 份，第二组 3 份，第三组 5 份</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data1, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]))  <span class="comment"># (tensor([0]), tensor([1, 2, 3]), tensor([4, 5, 6, 7, 8]))</span></span><br><span class="line"><span class="comment"># 按每两个一组分割</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data2, <span class="number">2</span>, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0, 1],</span></span><br><span class="line"><span class="string">        [3, 4],</span></span><br><span class="line"><span class="string">        [6, 7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 分割成 2 组，第一组 1 份，第二组 2 份</span></span><br><span class="line"><span class="built_in">print</span>(torch.split(data2, [<span class="number">1</span>, <span class="number">2</span>], dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [4, 5],</span></span><br><span class="line"><span class="string">        [7, 8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将具有一维或多维的张量 input 水平拆分为多个张量</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data1, <span class="number">3</span>))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]))</span></span><br><span class="line"><span class="comment"># 以索引 3、5、7 分成 4 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data1, [<span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>]))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]), tensor([7, 8]))</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data2, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1],</span></span><br><span class="line"><span class="string">        [4],</span></span><br><span class="line"><span class="string">        [7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 以索引 1 分成 2 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.hsplit(data2, [<span class="number">1</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [4, 5],</span></span><br><span class="line"><span class="string">        [7, 8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将具有二维或多维的张量 input 垂直拆分为多个张量</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.vsplit(data2, <span class="number">3</span>))  <span class="comment"># (tensor([[0, 1, 2]]), tensor([[3, 4, 5]]), tensor([[6, 7, 8]]))</span></span><br><span class="line"><span class="comment"># 以索引 2 分成 2 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.vsplit(data2, [<span class="number">2</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5]]), tensor([[6, 7, 8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将具有三个或更多维度的张量 input 在深度方向上拆分为多个张量</span></span><br><span class="line"><span class="comment"># 平均分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.dsplit(data3, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[[0],</span></span><br><span class="line"><span class="string">         [3],</span></span><br><span class="line"><span class="string">         [6]]]), tensor([[[1],</span></span><br><span class="line"><span class="string">         [4],</span></span><br><span class="line"><span class="string">         [7]]]), tensor([[[2],</span></span><br><span class="line"><span class="string">         [5],</span></span><br><span class="line"><span class="string">         [8]]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 以索引 1、2 分成 3 段</span></span><br><span class="line"><span class="built_in">print</span>(torch.dsplit(data3, [<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[[0],</span></span><br><span class="line"><span class="string">         [3],</span></span><br><span class="line"><span class="string">         [6]]]), tensor([[[1],</span></span><br><span class="line"><span class="string">         [4],</span></span><br><span class="line"><span class="string">         [7]]]), tensor([[[2],</span></span><br><span class="line"><span class="string">         [5],</span></span><br><span class="line"><span class="string">         [8]]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.tensor_split(data1, <span class="number">3</span>))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]))</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor_split(data2, <span class="number">3</span>, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1],</span></span><br><span class="line"><span class="string">        [4],</span></span><br><span class="line"><span class="string">        [7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor_split(data3, <span class="number">3</span>, dim=<span class="number">2</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">(tensor([[0],</span></span><br><span class="line"><span class="string">        [3],</span></span><br><span class="line"><span class="string">        [6]]), tensor([[1],</span></span><br><span class="line"><span class="string">        [4],</span></span><br><span class="line"><span class="string">        [7]]), tensor([[2],</span></span><br><span class="line"><span class="string">        [5],</span></span><br><span class="line"><span class="string">        [8]]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.hsplit(input, indices_or_sections)</code>当<code>input</code>是<code>1</code>维时，等价于<code>torch.tensor_split(input, indices_or_sections, dim=0)</code>，当<code>input</code>是大于等于<code>2</code>维时，等价于<code>torch.tensor_split(input, indices_or_sections, dim=1)</code>。<code>indices_or_sections</code>如果是数字，必须能被整除，否则会抛出异常。</p><p><code>torch.vsplit(input, indices_or_sections)</code>等价于<code>torch.tensor_split(input, indices_or_sections, dim=0)</code>。<code>input</code>必须大于等于<code>2</code>维。<code>indices_or_sections</code>如果是数字，必须能被整除，否则会抛出异常。</p><p><code>torch.dsplit(input, indices_or_sections)</code>等价于<code>torch.tensor_split(input, indices_or_sections, dim=2)</code>。<code>input</code>必须大于等于<code>3</code>维。<code>indices_or_sections</code>如果是数字，必须能被整除，否则会抛出异常。</p><h3 id="unbind"><a href="#unbind" class="headerlink" title=".unbind"></a>.unbind</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 1, 2],</span></span><br><span class="line"><span class="string">        [3, 4, 5],</span></span><br><span class="line"><span class="string">        [6, 7, 8]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按行解绑</span></span><br><span class="line"><span class="built_in">print</span>(torch.unbind(data))  <span class="comment"># (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]))</span></span><br><span class="line"><span class="comment"># 按列解绑</span></span><br><span class="line"><span class="built_in">print</span>(torch.unbind(data, dim=<span class="number">1</span>))  <span class="comment"># (tensor([0, 3, 6]), tensor([1, 4, 7]), tensor([2, 5, 8]))</span></span><br></pre></td></tr></tbody></table></figure><h2 id="逐点运算"><a href="#逐点运算" class="headerlink" title="逐点运算"></a>逐点运算</h2><h3 id="add-sub-mul-div-remainder-fmod-positive-neg-abs"><a href="#add-sub-mul-div-remainder-fmod-positive-neg-abs" class="headerlink" title=".add/.sub/.mul/.div/.remainder/.fmod/.positive/.neg/.abs"></a>.add/.sub/.mul/.div/.remainder/.fmod/.positive/.neg/.abs</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randint(-<span class="number">10</span>, <span class="number">10</span>, (<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-5,  9, -6, -2, -7],</span></span><br><span class="line"><span class="string">        [ 3,  1, -9,  9,  2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加</span></span><br><span class="line"><span class="built_in">print</span>(torch.add(data, <span class="number">2</span>))  <span class="comment"># 等价于 data + 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-3, 11, -4,  0, -5],</span></span><br><span class="line"><span class="string">        [ 5,  3, -7, 11,  4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减</span></span><br><span class="line"><span class="built_in">print</span>(torch.sub(data, <span class="number">2</span>))  <span class="comment"># 等价于 data - 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ -7,   7,  -8,  -4,  -9],</span></span><br><span class="line"><span class="string">        [  1,  -1, -11,   7,   0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.mul(data, <span class="number">2</span>))  <span class="comment"># 等价于 data * 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-10,  18, -12,  -4, -14],</span></span><br><span class="line"><span class="string">        [  6,   2, -18,  18,   4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 除</span></span><br><span class="line"><span class="built_in">print</span>(torch.div(data, <span class="number">2</span>))  <span class="comment"># 等价于 data / 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-2.5000,  4.5000, -3.0000, -1.0000, -3.5000],</span></span><br><span class="line"><span class="string">        [ 1.5000,  0.5000, -4.5000,  4.5000,  1.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 除完向下取整</span></span><br><span class="line"><span class="built_in">print</span>(torch.div(data, <span class="number">2</span>, rounding_mode=<span class="string">'floor'</span>))  <span class="comment"># 等价于 data // 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-3,  4, -3, -1, -4],</span></span><br><span class="line"><span class="string">        [ 1,  0, -5,  4,  1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 除完只保留整数</span></span><br><span class="line"><span class="built_in">print</span>(torch.div(data, <span class="number">2</span>, rounding_mode=<span class="string">'trunc'</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-2,  4, -3, -1, -3],</span></span><br><span class="line"><span class="string">        [ 1,  0, -4,  4,  1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取余不保留负数</span></span><br><span class="line"><span class="built_in">print</span>(torch.remainder(data, <span class="number">2</span>))  <span class="comment"># 等价于 data % 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1, 1, 0, 0, 1],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 1, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取余保留负数</span></span><br><span class="line"><span class="built_in">print</span>(torch.fmod(data, <span class="number">2</span>))  <span class="comment"># 等价于 data % -2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-1,  1,  0,  0, -1],</span></span><br><span class="line"><span class="string">        [ 1,  1, -1,  1,  0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加号</span></span><br><span class="line"><span class="built_in">print</span>(torch.positive(data))  <span class="comment"># 等价于 +data</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[-5,  9, -6, -2, -7],</span></span><br><span class="line"><span class="string">        [ 3,  1, -9,  9,  2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减号</span></span><br><span class="line"><span class="built_in">print</span>(torch.neg(data))  <span class="comment"># 等价于 -data</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 5, -9,  6,  2,  7],</span></span><br><span class="line"><span class="string">        [-3, -1,  9, -9, -2]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绝对值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">abs</span>(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5, 9, 6, 2, 7],</span></span><br><span class="line"><span class="string">        [3, 1, 9, 9, 2]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.subtract()</code>是<code>torch.sub()</code>的别名。<br><code>torch.multiply()</code>是<code>torch.mul()</code>的别名。<br><code>torch.divide()</code>是<code>torch.div()</code>的别名。<br><code>torch.true_divide()</code>是<code>torch.div()</code>在<code>rounding_mode=None</code>时的别名。<br><code>torch.negative()</code>是<code>torch.neg()</code>的别名。<br><code>torch.absolute()</code>是<code>torch.abs()</code>的别名。</p><p>在<code>PyTorch 1.13</code>后（含1.13），可以认为<code>torch.floor_divide()</code>是<code>torch.div()</code>在<code>rounding_mode='floor'</code>时的别名，效果是一样的。</p><h3 id="pow-square-sqrt-rsqrt-reciprocal"><a href="#pow-square-sqrt-rsqrt-reciprocal" class="headerlink" title=".pow/.square/.sqrt/.rsqrt/.reciprocal"></a>.pow/.square/.sqrt/.rsqrt/.reciprocal</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[9, 9],</span></span><br><span class="line"><span class="string">        [9, 9]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 次方</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">pow</span>(data, <span class="number">3</span>))  <span class="comment"># 等价于 data ** 3</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[729, 729],</span></span><br><span class="line"><span class="string">        [729, 729]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平方</span></span><br><span class="line"><span class="built_in">print</span>(torch.square(data))  <span class="comment"># 等价于 data ** 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[81, 81],</span></span><br><span class="line"><span class="string">        [81, 81]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开方</span></span><br><span class="line"><span class="built_in">print</span>(torch.sqrt(data))  <span class="comment"># 等价于 data ** 0.5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开方倒数</span></span><br><span class="line"><span class="built_in">print</span>(torch.rsqrt(data))  <span class="comment"># 等价于 data ** -0.5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.3333, 0.3333],</span></span><br><span class="line"><span class="string">        [0.3333, 0.3333]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 倒数</span></span><br><span class="line"><span class="built_in">print</span>(torch.reciprocal(data))  <span class="comment"># 等价于 data ** -1</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.1111, 0.1111],</span></span><br><span class="line"><span class="string">        [0.1111, 0.1111]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="exp-log-log2-log10-log1p"><a href="#exp-log-log2-log10-log1p" class="headerlink" title=".exp/.log/.log2/.log10/.log1p"></a>.exp/.log/.log2/.log10/.log1p</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.exp(torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br><span class="line"><span class="built_in">print</span>(data1)  <span class="comment"># tensor([ 1.0000,  2.7183,  7.3891, 20.0855, 54.5981])</span></span><br><span class="line"></span><br><span class="line">data2 = torch.exp(torch.ones(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(data2)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2.7183, 2.7183],</span></span><br><span class="line"><span class="string">        [2.7183, 2.7183]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 e 为底</span></span><br><span class="line"><span class="built_in">print</span>(torch.log(data1))  <span class="comment"># tensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.log(data2))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1.0000, 1.0000],</span></span><br><span class="line"><span class="string">        [1.0000, 1.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data3 = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>])</span><br><span class="line"><span class="comment"># 以 2 为底</span></span><br><span class="line"><span class="built_in">print</span>(torch.log2(data3))  <span class="comment"># tensor([0., 1., 2., 3., 4.])</span></span><br><span class="line"></span><br><span class="line">data4 = torch.tensor([<span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>, <span class="number">10000</span>])</span><br><span class="line"><span class="comment"># 以 10 为底</span></span><br><span class="line"><span class="built_in">print</span>(torch.log10(data4))  <span class="comment"># tensor([0., 1., 2., 3., 4.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 e 为底，input + 1 做为真数</span></span><br><span class="line"><span class="built_in">print</span>(torch.log1p(data1 - <span class="number">1</span>))  <span class="comment"># tensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.log1p(data2 - <span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1.0000, 1.0000],</span></span><br><span class="line"><span class="string">        [1.0000, 1.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.log1p()</code>计算<code>input + 1</code>的自然对数：<script type="math/tex">y_i = ln(x_i+1)</script>。</p><h3 id="sin-cos-tan-asin-acos-atan-atan2-sinh-cosh-tanh"><a href="#sin-cos-tan-asin-acos-atan-atan2-sinh-cosh-tanh" class="headerlink" title=".sin/.cos/.tan/.asin/.acos/.atan/.atan2/.sinh/.cosh/.tanh"></a>.sin/.cos/.tan/.asin/.acos/.atan/.atan2/.sinh/.cosh/.tanh</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.deg2rad(torch.arange(<span class="number">0</span>, <span class="number">361</span>, <span class="number">30</span>))  <span class="comment"># 角度转弧度</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.sin(data))</span><br><span class="line"><span class="comment"># 余弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.cos(data))</span><br><span class="line"><span class="comment"># 正切</span></span><br><span class="line"><span class="built_in">print</span>(torch.tan(data))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反正弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.asin(data))</span><br><span class="line"><span class="comment"># 反余弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.acos(data))</span><br><span class="line"><span class="comment"># 反正切</span></span><br><span class="line"><span class="built_in">print</span>(torch.atan(data))</span><br><span class="line"><span class="built_in">print</span>(torch.atan2(data, data))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 双曲正弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.sinh(data))</span><br><span class="line"><span class="comment"># 双曲余弦</span></span><br><span class="line"><span class="built_in">print</span>(torch.cosh(data))</span><br><span class="line"><span class="comment"># 双曲正切</span></span><br><span class="line"><span class="built_in">print</span>(torch.tanh(data))</span><br></pre></td></tr></tbody></table></figure><p><code>torch.arcsin()</code>是<code>torch.asin()</code>的别名。<br><code>torch.arccos()</code>是<code>torch.acos()</code>的别名。<br><code>torch.arctan()</code>是<code>torch.atan()</code>的别名。<br><code>torch.arctan2()</code>是<code>torch.atan2()</code>的别名。<br><code>torch.arcsinh()</code>是<code>torch.asinh()</code>的别名。<br><code>torch.arccosh()</code>是<code>torch.acosh()</code>的别名。<br><code>torch.arctanh()</code>是<code>torch.atanh()</code>的别名。</p><h3 id="angle-deg2rad"><a href="#angle-deg2rad" class="headerlink" title=".angle/.deg2rad"></a>.angle/.deg2rad</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复数转弧度，结果在（π, -π）之间</span></span><br><span class="line"><span class="built_in">print</span>(torch.angle(torch.tensor([[<span class="number">1</span>, <span class="number">1</span> + <span class="number">1j</span>, <span class="number">1j</span>, -<span class="number">1</span> + <span class="number">1j</span>, -<span class="number">1</span>, -<span class="number">1</span> -<span class="number">1j</span>, -<span class="number">1j</span>, <span class="number">1</span> -<span class="number">1j</span>]])))  <span class="comment"># tensor([[ 0.0000,  0.7854,  1.5708,  2.3562,  3.1416, -2.3562, -1.5708, -0.7854]])</span></span><br><span class="line"><span class="comment"># 弧度转角度</span></span><br><span class="line"><span class="built_in">print</span>(torch.angle(torch.tensor([[<span class="number">1</span>, <span class="number">1</span> + <span class="number">1j</span>, <span class="number">1j</span>, -<span class="number">1</span> + <span class="number">1j</span>, -<span class="number">1</span>, -<span class="number">1</span> -<span class="number">1j</span>, -<span class="number">1j</span>, <span class="number">1</span> -<span class="number">1j</span>]])) * <span class="number">180</span> / np.pi)  <span class="comment"># tensor([[   0.0000,   45.0000,   90.0000,  135.0000,  180.0000, -135.0000, -90.0000,  -45.0000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 角度转弧度</span></span><br><span class="line"><span class="built_in">print</span>(torch.deg2rad(torch.tensor([[<span class="number">0</span>, <span class="number">45</span>, <span class="number">90</span>, <span class="number">135</span>, <span class="number">180</span>, <span class="number">360</span>], [-<span class="number">360</span>, -<span class="number">180</span>, -<span class="number">135</span>, -<span class="number">90</span>, -<span class="number">45</span>, <span class="number">0</span>]])))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ 0.0000,  0.7854,  1.5708,  2.3562,  3.1416,  6.2832],</span></span><br><span class="line"><span class="string">        [-6.2832, -3.1416, -2.3562, -1.5708, -0.7854,  0.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="bitwise-and-bitwise-or-bitwise-not-bitwise-xor-bitwise-left-shift-bitwise-right-shift"><a href="#bitwise-and-bitwise-or-bitwise-not-bitwise-xor-bitwise-left-shift-bitwise-right-shift" class="headerlink" title=".bitwise_and/.bitwise_or/.bitwise_not/.bitwise_xor/.bitwise_left_shift/.bitwise_right_shift"></a>.bitwise_and/.bitwise_or/.bitwise_not/.bitwise_xor/.bitwise_left_shift/.bitwise_right_shift</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位与</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_and(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([1, 0, 3], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_and(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([False,  True, False])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位或</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_or(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-1, -2,  3], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_or(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([ True,  True, False])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位非</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_not(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([ 0,  1, -4], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_not(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([False, False,  True])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位异或</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_xor(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-2, -2,  0], dtype=torch.int8)</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_xor(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([ True, False, False])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 左移</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_left_shift(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-2, -2, 24], dtype=torch.int8)</span></span><br><span class="line"><span class="comment"># 右移</span></span><br><span class="line"><span class="built_in">print</span>(torch.bitwise_right_shift(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8)))  <span class="comment"># tensor([-1, -2,  0], dtype=torch.int8)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="floor-ceil-round"><a href="#floor-ceil-round" class="headerlink" title=".floor/.ceil/.round"></a>.floor/.ceil/.round</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">1</span>, <span class="number">5</span>) * <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(data)  <span class="comment"># tensor([[7.5763, 2.7931, 4.0307, 7.3468, 0.2928]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 向下取整</span></span><br><span class="line"><span class="built_in">print</span>(torch.floor(data))  <span class="comment"># tensor([[7., 2., 4., 7., 0.]])</span></span><br><span class="line"><span class="comment"># 向上取整</span></span><br><span class="line"><span class="built_in">print</span>(torch.ceil(data))  <span class="comment"># tensor([[8., 3., 5., 8., 1.]])</span></span><br><span class="line"><span class="comment"># 四舍五入</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">round</span>(data))  <span class="comment"># tensor([[8., 3., 4., 7., 0.]])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.floor()</code>向下取整，<code>torch.ceil()</code>向上取整，<code>torch.round()</code>四舍五入。</p><h3 id="trunc-frac"><a href="#trunc-frac" class="headerlink" title=".trunc/.frac"></a>.trunc/.frac</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">1</span>, <span class="number">5</span>) * <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(data)  <span class="comment"># tensor([[7.5763, 2.7931, 4.0307, 7.3468, 0.2928]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取整数</span></span><br><span class="line"><span class="built_in">print</span>(torch.trunc(data))  <span class="comment"># tensor([[7., 2., 4., 7., 0.]])</span></span><br><span class="line"><span class="comment"># 取小数</span></span><br><span class="line"><span class="built_in">print</span>(torch.frac(data))  <span class="comment"># tensor([[0.5763, 0.7931, 0.0307, 0.3468, 0.2928]])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.trunc()</code>取整数，<code>torch.frac()</code>取小数。</p><p><code>torch.fix()</code>是<code>torch.trunc()</code>的别名。</p><h3 id="clamp-clamp-min-clamp-max"><a href="#clamp-clamp-min-clamp-max" class="headerlink" title=".clamp/.clamp_min/.clamp_max"></a>.clamp/.clamp_min/.clamp_max</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand((<span class="number">2</span>, <span class="number">5</span>)) * <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.1526,  5.5862,  8.0614, 14.6937,  0.5856],</span></span><br><span class="line"><span class="string">        [15.9972,  7.9427, 15.0874, 11.3902,  8.7756]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于 10 的数变为 10</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp(data, <span class="number">10</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.1526, 10.0000, 10.0000, 14.6937, 10.0000],</span></span><br><span class="line"><span class="string">        [15.9972, 10.0000, 15.0874, 11.3902, 10.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于 15 的数变为 15</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp(data, <span class="built_in">max</span>=<span class="number">15</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.0000,  5.5862,  8.0614, 14.6937,  0.5856],</span></span><br><span class="line"><span class="string">        [15.0000,  7.9427, 15.0000, 11.3902,  8.7756]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于 10 的数变为 10，大于 15 的数变为 15</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp(data, <span class="number">10</span>, <span class="number">15</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.0000, 10.0000, 10.0000, 14.6937, 10.0000],</span></span><br><span class="line"><span class="string">        [15.0000, 10.0000, 15.0000, 11.3902, 10.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于 10 的数变为 10</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp_min(data, <span class="number">10</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.1526, 10.0000, 10.0000, 14.6937, 10.0000],</span></span><br><span class="line"><span class="string">        [15.9972, 10.0000, 15.0874, 11.3902, 10.0000]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于 15 的数变为 15</span></span><br><span class="line"><span class="built_in">print</span>(torch.clamp_max(data, <span class="number">15</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[15.0000,  5.5862,  8.0614, 14.6937,  0.5856],</span></span><br><span class="line"><span class="string">        [15.0000,  7.9427, 15.0000, 11.3902,  8.7756]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.clamp()</code>用于将输入的张量夹紧到区间<code>[min, max]</code>。</p><p><code>torch.clip()</code>是<code>torch.clamp()</code>的别名。</p><h3 id="dot-mm-bmm-matmul"><a href="#dot-mm-bmm-matmul" class="headerlink" title=".dot/.mm/.bmm/.matmul"></a>.dot/.mm/.bmm/.matmul</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一维矩阵相乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.dot(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.tensor([<span class="number">3</span>, <span class="number">4</span>])))  <span class="comment"># tensor(11)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维矩阵相乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.mm(torch.full((<span class="number">2</span>, <span class="number">3</span>), <span class="number">2</span>), torch.full((<span class="number">3</span>, <span class="number">2</span>), <span class="number">3</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[18, 18],</span></span><br><span class="line"><span class="string">        [18, 18]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 三维矩阵相乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.bmm(torch.full((<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="number">2</span>), torch.full((<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>), <span class="number">3</span>)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[[18, 18],</span></span><br><span class="line"><span class="string">         [18, 18]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[18, 18],</span></span><br><span class="line"><span class="string">         [18, 18]]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">data1 = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">800</span>, <span class="number">600</span>)</span><br><span class="line"><span class="built_in">print</span>(data1.shape)  <span class="comment"># torch.Size([4, 3, 800, 600])</span></span><br><span class="line">data2 = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">600</span>, <span class="number">400</span>)</span><br><span class="line"><span class="built_in">print</span>(data2.shape)  <span class="comment"># torch.Size([4, 3, 600, 400])</span></span><br><span class="line"><span class="comment"># data1 @ data2 等价于 data1.matmul(data2)</span></span><br><span class="line"><span class="built_in">print</span>(data1.matmul(data2).shape)  <span class="comment"># torch.Size([4, 3, 800, 400])</span></span><br></pre></td></tr></tbody></table></figure><p><code>dot()</code>只支持一维矩阵相乘，<code>mm()</code>只支持二维矩阵相乘，<code>bmm()</code>只支持三维矩阵相乘，<code>matmul()</code>支持任意维度矩阵相乘。</p><h2 id="比较运算"><a href="#比较运算" class="headerlink" title="比较运算"></a>比较运算</h2><h3 id="eq-ne-gt-ge-lt-le-equal"><a href="#eq-ne-gt-ge-lt-le-equal" class="headerlink" title=".eq/.ne/.gt/.ge/.lt/.le/.equal"></a>.eq/.ne/.gt/.ge/.lt/.le/.equal</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">10</span>).view(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[5, 6, 1, 2, 0],</span></span><br><span class="line"><span class="string">        [8, 9, 3, 7, 4]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.eq(data, <span class="number">5</span>))  <span class="comment"># 等价于 data == 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True, False, False, False, False],</span></span><br><span class="line"><span class="string">        [False, False, False, False, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.ne(data, <span class="number">5</span>))  <span class="comment"># 等价于 data != 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False,  True,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [ True,  True,  True,  True,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于</span></span><br><span class="line"><span class="built_in">print</span>(torch.gt(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &gt; 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False,  True, False, False, False],</span></span><br><span class="line"><span class="string">        [ True,  True, False,  True, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 大于等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.ge(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &gt;= 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True,  True, False, False, False],</span></span><br><span class="line"><span class="string">        [ True,  True, False,  True, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于</span></span><br><span class="line"><span class="built_in">print</span>(torch.lt(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &lt; 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False, False,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False,  True, False,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小于等于</span></span><br><span class="line"><span class="built_in">print</span>(torch.le(data, <span class="number">5</span>))  <span class="comment"># 等价于 data &lt;= 5</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[ True, False,  True,  True,  True],</span></span><br><span class="line"><span class="string">        [False, False,  True, False,  True]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较两个 tensor 是否相同</span></span><br><span class="line"><span class="built_in">print</span>(torch.equal(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.tensor([<span class="number">1</span>, <span class="number">2</span>])))  <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(torch.equal(torch.tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.tensor([<span class="number">2</span>, <span class="number">1</span>])))  <span class="comment"># False</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.not_equal()</code>是<code>torch.ne()</code>的别名。<br><code>torch.greater()</code>是<code>torch.gt()</code>的别名。<br><code>torch.greater_equal()</code>是<code>torch.ge()</code>的别名。<br><code>torch.less()</code>是<code>torch.lt()</code>的别名。<br><code>torch.less_equal()</code>是<code>torch.le()</code>的别名。</p><h3 id="isfinite-isinf-isposinf-isneginf-isnan-isreal-isin"><a href="#isfinite-isinf-isposinf-isneginf-isnan-isreal-isin" class="headerlink" title=".isfinite/.isinf/.isposinf/.isneginf/.isnan/.isreal/.isin"></a>.isfinite/.isinf/.isposinf/.isneginf/.isnan/.isreal/.isin</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([<span class="number">1</span>, <span class="built_in">float</span>(<span class="string">'inf'</span>), torch.inf, <span class="built_in">float</span>(<span class="string">'-inf'</span>), -torch.inf, <span class="built_in">float</span>(<span class="string">'nan'</span>), torch.nan, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"><span class="comment"># 是否不是无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isfinite(data))  <span class="comment"># tensor([ True, False, False, False, False, False, False,  True,  True])</span></span><br><span class="line"><span class="comment"># 是否是无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isinf(data))  <span class="comment"># tensor([False,  True,  True,  True,  True, False, False, False, False])</span></span><br><span class="line"><span class="comment"># 是否是正无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isposinf(data))  <span class="comment"># tensor([False,  True,  True, False, False, False, False, False, False])</span></span><br><span class="line"><span class="comment"># 是否是负无穷大</span></span><br><span class="line"><span class="built_in">print</span>(torch.isneginf(data))  <span class="comment"># tensor([False, False, False,  True,  True, False, False, False, False])</span></span><br><span class="line"><span class="comment"># 是否是 NaN</span></span><br><span class="line"><span class="built_in">print</span>(torch.isnan(data))  <span class="comment"># tensor([False, False, False, False, False,  True,  True, False, False])</span></span><br><span class="line"><span class="comment"># 是否是实数</span></span><br><span class="line"><span class="built_in">print</span>(torch.isreal(torch.tensor([<span class="number">1</span>, <span class="number">1</span>+<span class="number">1j</span>, <span class="number">2</span>+<span class="number">0j</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>), <span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor([ True, False,  True,  True,  True,  True])</span></span><br><span class="line"></span><br><span class="line">elements = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">test_elements = [<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="comment"># 对 elements 的每个元素进行判断是否在 test_elements 中</span></span><br><span class="line"><span class="built_in">print</span>(torch.isin(torch.tensor(elements), torch.tensor(test_elements)))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[False,  True],</span></span><br><span class="line"><span class="string">        [ True, False]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="isclose-allclose"><a href="#isclose-allclose" class="headerlink" title=".isclose/.allclose"></a>.isclose/.allclose</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否接近</span></span><br><span class="line"><span class="built_in">print</span>(torch.isclose(torch.tensor((<span class="number">1.</span>, <span class="number">2</span>, <span class="number">3</span>)), torch.tensor((<span class="number">1</span> + <span class="number">1e-10</span>, <span class="number">3</span>, <span class="number">4</span>))))  <span class="comment"># tensor([ True, False, False])</span></span><br><span class="line"><span class="built_in">print</span>(torch.isclose(torch.tensor((<span class="built_in">float</span>(<span class="string">'inf'</span>), <span class="number">4</span>)), torch.tensor((<span class="built_in">float</span>(<span class="string">'inf'</span>), <span class="number">6</span>)), rtol=<span class="number">.5</span>))  <span class="comment"># tensor([True, True])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否都接近</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">10000.</span>, <span class="number">1e-07</span>]), torch.tensor([<span class="number">10000.1</span>, <span class="number">1e-08</span>])))  <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">10000.</span>, <span class="number">1e-08</span>]), torch.tensor([<span class="number">10000.1</span>, <span class="number">1e-09</span>])))  <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)]), torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)])))  <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)]), torch.tensor([<span class="number">1.0</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>)]), equal_nan=<span class="literal">True</span>))  <span class="comment"># True</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)</code></p><p><code>torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)</code></p><p>校验是否接近公式：<script type="math/tex">|input - other| \le atol + rtol \times |other|</script></p><h3 id="maximum-minimum-fmax-fmin"><a href="#maximum-minimum-fmax-fmin" class="headerlink" title=".maximum/.minimum/.fmax/.fmin"></a>.maximum/.minimum/.fmax/.fmin</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data1 = torch.tensor([<span class="number">9.7</span>, <span class="built_in">float</span>(<span class="string">'nan'</span>), <span class="number">3.1</span>, torch.nan, <span class="number">11.1</span>])</span><br><span class="line">data2 = torch.tensor([-<span class="number">2.2</span>, <span class="number">0.5</span>, torch.nan, <span class="built_in">float</span>(<span class="string">'nan'</span>), <span class="number">7.8</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.maximum(data1, data2))  <span class="comment"># tensor([ 9.7000,     nan,     nan,     nan, 11.1000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.minimum(data1, data2))  <span class="comment"># tensor([-2.2000,     nan,     nan,     nan,  7.8000])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.fmax(data1, data2))  <span class="comment"># tensor([ 9.7000,  0.5000,  3.1000,     nan, 11.1000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.fmin(data1, data2))  <span class="comment"># tensor([-2.2000,  0.5000,  3.1000,     nan,  7.8000])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.maximum()</code>和<code>torch.minimum()</code>用于比较两数的大小，不支持比较<code>NaN</code>。</p><p><code>torch.fmax()</code>和<code>torch.fmin()</code>用于比较两数的大小，支持比较<code>NaN</code>。</p><h3 id="sort-msort-argsort"><a href="#sort-msort-argsort" class="headerlink" title=".sort/.msort/.argsort"></a>.sort/.msort/.argsort</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### dim 默认等于 -1，按列进行排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.sort(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.sort(</span></span><br><span class="line"><span class="string">values=tensor([[0.0293, 0.2793, 0.4031, 0.7347, 0.7576],</span></span><br><span class="line"><span class="string">        [0.3971, 0.4388, 0.5695, 0.7544, 0.7999]]),</span></span><br><span class="line"><span class="string">indices=tensor([[4, 1, 2, 3, 0],</span></span><br><span class="line"><span class="string">        [1, 4, 3, 2, 0]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 按行进行排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.sort(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.sort(</span></span><br><span class="line"><span class="string">values=tensor([[0.7576, 0.2793, 0.4031, 0.5695, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.3971, 0.7544, 0.7347, 0.4388]]),</span></span><br><span class="line"><span class="string">indices=tensor([[0, 0, 0, 1, 0],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 0, 1]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># descending=True 降序排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.sort(data, descending=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.sort(</span></span><br><span class="line"><span class="string">values=tensor([[0.7576, 0.7347, 0.4031, 0.2793, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.7544, 0.5695, 0.4388, 0.3971]]),</span></span><br><span class="line"><span class="string">indices=tensor([[0, 3, 2, 1, 4],</span></span><br><span class="line"><span class="string">        [0, 2, 3, 4, 1]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿第一维度进行排序</span></span><br><span class="line"><span class="built_in">print</span>(torch.msort(data))  <span class="comment"># 等价于 torch.sort(data, dim=0)[0]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.5695, 0.0293],</span></span><br><span class="line"><span class="string">        [0.7999, 0.3971, 0.7544, 0.7347, 0.4388]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### dim 默认等于 -1，按列进行排序，返回索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argsort(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[4, 1, 2, 3, 0],</span></span><br><span class="line"><span class="string">        [1, 4, 3, 2, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 按行进行排序，返回索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argsort(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 0, 0, 1, 0],</span></span><br><span class="line"><span class="string">        [1, 1, 1, 0, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># descending=True 降序排序，返回索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argsort(data, descending=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0, 3, 2, 1, 4],</span></span><br><span class="line"><span class="string">        [0, 2, 3, 4, 1]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.sort()</code>排序后返回排序结果和索引。</p><p><code>torch.msort(input))</code>等价于<code>torch.sort()</code>对第一维度进行排序在取排序结果，<code>torch.sort(input, dim=0)[0]</code>，返回结果不包含索引。</p><p><code>torch.argsort()</code>返回排序后的索引。</p><h3 id="topk-kthvalue"><a href="#topk-kthvalue" class="headerlink" title=".topk/.kthvalue"></a>.topk/.kthvalue</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.randperm(<span class="number">30</span>, dtype=torch.double).view(<span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[25.,  4.,  6.,  8., 23., 18., 17., 20., 19., 12.],</span></span><br><span class="line"><span class="string">        [ 5., 14., 22.,  3., 27., 15.,  9., 13.,  7., 11.],</span></span><br><span class="line"><span class="string">        [10.,  2., 24., 29., 21., 26., 28.,  1., 16.,  0.]],</span></span><br><span class="line"><span class="string">       dtype=torch.float64)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取前 k 大的数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.topk(data, <span class="number">3</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.topk(</span></span><br><span class="line"><span class="string">values=tensor([[25., 23., 20.],</span></span><br><span class="line"><span class="string">        [27., 22., 15.],</span></span><br><span class="line"><span class="string">        [29., 28., 26.]], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([[0, 4, 7],</span></span><br><span class="line"><span class="string">        [4, 2, 5],</span></span><br><span class="line"><span class="string">        [3, 6, 5]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 获取前 k 小的数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.topk(data, <span class="number">3</span>, largest=<span class="literal">False</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.topk(</span></span><br><span class="line"><span class="string">values=tensor([[4., 6., 8.],</span></span><br><span class="line"><span class="string">        [3., 5., 7.],</span></span><br><span class="line"><span class="string">        [0., 1., 2.]], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([[1, 2, 3],</span></span><br><span class="line"><span class="string">        [3, 0, 8],</span></span><br><span class="line"><span class="string">        [9, 7, 1]]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取第 k 小的数据</span></span><br><span class="line"><span class="built_in">print</span>(torch.kthvalue(data, <span class="number">8</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.kthvalue(</span></span><br><span class="line"><span class="string">values=tensor([20., 15., 26.], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([7, 5, 5]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.kthvalue(data, <span class="number">2</span>, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.kthvalue(</span></span><br><span class="line"><span class="string">values=tensor([10.,  4., 22.,  8., 23., 18., 17., 13., 16., 11.], dtype=torch.float64),</span></span><br><span class="line"><span class="string">indices=tensor([2, 0, 1, 0, 0, 0, 0, 1, 2, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.topk()</code>获取前<code>k</code>大的数据，<code>torch.kthvalue()</code>获取第<code>k</code>小的数据。</p><h2 id="归约运算"><a href="#归约运算" class="headerlink" title="归约运算"></a>归约运算</h2><h3 id="max-min-amax-amin-aminmax-argmax-argmin"><a href="#max-min-amax-amin-aminmax-argmax-argmin" class="headerlink" title=".max/.min/.amax/.amin/.aminmax/.argmax/.argmin"></a>.max/.min/.amax/.amin/.aminmax/.argmax/.argmin</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">data = torch.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[0.7576, 0.2793, 0.4031, 0.7347],</span></span><br><span class="line"><span class="string">        [0.0293, 0.7999, 0.3971, 0.7544],</span></span><br><span class="line"><span class="string">        [0.5695, 0.4388, 0.6387, 0.5247],</span></span><br><span class="line"><span class="string">        [0.6826, 0.3051, 0.4635, 0.4550]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(data))  <span class="comment"># tensor(0.7999)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.max(</span></span><br><span class="line"><span class="string">values=tensor([0.7576, 0.7999, 0.6387, 0.7544]),</span></span><br><span class="line"><span class="string">indices=tensor([0, 1, 2, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(data))  <span class="comment"># tensor(0.0293)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(data, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.min(</span></span><br><span class="line"><span class="string">values=tensor([0.2793, 0.0293, 0.4388, 0.3051]),</span></span><br><span class="line"><span class="string">indices=tensor([1, 0, 1, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值</span></span><br><span class="line"><span class="built_in">print</span>(torch.amax(data))  <span class="comment"># tensor(0.7999)</span></span><br><span class="line"><span class="built_in">print</span>(torch.amax(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([0.7576, 0.7999, 0.6387, 0.7544])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.amin(data))  <span class="comment"># tensor(0.0293)</span></span><br><span class="line"><span class="built_in">print</span>(torch.amin(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([0.2793, 0.0293, 0.4388, 0.3051])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值和最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.aminmax(data))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.aminmax(</span></span><br><span class="line"><span class="string">min=tensor(0.0293),</span></span><br><span class="line"><span class="string">max=tensor(0.7999))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.aminmax(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.aminmax(</span></span><br><span class="line"><span class="string">min=tensor([0.0293, 0.2793, 0.3971, 0.4550]),</span></span><br><span class="line"><span class="string">max=tensor([0.7576, 0.7999, 0.6387, 0.7544]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmax(data))  <span class="comment"># tensor(5)</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmax(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([0, 1, 2, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmin(data))  <span class="comment"># tensor(4)</span></span><br><span class="line"><span class="built_in">print</span>(torch.argmin(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([1, 0, 1, 1])</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.max()</code>和<code>torch.min()</code>指定了<code>dim</code>时，返回极值和索引。</p><p><code>torch.amax()</code>和<code>torch.amin()</code>只返回极值，不返回索引。</p><p><code>torch.argmax()</code>和<code>torch.argmin()</code>返回极值对应的索引。不指定<code>dim</code>时，返回的是打平后的索引。</p><h3 id="mean-nanmean"><a href="#mean-nanmean" class="headerlink" title=".mean/.nanmean"></a>.mean/.nanmean</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均值</span></span><br><span class="line"><span class="built_in">print</span>(torch.mean(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.mean(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([nan, 6., 7., 8., nan])</span></span><br><span class="line"><span class="built_in">print</span>(torch.mean(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([nan, 7., nan])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 平均值（忽略 NaN 值）</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmean(data))  <span class="comment"># tensor(7.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmean(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([7.5000, 6.0000, 7.0000, 8.0000, 6.5000])</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmean(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([ 2.5000,  7.0000, 11.5000])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="median-nanmedian"><a href="#median-nanmedian" class="headerlink" title=".median/.nanmedian"></a>.median/.nanmedian</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中位数</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.median(</span></span><br><span class="line"><span class="string">values=tensor([nan, 6., 7., 8., nan]),</span></span><br><span class="line"><span class="string">indices=tensor([0, 1, 1, 1, 2]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.median(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([nan, 7., nan])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.median(</span></span><br><span class="line"><span class="string">values=tensor([nan, 7., nan]),</span></span><br><span class="line"><span class="string">indices=tensor([0, 2, 4]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 中位数（忽略 NaN 值）</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmedian(data))  <span class="comment"># tensor(7.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmedian(data, dim=<span class="number">0</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.nanmedian(</span></span><br><span class="line"><span class="string">values=tensor([5., 6., 7., 8., 4.]),</span></span><br><span class="line"><span class="string">indices=tensor([1, 1, 1, 1, 0]))</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="built_in">print</span>(torch.nanmedian(data, dim=<span class="number">1</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">torch.return_types.nanmedian(</span></span><br><span class="line"><span class="string">values=tensor([ 2.,  7., 11.]),</span></span><br><span class="line"><span class="string">indices=tensor([2, 2, 1]))</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><h3 id="sum-nansum"><a href="#sum-nansum" class="headerlink" title=".sum/.nansum"></a>.sum/.nansum</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([nan, 18., 21., 24., nan])</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">sum</span>(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([nan, 35., nan])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加（忽略 NaN 值）</span></span><br><span class="line"><span class="built_in">print</span>(torch.nansum(data))  <span class="comment"># tensor(91.)</span></span><br><span class="line"><span class="built_in">print</span>(torch.nansum(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([15., 18., 21., 24., 13.])</span></span><br><span class="line"><span class="built_in">print</span>(torch.nansum(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([10., 35., 46.])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="prod"><a href="#prod" class="headerlink" title=".prod"></a>.prod</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, torch.nan]])</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[nan,  1.,  2.,  3.,  4.],</span></span><br><span class="line"><span class="string">        [ 5.,  6.,  7.,  8.,  9.],</span></span><br><span class="line"><span class="string">        [10., 11., 12., 13., nan]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 累乘</span></span><br><span class="line"><span class="built_in">print</span>(torch.prod(data))  <span class="comment"># tensor(nan)</span></span><br><span class="line"><span class="built_in">print</span>(torch.prod(data, dim=<span class="number">0</span>))  <span class="comment"># tensor([ nan,  66., 168., 312.,  nan])</span></span><br><span class="line"><span class="built_in">print</span>(torch.prod(data, dim=<span class="number">1</span>))  <span class="comment"># tensor([   nan, 15120.,    nan])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="var-var-mean-std-std-mean"><a href="#var-var-mean-std-std-mean" class="headerlink" title=".var/.var_mean/.std/.std_mean"></a>.var/.var_mean/.std/.std_mean</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.arange(<span class="number">1</span>, <span class="number">10</span>, dtype=torch.double).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[1., 2., 3.],</span></span><br><span class="line"><span class="string">        [4., 5., 6.],</span></span><br><span class="line"><span class="string">        [7., 8., 9.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本方差</span></span><br><span class="line"><span class="built_in">print</span>(torch.var(data))  <span class="comment"># tensor(7.5000, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 总体方差</span></span><br><span class="line"><span class="built_in">print</span>(torch.var(data, unbiased=<span class="literal">False</span>))  <span class="comment"># tensor(6.6667, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 同时计算方差和平均值</span></span><br><span class="line"><span class="built_in">print</span>(torch.var_mean(data))  <span class="comment"># (tensor(7.5000, dtype=torch.float64), tensor(5., dtype=torch.float64))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本标准差</span></span><br><span class="line"><span class="built_in">print</span>(torch.std(data))  <span class="comment"># tensor(2.7386, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 总体标准差</span></span><br><span class="line"><span class="built_in">print</span>(torch.std(data, unbiased=<span class="literal">False</span>))  <span class="comment"># tensor(2.5820, dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># 同时计算标准差和平均值</span></span><br><span class="line"><span class="built_in">print</span>(torch.std_mean(data))  <span class="comment"># (tensor(2.7386, dtype=torch.float64), tensor(5., dtype=torch.float64))</span></span><br></pre></td></tr></tbody></table></figure><p>总体方差：<script type="math/tex">\sigma^2 = \frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2</script></p><p>样本方差：<script type="math/tex">s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2</script></p><p>总体标准差：<script type="math/tex">\sigma = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2}</script></p><p>样本标准差：<script type="math/tex">s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar{x})^2}</script></p><h3 id="norm"><a href="#norm" class="headerlink" title=".norm"></a>.norm</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">data = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">2.</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2., 2.],</span></span><br><span class="line"><span class="string">        [2., 2.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.norm(data, p=<span class="number">1</span>))  <span class="comment"># tensor(8.)</span></span><br><span class="line"><span class="comment"># 二范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.norm(data))  <span class="comment"># tensor(4.)</span></span><br><span class="line"><span class="comment"># 三范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.norm(data, p=<span class="number">3</span>))  <span class="comment"># tensor(3.1748)</span></span><br></pre></td></tr></tbody></table></figure><p>一范数：<script type="math/tex">||x||_1 = \sum_{i=1}^N|x_i|</script></p><p>二范数：<script type="math/tex">||x||_2 = \sqrt{\sum_{i=1}^Nx_i^2}</script></p><p>p范数：<script type="math/tex">||x||_p = (\sum_{i=1}^N|x_i|^p)^\frac{1}{p}</script></p><h3 id="dist"><a href="#dist" class="headerlink" title=".dist"></a>.dist</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">4.</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[4., 4.],</span></span><br><span class="line"><span class="string">        [4., 4.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">other = torch.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">2.</span>)</span><br><span class="line"><span class="built_in">print</span>(other)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">tensor([[2., 2.],</span></span><br><span class="line"><span class="string">        [2., 2.]])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.dist(<span class="built_in">input</span>, other, p=<span class="number">1</span>))  <span class="comment"># tensor(8.)</span></span><br><span class="line"><span class="comment"># 二范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.dist(<span class="built_in">input</span>, other))  <span class="comment"># tensor(4.)</span></span><br><span class="line"><span class="comment"># 三范数</span></span><br><span class="line"><span class="built_in">print</span>(torch.dist(<span class="built_in">input</span>, other, p=<span class="number">3</span>))  <span class="comment"># tensor(3.1748)</span></span><br></pre></td></tr></tbody></table></figure><p><code>torch.dist()</code>返回<code>(input - other)</code>的<code>p</code>范数。</p><h3 id="any-all"><a href="#any-all" class="headerlink" title=".any/.all"></a>.any/.all</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([])))  <span class="comment"># tensor(False)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">any</span>(torch.tensor([<span class="literal">False</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(False)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>])))  <span class="comment"># tensor(True)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([<span class="literal">True</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(False)</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">all</span>(torch.tensor([<span class="literal">False</span>, <span class="literal">False</span>])))  <span class="comment"># tensor(False)</span></span><br></pre></td></tr></tbody></table></figure><h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><h3 id="save"><a href="#save" class="headerlink" title=".save"></a>.save</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to file</span></span><br><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">torch.save(x, <span class="string">'tensor.pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to io.BytesIO buffer</span></span><br><span class="line">buffer = io.BytesIO()</span><br><span class="line">torch.save(x, buffer)</span><br></pre></td></tr></tbody></table></figure><h3 id="load"><a href="#load" class="headerlink" title=".load"></a>.load</h3><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a module with 'ascii' encoding for unpickling</span></span><br><span class="line">torch.load(<span class="string">'tensors.pt'</span>, encoding=<span class="string">'ascii'</span>)</span><br><span class="line"><span class="comment"># Load all tensors onto the CPU</span></span><br><span class="line">torch.load(<span class="string">'tensors.pt'</span>, map_location=torch.device(<span class="string">'cpu'</span>))</span><br><span class="line"><span class="comment"># Map tensors from GPU 1 to GPU 0</span></span><br><span class="line">torch.load(<span class="string">'tensors.pt'</span>, map_location={<span class="string">'cuda:1'</span>: <span class="string">'cuda:0'</span>})</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load from io.BytesIO buffer</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'tensor.pt'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    buffer = io.BytesIO(f.read())</span><br><span class="line">torch.load(buffer)</span><br></pre></td></tr></tbody></table></figure><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://wpz.me">wpz</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://wpz.me/posts/668e89ae/">https://wpz.me/posts/668e89ae/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://wpz.me/go/#aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpoLWhhbnM=" rel="noopener external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wpz.me" target="_blank">Wpz's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/3d0f623f/" title="汉诺塔"><div class="cover" style="background:var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">汉诺塔</div></div></a></div><div class="next-post pull-right"><a href="/posts/e8b7be43/" title="Mathjax 与 LaTex 公式"><div class="cover" style="background:var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Mathjax 与 LaTex 公式</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="ribbon"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">wpz</div><div class="author-info__description">你只管努力，剩下的交给时间！</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" href="javascript:window.open('http://lb94wpz.github.io');"><i class="fa fa-bookmark"></i><span>Wpz's Blog (拖拽收藏)</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://wpz.me/go/#aHR0cHM6Ly9naXRodWIuY29tL2xiOTR3cHo=" rel="noopener external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="https://wpz.me/go/#aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTE2NjQ3OTQwNDQ=" rel="noopener external nofollow noreferrer" target="_blank" title="CloudMusic"><i class="fa fa-music" style="color:#24292e"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss" style="color:#24292e"></i></a></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div><div class="twopeople"><div class="twopeople"><div class="container" style="height:200px"><canvas class="illo" width="800" height="800" style="max-width:200px;max-height:200px;touch-action:none;width:640px;height:640px"></canvas></div><script src="/js/twopeople/twopeople1.js"></script><script src="/js/twopeople/zdog.dist.js"></script><script id="rendered-js" src="/js/twopeople/twopeople.js"></script><style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">Tensor 数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE-Tensor-%E9%BB%98%E8%AE%A4%E7%B1%BB%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">设置 Tensor 默认类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%E4%B8%8E%E5%BC%A0%E9%87%8F"><span class="toc-number">3.</span> <span class="toc-text">标量与张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-Tensor"><span class="toc-number">4.</span> <span class="toc-text">创建 Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor"><span class="toc-number">4.1.</span> <span class="toc-text">.tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#from-numpy"><span class="toc-number">4.2.</span> <span class="toc-text">.from_numpy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor"><span class="toc-number">4.3.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#empty-zeros-ones-full-eye"><span class="toc-number">4.4.</span> <span class="toc-text">.empty&#x2F;.zeros&#x2F;.ones&#x2F;.full&#x2F;.eye</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#arange-linspace-logspace"><span class="toc-number">4.5.</span> <span class="toc-text">.arange&#x2F;.linspace&#x2F;.logspace</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="toc-number">5.</span> <span class="toc-text">随机采样</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90"><span class="toc-number">5.1.</span> <span class="toc-text">随机种子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">随机函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#rand-rand-like"><span class="toc-number">5.2.1.</span> <span class="toc-text">.rand&#x2F;.rand_like</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#randint-randint-like"><span class="toc-number">5.2.2.</span> <span class="toc-text">.randint&#x2F;.randint_like</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#randperm"><span class="toc-number">5.2.3.</span> <span class="toc-text">.randperm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#randn-randn-like"><span class="toc-number">5.2.4.</span> <span class="toc-text">.randn&#x2F;.randn_like</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#normal"><span class="toc-number">5.2.5.</span> <span class="toc-text">.normal</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bernoulli"><span class="toc-number">5.2.6.</span> <span class="toc-text">.bernoulli</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#poisson"><span class="toc-number">5.2.7.</span> <span class="toc-text">.poisson</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multinomial"><span class="toc-number">5.2.8.</span> <span class="toc-text">.multinomial</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-number">6.</span> <span class="toc-text">索引与切片</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Python-%E8%AF%AD%E6%B3%95"><span class="toc-number">6.1.</span> <span class="toc-text">Python 语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#narrow-narrow-copy"><span class="toc-number">6.2.</span> <span class="toc-text">.narrow&#x2F;.narrow_copy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#select-index-select"><span class="toc-number">6.3.</span> <span class="toc-text">.select&#x2F;.index_select</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#masked-select"><span class="toc-number">6.4.</span> <span class="toc-text">.masked_select</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gather"><span class="toc-number">6.5.</span> <span class="toc-text">.gather</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#take-take-along-dim"><span class="toc-number">6.6.</span> <span class="toc-text">.take&#x2F;.take_along_dim</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#argwhere-nonzero"><span class="toc-number">6.7.</span> <span class="toc-text">.argwhere&#x2F;.nonzero</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#where"><span class="toc-number">6.8.</span> <span class="toc-text">.where</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#unravel-index"><span class="toc-number">6.9.</span> <span class="toc-text">.unravel_index</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2"><span class="toc-number">7.</span> <span class="toc-text">维度变换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#t-transpose-movedim-permute"><span class="toc-number">7.1.</span> <span class="toc-text">.t&#x2F;.transpose&#x2F;.movedim&#x2F;.permute</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#view-reshape"><span class="toc-number">7.2.</span> <span class="toc-text">.view&#x2F;.reshape</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#squeeze-unsqueeze"><span class="toc-number">7.3.</span> <span class="toc-text">.squeeze&#x2F;.unsqueeze</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#expand-repeat"><span class="toc-number">7.4.</span> <span class="toc-text">.expand&#x2F;.repeat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tile"><span class="toc-number">7.5.</span> <span class="toc-text">.tile</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E4%B8%8E%E6%8B%86%E5%88%86"><span class="toc-number">8.</span> <span class="toc-text">合并与拆分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cat"><span class="toc-number">8.1.</span> <span class="toc-text">.cat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stack-hstack-vstack-column-stack-dstack"><span class="toc-number">8.2.</span> <span class="toc-text">.stack&#x2F;.hstack&#x2F;.vstack&#x2F;.column_stack&#x2F;.dstack</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chunk"><span class="toc-number">8.3.</span> <span class="toc-text">.chunk</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#split-hsplit-vsplit-dsplit-tensor-split"><span class="toc-number">8.4.</span> <span class="toc-text">.split&#x2F;.hsplit&#x2F;.vsplit&#x2F;.dsplit&#x2F;.tensor_split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#unbind"><span class="toc-number">8.5.</span> <span class="toc-text">.unbind</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%90%E7%82%B9%E8%BF%90%E7%AE%97"><span class="toc-number">9.</span> <span class="toc-text">逐点运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#add-sub-mul-div-remainder-fmod-positive-neg-abs"><span class="toc-number">9.1.</span> <span class="toc-text">.add&#x2F;.sub&#x2F;.mul&#x2F;.div&#x2F;.remainder&#x2F;.fmod&#x2F;.positive&#x2F;.neg&#x2F;.abs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pow-square-sqrt-rsqrt-reciprocal"><span class="toc-number">9.2.</span> <span class="toc-text">.pow&#x2F;.square&#x2F;.sqrt&#x2F;.rsqrt&#x2F;.reciprocal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#exp-log-log2-log10-log1p"><span class="toc-number">9.3.</span> <span class="toc-text">.exp&#x2F;.log&#x2F;.log2&#x2F;.log10&#x2F;.log1p</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sin-cos-tan-asin-acos-atan-atan2-sinh-cosh-tanh"><span class="toc-number">9.4.</span> <span class="toc-text">.sin&#x2F;.cos&#x2F;.tan&#x2F;.asin&#x2F;.acos&#x2F;.atan&#x2F;.atan2&#x2F;.sinh&#x2F;.cosh&#x2F;.tanh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#angle-deg2rad"><span class="toc-number">9.5.</span> <span class="toc-text">.angle&#x2F;.deg2rad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bitwise-and-bitwise-or-bitwise-not-bitwise-xor-bitwise-left-shift-bitwise-right-shift"><span class="toc-number">9.6.</span> <span class="toc-text">.bitwise_and&#x2F;.bitwise_or&#x2F;.bitwise_not&#x2F;.bitwise_xor&#x2F;.bitwise_left_shift&#x2F;.bitwise_right_shift</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#floor-ceil-round"><span class="toc-number">9.7.</span> <span class="toc-text">.floor&#x2F;.ceil&#x2F;.round</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#trunc-frac"><span class="toc-number">9.8.</span> <span class="toc-text">.trunc&#x2F;.frac</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#clamp-clamp-min-clamp-max"><span class="toc-number">9.9.</span> <span class="toc-text">.clamp&#x2F;.clamp_min&#x2F;.clamp_max</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dot-mm-bmm-matmul"><span class="toc-number">9.10.</span> <span class="toc-text">.dot&#x2F;.mm&#x2F;.bmm&#x2F;.matmul</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E8%BF%90%E7%AE%97"><span class="toc-number">10.</span> <span class="toc-text">比较运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#eq-ne-gt-ge-lt-le-equal"><span class="toc-number">10.1.</span> <span class="toc-text">.eq&#x2F;.ne&#x2F;.gt&#x2F;.ge&#x2F;.lt&#x2F;.le&#x2F;.equal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#isfinite-isinf-isposinf-isneginf-isnan-isreal-isin"><span class="toc-number">10.2.</span> <span class="toc-text">.isfinite&#x2F;.isinf&#x2F;.isposinf&#x2F;.isneginf&#x2F;.isnan&#x2F;.isreal&#x2F;.isin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#isclose-allclose"><span class="toc-number">10.3.</span> <span class="toc-text">.isclose&#x2F;.allclose</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#maximum-minimum-fmax-fmin"><span class="toc-number">10.4.</span> <span class="toc-text">.maximum&#x2F;.minimum&#x2F;.fmax&#x2F;.fmin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sort-msort-argsort"><span class="toc-number">10.5.</span> <span class="toc-text">.sort&#x2F;.msort&#x2F;.argsort</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#topk-kthvalue"><span class="toc-number">10.6.</span> <span class="toc-text">.topk&#x2F;.kthvalue</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E7%BA%A6%E8%BF%90%E7%AE%97"><span class="toc-number">11.</span> <span class="toc-text">归约运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#max-min-amax-amin-aminmax-argmax-argmin"><span class="toc-number">11.1.</span> <span class="toc-text">.max&#x2F;.min&#x2F;.amax&#x2F;.amin&#x2F;.aminmax&#x2F;.argmax&#x2F;.argmin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mean-nanmean"><span class="toc-number">11.2.</span> <span class="toc-text">.mean&#x2F;.nanmean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#median-nanmedian"><span class="toc-number">11.3.</span> <span class="toc-text">.median&#x2F;.nanmedian</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sum-nansum"><span class="toc-number">11.4.</span> <span class="toc-text">.sum&#x2F;.nansum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#prod"><span class="toc-number">11.5.</span> <span class="toc-text">.prod</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#var-var-mean-std-std-mean"><span class="toc-number">11.6.</span> <span class="toc-text">.var&#x2F;.var_mean&#x2F;.std&#x2F;.std_mean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#norm"><span class="toc-number">11.7.</span> <span class="toc-text">.norm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dist"><span class="toc-number">11.8.</span> <span class="toc-text">.dist</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#any-all"><span class="toc-number">11.9.</span> <span class="toc-text">.any&#x2F;.all</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">12.</span> <span class="toc-text">序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#save"><span class="toc-number">12.1.</span> <span class="toc-text">.save</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#load"><span class="toc-number">12.2.</span> <span class="toc-text">.load</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/e8b7be43/" title="Mathjax 与 LaTex 公式">Mathjax 与 LaTex 公式</a><time datetime="2024-09-06T10:20:23.000Z" title="发表于 2024-09-06 18:20:23">2024-09-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/668e89ae/" title="（一）PyTorch 张量">（一）PyTorch 张量</a><time datetime="2024-08-26T08:17:41.000Z" title="发表于 2024-08-26 16:17:41">2024-08-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/3d0f623f/" title="汉诺塔">汉诺塔</a><time datetime="2019-06-29T06:08:10.000Z" title="发表于 2019-06-29 14:08:10">2019-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/6ab4a85b/" title="二叉树前序、中序、后序遍历">二叉树前序、中序、后序遍历</a><time datetime="2019-06-29T02:15:28.000Z" title="发表于 2019-06-29 10:15:28">2019-06-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/c2a5fdc5/" title="堆排序">堆排序</a><time datetime="2019-05-29T01:08:29.000Z" title="发表于 2019-05-29 09:08:29">2019-05-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By wpz</div><div class="framework-info"><span>框架 </span><a target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly9oZXhvLmlv" rel="noopener external nofollow noreferrer">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly9naXRodWIuY29tL2plcnJ5YzEyNy9oZXhvLXRoZW1lLWJ1dHRlcmZseQ==" rel="noopener external nofollow noreferrer">Butterfly</a></div><div class="footer_custom_text">我也是有底线的！！！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="mouse" type="button" title="右键模式" onclick="changeMouseMode()"><i class="iconfont icon-mouseR"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button><button id="go-down" type="button" title="回到底部" onclick="btf.scrollToDest(document.body.scrollHeight,500)"><i class="fas fa-arrow-down"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>(()=>{const e=document.querySelectorAll("#article-container .mermaid-wrap");if(0===e.length)return;const t=()=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";Array.from(e).forEach((e,n)=>{const d=e.firstElementChild,r="mermaid-"+n,a="%%{init:{ 'theme':'"+t+"'}}%%\n"+d.textContent,i=mermaid.render(r,a);var m;"string"==typeof i?(m=i,d.insertAdjacentHTML("afterend",m)):i.then(({svg:e})=>{d.insertAdjacentHTML("afterend",e)})})},n=()=>{window.loadMermaid?t():getScript("https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js").then(t)};btf.addGlobalFn("themeChange",t,"mermaid"),window.pjax?n():document.addEventListener("DOMContentLoaded",n)})()</script></div><div class="aplayer no-destroy" data-server="netease" data-type="playlist" data-id="2503010581" data-order="random" data-fixed="true" data-list-folded="true"></div><script src="/js/aplayer.js"></script><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/title-change.min.js"></script><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/star.min.js"></script><script async src="https://fastly.jsdelivr.net/gh/lb94wpz/live2d-widget/autoload.js"></script><div class="go-to-top faa-float animated" style="top:-900px"></div><script defer src="https://cdn.bootcdn.net/ajax/libs/jquery/3.7.1/jquery.min.js"></script><script defer src="https://fastly.jsdelivr.net/gh/lb94wpz/CDN/js/szgotop.min.js"></script><script async src="/js/sun_moon.js"></script><script defer src="/js/rightmenu.js"></script><script defer src="https://cdn1.tianli0.top/npm/sweetalert2@8.19.0/dist/sweetalert2.all.js"></script><script defer src="/js/lunar.js"></script><script defer src="/js/festival.js"></script><script defer id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zindex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors=["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",(function(){if(btf.removeGlobalFnEvent("pjax"),btf.removeGlobalFnEvent("themeChange"),document.getElementById("rightside").classList.remove("rightside-show"),window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),"object"==typeof disqusjs&&disqusjs.destroy()})),document.addEventListener("pjax:complete",(function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach(e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)}),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll()})),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="/"><i class="fa fa-home"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-selection"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:rmf.search('Google');"><i class="iconfont icon-google"></i><span>谷歌搜索</span></a><a class="rightMenu-item" href="javascript:rmf.search('Bing');"><i class="iconfont icon-bing"></i><span>必应搜索</span></a><a class="rightMenu-item" href="javascript:rmf.searchInThisPage();"><i class="fas fa-search"></i><span>站内搜索</span></a><a class="rightMenu-item" id="menu-to" href="javascript:window.open(window.getSelection().toString());"><i class="iconfont icon-tiaozhuan"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-a"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.open()"><i class="iconfont icon-tiaozhuan"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a><a class="rightMenu-item" href="javascript:toRandomPost();"><i class="fa fa-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:rmf.playMusic();"><i class="fa fa-music"></i><span>听听小曲</span></a><a class="rightMenu-item" id="read-mode" href="javascript:rmf.switchReadMode();"><i class="fa fa-book"></i><span>阅读模式</span></a><a class="rightMenu-item" href="javascript:rmf.switchTranslate();"><i class="iconfont icon-jianfanqiehuan"></i><span>简繁切换</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="iconfont icon-zhouye"></i><span>昼夜更替</span></a><a class="rightMenu-item" href="javascript:changeMouseMode();"><i class="iconfont icon-mouseR"></i><span>切换鼠标右键</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly93d3cudHJhdmVsbGluZ3MuY24vZ28uaHRtbA==" rel="noopener external nofollow noreferrer"><i class="fa fa-bus"></i><span>开往</span></a><a class="rightMenu-item" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly9mb3JldmVyYmxvZy5jbi9nby5odG1s" rel="noopener external nofollow noreferrer"><i class="fa fa-spinner"></i><span>虫洞</span></a><a class="rightMenu-item" target="_blank" href="https://wpz.me/go/#aHR0cHM6Ly90cmF2ZWwubW9lL2dvLmh0bWw/dHJhdmVsPW9u" rel="noopener external nofollow noreferrer"><i class="iconfont icon-xingqiu"></i><span>跃迁</span></a></div></div><script data-pjax>function butterfly_swiper_injector_config(){var e=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),e.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="/",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","500ms"),arr[i].setAttribute("data-wow-offset","100"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration",""),arr[i].setAttribute("data-wow-delay",""),arr[i].setAttribute("data-wow-offset",""),arr[i].setAttribute("data-wow-iteration","")</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>function history_calendar_injector_config(){var i=document.getElementsByClassName("sticky_layout")[0];console.log("已挂载history_calendar"),i.insertAdjacentHTML("afterbegin",'<div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height: 100px;overflow: hidden"><div class="history_swiper-container" id="history-container" style="width: 100%;height: 100%"><div class="swiper-wrapper" id="history_container_wrapper" style="height:20px"></div></div></div></div>')}document.getElementsByClassName("sticky_layout")[0]&&"/"===location.pathname&&history_calendar_injector_config()</script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script></body></html><script>var posts = ["posts/f189e9a9/","posts/e8b7be43/","posts/668e89ae/","posts/6ab4a85b/","posts/14e6f1eb/","posts/3d0f623f/","posts/c2a5fdc5/","posts/96555fb2/","posts/ff8068c0/","posts/43d00a99/","posts/1ac49179/","posts/e2f703e6/","posts/b3b04b5f/","posts/2262a086/","posts/6cc4d10b/","posts/8777e763/","posts/c09adc05/","posts/6574bc1f/"];function toRandomPost(){ window.pjax ? pjax.loadUrl('/' + posts[Math.floor(Math.random() * posts.length)]) : window.open('/' + posts[Math.floor(Math.random() * posts.length)], "_self"); };</script>